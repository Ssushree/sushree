{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7057dd",
   "metadata": {},
   "source": [
    "# Implementation of the paper \"Fine-grained generalized zero-shot learning via dense attribute-based attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9ccf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.resnet as models\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import torch.optim as optim\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac6ac9",
   "metadata": {},
   "source": [
    "# AWA2 dataset\n",
    "#images = 37322\n",
    "\n",
    "#classes = 50, 40 seen classes and 10 unseen classes\n",
    "\n",
    "Each class has 85 number of attributes that represent the class infromation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb6dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Sushree/Jio_Institute/Dataset/\n",
      "C:/Sushree/Jio_Institute/Dataset/data/xlsa17/data/AWA2/res101.mat\n"
     ]
    }
   ],
   "source": [
    "img_dir = 'C:/Sushree/Jio_Institute/Dataset/'\n",
    "print(img_dir)\n",
    "\n",
    "file_paths = 'C:/Sushree/Jio_Institute/Dataset/data/xlsa17/data/AWA2/res101.mat'\n",
    "print(file_paths)\n",
    "\n",
    "#resNet101.mat includes the following fields:\n",
    "#-features: columns correspond to image instances\n",
    "#-labels: label number of a class is its row number in allclasses.txt\n",
    "#-image_files: image sources  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d3b51",
   "metadata": {},
   "source": [
    "# Let's visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd214aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_distribution(file_paths):    \n",
    "    matcontent = sio.loadmat(file_paths)\n",
    "    print(matcontent)\n",
    "\n",
    "    image_files = np.squeeze(matcontent['image_files'])\n",
    "    #print(image_files)\n",
    "\n",
    "    labels = np.squeeze(matcontent['labels'])\n",
    "    print(labels)\n",
    "    print(labels.size)  # 37322 for AWA2\n",
    "\n",
    "    class_names = []\n",
    "    for idx in range(len(image_files)):\n",
    "        image_file = image_files[idx][0]\n",
    "        class_name = image_file.split('/')[5:][3]\n",
    "        class_names.append(class_name)\n",
    "\n",
    "    print(len(class_names))   \n",
    "    #print(class_names)\n",
    "    \n",
    "    num_bins = 50 # # for AWA2\n",
    "    \n",
    "    plt.figure(figsize=(20,6))\n",
    "    \n",
    "    plt.title(\"Data Distribution: AWA2\")\n",
    "    plt.xlabel(\"Categories\")\n",
    "    plt.ylabel(\"Number of Classes\")\n",
    "    \n",
    "    plt.xticks(rotation = 75)\n",
    "    plt.grid(color = 'red', linestyle = '--', linewidth = 0.3)\n",
    "    plt.hist(class_names, num_bins, align=\"mid\")\n",
    "\n",
    "visualize_data_distribution(file_paths)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72caa267",
   "metadata": {},
   "source": [
    "# Let's extract deep features (consider pre-trained ResNet 101 with no fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28dc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomedDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir , file_paths, transform=None):\n",
    "        self.matcontent = sio.loadmat(file_paths)\n",
    "        self.image_files = np.squeeze(self.matcontent['image_files'])\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx][0]\n",
    "        image_file = os.path.join(self.img_dir, '/'.join(image_file.split('/')[5:]))\n",
    "        image = Image.open(image_file)\n",
    "        \n",
    "        if image.mode == 'L':\n",
    "            image=image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57bc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "AWA2Dataset = CustomedDataset(img_dir , file_paths, data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51604ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sushree.Behera\\anaconda3\\envs\\tf_sushree\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sushree.Behera\\anaconda3\\envs\\tf_sushree\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-313          [-1, 512, 14, 14]               0\n",
      "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-316            [-1, 512, 7, 7]               0\n",
      "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-321           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-322           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-325            [-1, 512, 7, 7]               0\n",
      "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-328            [-1, 512, 7, 7]               0\n",
      "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-331           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-332           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-335            [-1, 512, 7, 7]               0\n",
      "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-338            [-1, 512, 7, 7]               0\n",
      "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-341           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-342           [-1, 2048, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 42,500,160\n",
      "Trainable params: 0\n",
      "Non-trainable params: 42,500,160\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 429.71\n",
      "Params size (MB): 162.13\n",
      "Estimated Total Size (MB): 592.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "model_ref = models.resnet101(pretrained=True)\n",
    "model_ref.eval()\n",
    "\n",
    "model_f = nn.Sequential(*list(model_ref.children())[:-2])\n",
    "model_f.eval()\n",
    "\n",
    "\n",
    "for param in model_f.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(model_f)\n",
    "        \n",
    "from torchsummary import summary\n",
    "summary(model_f, (3, 224, 224))    \n",
    "\n",
    "dataset_loader = torch.utils.data.DataLoader(AWA2Dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "740abf7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "[[[[6.39874101e-01 7.99183667e-01 1.18703568e+00 ... 2.31745338e+00\n",
      "    1.89217317e+00 1.05180609e+00]\n",
      "   [6.93525970e-01 7.76937425e-01 7.62710333e-01 ... 1.08098006e+00\n",
      "    2.08995390e+00 1.81486404e+00]\n",
      "   [1.15845823e+00 5.59391797e-01 0.00000000e+00 ... 4.65849042e-02\n",
      "    2.83482373e-01 5.52386284e-01]\n",
      "   ...\n",
      "   [1.19192398e+00 1.74771190e+00 8.74018908e-01 ... 1.49633393e-01\n",
      "    3.17509443e-01 1.58631936e-01]\n",
      "   [4.36887980e-01 9.93416965e-01 1.06428409e+00 ... 2.08480895e-01\n",
      "    6.19054079e-01 5.32939732e-01]\n",
      "   [0.00000000e+00 3.55925024e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.73646901e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 7.45262146e-01\n",
      "    1.34062600e+00 1.13507092e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 3.50394994e-01 ... 8.28158677e-01\n",
      "    8.92942309e-01 4.84428734e-01]\n",
      "   [4.52171117e-01 2.67419428e-01 2.85376906e-01 ... 4.03962821e-01\n",
      "    8.26062381e-01 0.00000000e+00]\n",
      "   ...\n",
      "   [1.47940278e-01 3.63793701e-01 2.76857555e-01 ... 5.15581489e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [5.59965134e-01 3.67233664e-01 4.24294829e-01 ... 2.21923813e-02\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.81449705e-02 2.36172438e-01 1.40959740e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 5.11490107e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 3.71824265e-01 ... 8.17978442e-01\n",
      "    1.80885828e+00 2.24529243e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 4.17194813e-01\n",
      "    3.63150567e-01 3.38334590e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.82212186e+00\n",
      "    8.32492352e-01 0.00000000e+00]\n",
      "   ...\n",
      "   [5.82398698e-02 0.00000000e+00 4.98473823e-01 ... 1.70949727e-01\n",
      "    1.43480331e-01 4.43244994e-01]\n",
      "   [6.31658077e-01 8.21632743e-01 4.91754830e-01 ... 4.26747680e-01\n",
      "    7.60734677e-01 1.07546175e+00]\n",
      "   [1.38318646e+00 1.80596697e+00 1.38700521e+00 ... 1.15266669e+00\n",
      "    1.38538337e+00 1.05329156e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    4.05795157e-01 6.93112537e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 5.08140028e-03 ... 8.28860104e-01\n",
      "    6.71398342e-01 4.24551308e-01]\n",
      "   [2.37038583e-01 2.66557157e-01 7.67712116e-01 ... 1.28425062e+00\n",
      "    6.20516181e-01 7.44941652e-01]\n",
      "   ...\n",
      "   [1.28428531e+00 1.71725392e+00 2.63035703e+00 ... 9.47679356e-02\n",
      "    1.71543312e+00 2.60262108e+00]\n",
      "   [4.23057854e-01 4.77047682e-01 2.49762565e-01 ... 3.00439894e-01\n",
      "    1.03176641e+00 1.88595200e+00]\n",
      "   [2.13334799e-01 2.16335177e-01 5.28582335e-02 ... 8.56212750e-02\n",
      "    3.50161821e-01 5.17043710e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 9.39932585e-01\n",
      "    6.80951595e-01 5.89854419e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.13200974e+00 ... 2.40572834e+00\n",
      "    1.32500386e+00 1.03432345e+00]\n",
      "   [0.00000000e+00 1.44011855e+00 2.27281260e+00 ... 2.27078414e+00\n",
      "    2.13808298e+00 1.34902859e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 5.25383890e-01 2.01880619e-01 ... 4.03698981e-01\n",
      "    2.20085573e+00 1.63214183e+00]\n",
      "   [1.23313457e-01 5.78132629e-01 2.66762584e-01 ... 9.25375879e-01\n",
      "    1.99617028e+00 2.67324305e+00]\n",
      "   [2.04199657e-01 3.50903392e-01 3.99000466e-01 ... 1.30082321e+00\n",
      "    2.50113869e+00 2.49756765e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 3.57371211e-01 ... 6.84313953e-01\n",
      "    2.41114125e-01 7.52918795e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.30936259e-01 2.30697617e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [2.16817528e-01 3.05611670e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [5.49965680e-01 4.94750202e-01 3.30639452e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [8.04594904e-02 4.32615578e-01 2.35643566e-01 ... 2.75337100e-02\n",
      "    4.41406369e-02 4.24710885e-02]]]\n",
      "\n",
      "\n",
      " [[[3.08311105e-01 4.67601895e-01 1.28563380e+00 ... 8.15824270e-01\n",
      "    4.21639591e-01 0.00000000e+00]\n",
      "   [1.38164490e-01 4.15342003e-01 1.41120648e+00 ... 2.43644357e+00\n",
      "    1.31208622e+00 0.00000000e+00]\n",
      "   [6.74209177e-01 5.55548549e-01 1.17691088e+00 ... 1.85592294e+00\n",
      "    1.27894783e+00 9.69088137e-01]\n",
      "   ...\n",
      "   [5.50723910e-01 6.16761506e-01 8.43856215e-01 ... 1.17314231e+00\n",
      "    1.11635637e+00 1.12208581e+00]\n",
      "   [0.00000000e+00 1.69388592e-01 8.37428212e-01 ... 1.38837886e+00\n",
      "    7.15738595e-01 7.83449709e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 2.33021423e-01 ... 6.10590339e-01\n",
      "    7.39122033e-01 8.26473475e-01]]\n",
      "\n",
      "  [[1.62545443e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.28201321e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.51437259e-01\n",
      "    0.00000000e+00 1.20889768e-01]\n",
      "   [0.00000000e+00 8.59433264e-02 0.00000000e+00 ... 9.56437111e-01\n",
      "    9.85726476e-01 5.70361674e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 2.08544433e-02 0.00000000e+00 ... 1.23807144e+00\n",
      "    2.44618320e+00 1.87699485e+00]\n",
      "   [0.00000000e+00 1.14667639e-02 0.00000000e+00 ... 9.12477493e-01\n",
      "    1.30541301e+00 1.70544493e+00]\n",
      "   [0.00000000e+00 1.12359002e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.31950021e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 2.46994182e-01 ... 8.79548967e-01\n",
      "    5.43784022e-01 1.28098536e+00]\n",
      "   [2.04045758e-01 5.82501292e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    2.22330138e-01 1.12591815e+00]\n",
      "   [1.36692011e+00 2.17959785e+00 4.79392946e-01 ... 1.68006444e+00\n",
      "    0.00000000e+00 5.73313892e-01]\n",
      "   ...\n",
      "   [1.33411527e+00 1.55579269e+00 1.66354132e+00 ... 1.08000529e+00\n",
      "    0.00000000e+00 1.09450471e+00]\n",
      "   [2.33779502e+00 2.04365206e+00 1.39796531e+00 ... 4.85901594e-01\n",
      "    1.31109953e+00 8.49975288e-01]\n",
      "   [2.93809462e+00 2.61127996e+00 2.13953137e+00 ... 1.29008079e+00\n",
      "    9.39722300e-01 9.54101324e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 8.43943059e-02 ... 2.95103550e-01\n",
      "    1.49155438e-01 2.82258809e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.83111623e-01 ... 5.84541202e-01\n",
      "    2.33394369e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 9.75085273e-02 ... 4.55659002e-01\n",
      "    3.99460375e-01 4.21872258e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.36411881e+00\n",
      "    2.09653354e+00 1.75620294e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.19875908e+00\n",
      "    1.56647706e+00 1.46491563e+00]\n",
      "   [0.00000000e+00 6.82684183e-02 0.00000000e+00 ... 3.63968551e-01\n",
      "    2.27215201e-01 7.74990544e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 7.23129153e-01 ... 1.48661339e+00\n",
      "    8.00047100e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 1.41031623e+00 ... 2.53235006e+00\n",
      "    1.81141210e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 1.27005184e+00 ... 2.50580168e+00\n",
      "    1.23961759e+00 2.72799253e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 8.33931923e-01 ... 1.78024554e+00\n",
      "    1.46394968e+00 1.42124391e+00]\n",
      "   [0.00000000e+00 1.73850283e-01 1.29851186e+00 ... 2.93291187e+00\n",
      "    1.90575743e+00 1.48006546e+00]\n",
      "   [9.20778289e-02 2.51739204e-01 1.17480588e+00 ... 2.15759230e+00\n",
      "    6.17497444e-01 4.52639580e-01]]\n",
      "\n",
      "  [[1.32165954e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [9.58885550e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 1.27338886e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 6.62929192e-02]\n",
      "   ...\n",
      "   [0.00000000e+00 5.90430200e-03 1.06328011e+00 ... 1.28704533e-02\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.68629134e-01 1.20530963e-01 4.80680019e-01 ... 4.03786719e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 3.65166552e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[6.68483198e-01 4.39019442e-01 1.12000751e+00 ... 4.76623476e-01\n",
      "    1.33359289e+00 1.91229832e+00]\n",
      "   [6.07434392e-01 1.17891598e+00 1.68001723e+00 ... 4.48530734e-01\n",
      "    1.24529886e+00 2.05590415e+00]\n",
      "   [2.63958782e-01 1.64076418e-01 5.74326336e-01 ... 1.25434756e+00\n",
      "    9.16179299e-01 7.98750579e-01]\n",
      "   ...\n",
      "   [1.01892635e-01 0.00000000e+00 0.00000000e+00 ... 3.22143048e-01\n",
      "    7.31466055e-01 8.15877259e-01]\n",
      "   [1.50505260e-01 3.68140340e-02 0.00000000e+00 ... 3.48629504e-01\n",
      "    4.93965745e-01 9.06628728e-01]\n",
      "   [5.52745283e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 7.12876618e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.31761187e-02\n",
      "    1.13701500e-01 3.28787476e-01]\n",
      "   [0.00000000e+00 1.51005760e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 3.60027224e-01]\n",
      "   [0.00000000e+00 1.16035320e-01 0.00000000e+00 ... 1.78781062e-01\n",
      "    6.77643895e-01 7.53069758e-01]\n",
      "   ...\n",
      "   [1.65085435e-01 2.51179129e-01 5.78583002e-01 ... 4.99597669e-01\n",
      "    9.45178270e-01 1.70935893e+00]\n",
      "   [2.65333086e-01 1.25769064e-01 3.73002410e-01 ... 2.52805978e-01\n",
      "    7.24189639e-01 7.62823820e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.57758832e-01 ... 2.28836328e-01\n",
      "    2.02226475e-01 9.91339833e-02]]\n",
      "\n",
      "  [[1.33058929e+00 4.64252383e-01 2.91798651e-01 ... 2.17452139e-01\n",
      "    4.92613837e-02 7.15993106e-01]\n",
      "   [2.40935564e+00 1.75224149e+00 1.41962039e+00 ... 3.57364714e-01\n",
      "    5.98174751e-01 4.21549022e-01]\n",
      "   [2.92331719e+00 2.96303368e+00 2.85499191e+00 ... 7.74531662e-01\n",
      "    4.02741492e-01 5.38182735e-01]\n",
      "   ...\n",
      "   [6.66990340e-01 1.40589178e+00 2.17982483e+00 ... 1.14671636e+00\n",
      "    1.83167934e-01 0.00000000e+00]\n",
      "   [3.72757375e-01 6.87546790e-01 1.17240143e+00 ... 1.88679188e-01\n",
      "    7.10223794e-01 1.23913646e-01]\n",
      "   [2.69992232e-01 4.97173369e-01 9.08103943e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[4.63700116e-01 6.79762483e-01 7.77264357e-01 ... 6.24281764e-01\n",
      "    2.67217368e-01 3.15150201e-01]\n",
      "   [7.15787768e-01 7.40884781e-01 1.06016672e+00 ... 7.14191914e-01\n",
      "    1.72006205e-01 2.31433868e-01]\n",
      "   [5.68203151e-01 4.42882091e-01 1.20000935e+00 ... 1.97087204e+00\n",
      "    1.16286516e+00 6.40314937e-01]\n",
      "   ...\n",
      "   [6.15791380e-01 8.55128765e-01 1.49193287e+00 ... 1.77143693e+00\n",
      "    2.06174302e+00 2.41811132e+00]\n",
      "   [2.10821226e-01 5.55849135e-01 5.11231720e-01 ... 1.22187781e+00\n",
      "    2.03863335e+00 2.11085081e+00]\n",
      "   [2.58380175e-01 1.12262040e-01 0.00000000e+00 ... 8.13515186e-01\n",
      "    1.04578888e+00 1.31900203e+00]]\n",
      "\n",
      "  [[0.00000000e+00 3.19620073e-01 1.94827795e-01 ... 1.25114679e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.00410320e-01 7.10070491e-01 1.47124565e+00 ... 1.39903736e+00\n",
      "    6.47111297e-01 0.00000000e+00]\n",
      "   [1.77984476e-01 1.20423210e+00 1.35761094e+00 ... 7.98458278e-01\n",
      "    4.04058620e-02 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 7.84592986e-01 ... 6.30109429e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 6.16878793e-02 3.69900525e-01 ... 9.84471917e-01\n",
      "    3.38212103e-01 3.54107499e-01]\n",
      "   [3.02089870e-01 2.42767304e-01 0.00000000e+00 ... 2.03644693e-01\n",
      "    2.12043703e-01 1.41899779e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    2.79395312e-01 4.83497202e-01]\n",
      "   [0.00000000e+00 4.58535194e-01 8.49505186e-01 ... 3.91073823e-01\n",
      "    7.92993784e-01 8.97004724e-01]\n",
      "   [0.00000000e+00 6.85269594e-01 1.37494230e+00 ... 1.49164760e+00\n",
      "    1.00909555e+00 7.72499859e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 5.17591894e-01 ... 9.29461181e-01\n",
      "    8.08314741e-01 4.17875320e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.60397112e-01 ... 2.82987148e-01\n",
      "    4.73607689e-01 4.52244908e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.92293894e+00 1.67794645e+00 2.31160545e+00 ... 9.65017676e-01\n",
      "    4.18456793e-01 6.55493617e-01]\n",
      "   [1.63061380e+00 1.03211665e+00 2.35491730e-02 ... 7.03292668e-01\n",
      "    9.18900371e-01 7.71851420e-01]\n",
      "   [2.37527907e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.11182451e+00 1.91644323e+00]\n",
      "   ...\n",
      "   [4.76732492e-01 9.87633228e-01 6.14467263e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 6.55286729e-01]\n",
      "   [5.35329163e-01 5.38629174e-01 4.71766710e-01 ... 0.00000000e+00\n",
      "    2.54850894e-01 2.33008593e-01]\n",
      "   [1.43650234e-01 4.00627293e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    3.33945125e-01 4.05098826e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 2.77005970e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 3.03915516e-03 3.46431941e-01 ... 4.23119158e-01\n",
      "    5.34651279e-01 1.70485675e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 4.95039672e-03 1.76289827e-01 ... 8.50831568e-02\n",
      "    2.60670185e-01 1.01736002e-01]\n",
      "   [3.81345004e-02 7.41109401e-02 1.99498147e-01 ... 1.93515360e-01\n",
      "    5.66422641e-01 4.17807460e-01]\n",
      "   [1.41694307e-01 1.55912533e-01 2.53138363e-01 ... 5.17708302e-01\n",
      "    5.70637763e-01 3.78429145e-01]]\n",
      "\n",
      "  [[1.01565552e+00 8.32855284e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.30835801e-01 1.09472647e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.92887872e-01 5.65915525e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.50809395e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.29441455e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.74531043e-01 0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.00804508e+00 7.14434385e-01 8.65297616e-01 ... 7.90163159e-01\n",
      "    3.25061351e-01 1.32684380e-01]\n",
      "   [1.51942825e+00 1.41663158e+00 8.04132819e-01 ... 7.75298238e-01\n",
      "    4.05940235e-01 7.23201483e-02]\n",
      "   [1.43620706e+00 1.67145526e+00 1.34305620e+00 ... 9.84311938e-01\n",
      "    6.62138164e-01 3.57169509e-01]\n",
      "   ...\n",
      "   [1.00110531e+00 1.14945650e+00 6.80217862e-01 ... 1.17156696e+00\n",
      "    1.05967557e+00 8.50387454e-01]\n",
      "   [1.01760209e-01 3.86284620e-01 2.31625021e-01 ... 5.51944196e-01\n",
      "    4.07724798e-01 3.98483127e-01]\n",
      "   [0.00000000e+00 2.71109283e-01 0.00000000e+00 ... 1.34220824e-01\n",
      "    3.40287127e-02 6.55303523e-02]]\n",
      "\n",
      "  [[5.49776435e-01 4.97630179e-01 8.74597490e-01 ... 1.58065677e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [9.54586864e-01 1.38021374e+00 1.42346704e+00 ... 9.51221824e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.68227720e+00 2.43496656e+00 2.29343891e+00 ... 1.82934046e+00\n",
      "    3.13111216e-01 0.00000000e+00]\n",
      "   ...\n",
      "   [1.38209951e+00 1.38000321e+00 1.76114988e+00 ... 1.38127446e+00\n",
      "    6.80207789e-01 4.76977527e-01]\n",
      "   [3.90017509e-01 5.22849441e-01 9.76838291e-01 ... 7.90501356e-01\n",
      "    1.43952465e+00 2.66251087e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 3.60567898e-01 ... 5.85091472e-01\n",
      "    6.23385072e-01 4.14230675e-01]]\n",
      "\n",
      "  [[7.58640319e-02 2.31871098e-01 7.56582201e-01 ... 4.68957990e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.04036915e-01 8.59527946e-01 5.84300160e-01 ... 3.86227071e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.11702931e+00 8.09094787e-01 4.72337365e-01 ... 3.92999411e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [5.57452627e-02 7.81106725e-02 8.17939043e-01 ... 1.96712643e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 4.36407886e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[0.00000000e+00 0.00000000e+00 2.72853017e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.02375464e-03 0.00000000e+00 6.52940869e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [1.78334475e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 2.20589682e-01]]\n",
      "\n",
      "  [[0.00000000e+00 4.44128186e-01 2.09958553e-01 ... 1.74027070e-01\n",
      "    2.06720561e-01 3.19311261e-01]\n",
      "   [9.75349993e-02 2.92502016e-01 2.89243937e-01 ... 5.68485379e-01\n",
      "    5.81588149e-01 6.02758169e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.02019310e-03 ... 3.67089927e-01\n",
      "    1.02395880e+00 6.27715051e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    4.65879962e-02 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.47003233e-01 5.71110725e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    3.64991516e-01 5.22359014e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [2.80735731e-01 5.33427298e-01 1.06724954e+00 ... 1.40839666e-01\n",
      "    1.01785794e-01 3.13669086e-01]\n",
      "   [5.27213871e-01 7.77090311e-01 9.70631719e-01 ... 5.44797242e-01\n",
      "    1.39705014e+00 7.39788711e-01]\n",
      "   [1.13060355e+00 5.48880696e-01 6.10890269e-01 ... 6.78240776e-01\n",
      "    6.06182456e-01 1.61687970e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.30052269e-02 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.78554225e-01 4.87097591e-01 1.91642508e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 6.82397366e-01 1.09624219e+00 ... 4.55925316e-01\n",
      "    8.16410929e-02 0.00000000e+00]\n",
      "   [0.00000000e+00 3.56455147e-01 8.96598339e-01 ... 6.28480554e-01\n",
      "    2.99769372e-01 1.03685133e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.20779485e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 9.13514495e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 2.21169144e-02 ... 5.55452526e-01\n",
      "    2.80520469e-01 2.30509773e-01]\n",
      "   [0.00000000e+00 1.69242471e-02 9.44500089e-01 ... 2.33971715e+00\n",
      "    1.18189573e+00 0.00000000e+00]\n",
      "   [9.15339068e-02 6.76165938e-01 8.10533762e-01 ... 1.46623933e+00\n",
      "    1.57276964e+00 4.19956177e-01]\n",
      "   ...\n",
      "   [3.11536074e-01 8.10814321e-01 1.53313112e+00 ... 1.23163712e+00\n",
      "    7.04808176e-01 3.35781097e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 7.80828178e-01 ... 6.22624755e-01\n",
      "    1.54208392e-01 8.29945356e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 4.32011247e-01\n",
      "    4.62323010e-01 4.99602318e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 1.45182073e-01 ... 9.21013057e-01\n",
      "    6.14135265e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 5.20670772e-01 ... 1.78903174e+00\n",
      "    1.55274844e+00 8.10216069e-01]\n",
      "   [5.81919439e-02 5.62223136e-01 4.89252955e-01 ... 7.68592775e-01\n",
      "    8.24917793e-01 6.21446252e-01]\n",
      "   ...\n",
      "   [9.35361028e-01 5.48255146e-01 1.45042345e-01 ... 1.16885155e-01\n",
      "    3.95377517e-01 0.00000000e+00]\n",
      "   [6.14076495e-01 4.21914548e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.12726867e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[3.94280732e-01 3.58180135e-01 1.40541121e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 3.64718288e-02]\n",
      "   [2.53408134e-01 1.37533218e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [9.59705710e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 1.11210588e-02 ... 4.67185110e-01\n",
      "    1.53894305e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.53406034e-02\n",
      "    3.27407062e-01 6.99248016e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    5.92668653e-01 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 3.24569970e-01 4.73426282e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 4.57489192e-02 2.20505036e-02 ... 0.00000000e+00\n",
      "    7.40706846e-02 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 5.36426492e-02 ... 5.38506657e-02\n",
      "    1.27461582e-01 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.78010261e+00\n",
      "    5.84575415e-01 0.00000000e+00]\n",
      "   [1.58521570e-02 0.00000000e+00 2.26549476e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [8.98221552e-01 6.87699676e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [6.14572763e-01 9.29766536e-01 7.62979686e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.57641932e-01 4.64573145e-01 7.86782324e-01 ... 9.22655880e-01\n",
      "    1.12731123e+00 8.88088286e-01]\n",
      "   [4.52083558e-01 8.71334910e-01 6.95683837e-01 ... 1.13879800e+00\n",
      "    1.51644254e+00 1.50677180e+00]\n",
      "   [6.33504093e-01 8.24636698e-01 4.75157619e-01 ... 9.41402137e-01\n",
      "    1.41643822e+00 1.25277948e+00]\n",
      "   ...\n",
      "   [5.20292401e-01 3.70451361e-01 4.50017035e-01 ... 0.00000000e+00\n",
      "    4.27562892e-01 1.44854888e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    3.46369445e-02 1.75429344e-01]\n",
      "   [0.00000000e+00 3.24904382e-01 6.06723070e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 2.47663692e-01]]\n",
      "\n",
      "  [[7.36375868e-01 9.21661735e-01 1.27530575e+00 ... 8.32356930e-01\n",
      "    8.26085210e-01 5.17887771e-01]\n",
      "   [7.93586910e-01 1.47071767e+00 1.59354615e+00 ... 1.43937898e+00\n",
      "    1.01216745e+00 7.92087764e-02]\n",
      "   [7.77637243e-01 1.47613287e+00 1.83377373e+00 ... 1.61594558e+00\n",
      "    6.42657816e-01 1.22177213e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 2.91505396e-01 1.43851066e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 1.35543033e-01 8.86865035e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 5.23117036e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[7.35271096e-01 6.69183850e-01 7.39968061e-01 ... 1.95713595e-01\n",
      "    2.39799470e-01 7.87490487e-01]\n",
      "   [3.85534704e-01 4.34981316e-01 2.01020211e-01 ... 5.85054934e-01\n",
      "    1.43464729e-01 5.17304279e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 3.97029109e-02 ... 1.18265301e-01\n",
      "    7.20391795e-02 3.29290554e-02]\n",
      "   ...\n",
      "   [1.72996759e-01 4.96001959e-01 5.12904339e-02 ... 1.90125927e-01\n",
      "    2.86213122e-02 0.00000000e+00]\n",
      "   [0.00000000e+00 1.08692683e-02 9.40324664e-02 ... 6.93362832e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 4.46604609e-01\n",
      "    1.00329965e-01 0.00000000e+00]]]] (37322, 2048, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "for i_batch, imgs in enumerate(dataset_loader):\n",
    "    print(i_batch)\n",
    "    #pdb.set_trace()\n",
    "    #imgs = imgs.to(device)\n",
    "    features = model_f(imgs)\n",
    "    all_features.append(features.numpy())\n",
    "    \n",
    "all_features = np.concatenate(all_features, axis=0)\n",
    "print(all_features, all_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee639a91",
   "metadata": {},
   "source": [
    "# Let's extract semantic attributes of each category (consider pre-trained word2vec model with no fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566d27b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n",
      "Done replacing OOD words\n",
      "Done preprocessing attribute des\n"
     ]
    }
   ],
   "source": [
    "print('Load pretrain w2v model')\n",
    "\n",
    "model_name = 'word2vec-google-news-300'#best model\n",
    "model = api.load(model_name)\n",
    "\n",
    "dim_w2v = 300\n",
    "\n",
    "#%%\n",
    "replace_word = [('newworld','new world'),('oldworld','old world'),('nestspot','nest spot'),('toughskin','tough skin'),\n",
    "                ('longleg','long leg'),('chewteeth','chew teeth'),('meatteeth','meat teeth'),('strainteeth','strain teeth'),\n",
    "                ('quadrapedal','quadrupedal')]  # for AWA2\n",
    "\n",
    "\n",
    "#For AWA2\n",
    "path = 'C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/attribute/predicates.txt'\n",
    "df=pd.read_csv(path,sep='\\t',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/attribute/new_des.csv')\n",
    "print('Done preprocessing attribute des')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3426bda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n",
      "white\n",
      "blue\n",
      "brown\n",
      "gray\n",
      "orange\n",
      "red\n",
      "yellow\n",
      "patches\n",
      "spots\n",
      "stripes\n",
      "furry\n",
      "hairless\n",
      "tough skin\n",
      "big\n",
      "small\n",
      "bulbous\n",
      "lean\n",
      "flippers\n",
      "hands\n",
      "hooves\n",
      "pads\n",
      "paws\n",
      "long leg\n",
      "longneck\n",
      "tail\n",
      "chew teeth\n",
      "meat teeth\n",
      "buckteeth\n",
      "strain teeth\n",
      "horns\n",
      "claws\n",
      "tusks\n",
      "smelly\n",
      "flys\n",
      "hops\n",
      "swims\n",
      "tunnels\n",
      "walks\n",
      "fast\n",
      "slow\n",
      "strong\n",
      "weak\n",
      "muscle\n",
      "bipedal\n",
      "quadrupedal\n",
      "active\n",
      "inactive\n",
      "nocturnal\n",
      "hibernate\n",
      "agility\n",
      "fish\n",
      "meat\n",
      "plankton\n",
      "vegetation\n",
      "insects\n",
      "forager\n",
      "grazer\n",
      "hunter\n",
      "scavenger\n",
      "skimmer\n",
      "stalker\n",
      "new world\n",
      "old world\n",
      "arctic\n",
      "coastal\n",
      "desert\n",
      "bush\n",
      "plains\n",
      "forest\n",
      "fields\n",
      "jungle\n",
      "mountains\n",
      "ocean\n",
      "ground\n",
      "water\n",
      "tree\n",
      "cave\n",
      "fierce\n",
      "timid\n",
      "smart\n",
      "group\n",
      "solitary\n",
      "nest spot\n",
      "domestic\n",
      "counter_err  0\n"
     ]
    }
   ],
   "source": [
    "counter_err = 0\n",
    "\n",
    "all_w2v = []\n",
    "for s in des:\n",
    "    print(s)\n",
    "    words = s.split(' ')\n",
    "    if words[-1] == '':     #remove empty element\n",
    "        words = words[:-1]\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    for w in words:\n",
    "        try:\n",
    "            w2v += model[w]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            counter_err += 1\n",
    "    all_w2v.append(w2v[np.newaxis,:])\n",
    "    \n",
    "print('counter_err ',counter_err)\n",
    "\n",
    "#%%\n",
    "all_w2v=np.concatenate(all_w2v,axis=0)\n",
    "#pdb.set_trace()\n",
    "#%%\n",
    "\n",
    "with open('C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/w2v/AWA2_attribute.pkl','wb') as f:\n",
    "    pickle.dump(all_w2v,f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02655a5",
   "metadata": {},
   "source": [
    "# Read the attributes and save as \"w2v_att\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9433e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save w2v_att\n",
      "[[ 0.10498047  0.01843262  0.00897217 ...  0.09228516  0.06103516\n",
      "  -0.1328125 ]\n",
      " [ 0.02697754  0.06933594  0.02416992 ...  0.06933594  0.06982422\n",
      "  -0.03369141]\n",
      " [ 0.0390625   0.08642578  0.22363281 ...  0.04663086  0.02258301\n",
      "  -0.15722656]\n",
      " ...\n",
      " [ 0.21289062  0.15039062 -0.14746094 ... -0.18652344 -0.10351562\n",
      "   0.24609375]\n",
      " [-0.04574585  0.37304688 -0.51708984 ... -0.26953125  0.06518555\n",
      "   0.38085938]\n",
      " [-0.29492188  0.15039062 -0.1484375  ...  0.11572266  0.07519531\n",
      "   0.00210571]] (85, 300)\n"
     ]
    }
   ],
   "source": [
    "attribute_path = 'C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/w2v/AWA2_attribute.pkl'\n",
    "\n",
    "with open(attribute_path,'rb') as f:\n",
    "    w2v_att = pickle.load(f)\n",
    "assert w2v_att.shape == (85,300) # for AWA2\n",
    "print('save w2v_att')\n",
    "\n",
    "print(w2v_att, w2v_att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633a585",
   "metadata": {},
   "source": [
    "# Let's gather additional information (training, validation, and test indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5944826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Sushree/Jio_Institute/Dataset/data/xlsa17/data/AWA2/att_splits.mat\n"
     ]
    }
   ],
   "source": [
    "#%% get remaining metadata\n",
    "matcontent = AWA2Dataset.matcontent\n",
    "labels = matcontent['labels'].astype(int).squeeze() - 1\n",
    "\n",
    "split_path = 'C:/Sushree/Jio_Institute/Dataset/data/xlsa17/data/AWA2/att_splits.mat'\n",
    "print(split_path)\n",
    "    \n",
    "#att_splits.mat includes the following fields:\n",
    "#-att: columns correpond to class attribute vectors normalized to have unit l2 norm, following the classes order in allclasses.txt \n",
    "#-original_att: the original class attribute vectors without normalization\n",
    "#-trainval_loc: instances indexes of train+val set features (for only seen classes) in resNet101.mat\n",
    "#-test_seen_loc: instances indexes of test set features for seen classes\n",
    "#-test_unseen_loc: instances indexes of test set features for unseen classes    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03737527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Fri Aug 21 10:36:20 2020', '__version__': '1.0', '__globals__': [], 'allclasses_names': array([[array(['antelope'], dtype='<U8')],\n",
      "       [array(['grizzly+bear'], dtype='<U12')],\n",
      "       [array(['killer+whale'], dtype='<U12')],\n",
      "       [array(['beaver'], dtype='<U6')],\n",
      "       [array(['dalmatian'], dtype='<U9')],\n",
      "       [array(['persian+cat'], dtype='<U11')],\n",
      "       [array(['horse'], dtype='<U5')],\n",
      "       [array(['german+shepherd'], dtype='<U15')],\n",
      "       [array(['blue+whale'], dtype='<U10')],\n",
      "       [array(['siamese+cat'], dtype='<U11')],\n",
      "       [array(['skunk'], dtype='<U5')],\n",
      "       [array(['mole'], dtype='<U4')],\n",
      "       [array(['tiger'], dtype='<U5')],\n",
      "       [array(['hippopotamus'], dtype='<U12')],\n",
      "       [array(['leopard'], dtype='<U7')],\n",
      "       [array(['moose'], dtype='<U5')],\n",
      "       [array(['spider+monkey'], dtype='<U13')],\n",
      "       [array(['humpback+whale'], dtype='<U14')],\n",
      "       [array(['elephant'], dtype='<U8')],\n",
      "       [array(['gorilla'], dtype='<U7')],\n",
      "       [array(['ox'], dtype='<U2')],\n",
      "       [array(['fox'], dtype='<U3')],\n",
      "       [array(['sheep'], dtype='<U5')],\n",
      "       [array(['seal'], dtype='<U4')],\n",
      "       [array(['chimpanzee'], dtype='<U10')],\n",
      "       [array(['hamster'], dtype='<U7')],\n",
      "       [array(['squirrel'], dtype='<U8')],\n",
      "       [array(['rhinoceros'], dtype='<U10')],\n",
      "       [array(['rabbit'], dtype='<U6')],\n",
      "       [array(['bat'], dtype='<U3')],\n",
      "       [array(['giraffe'], dtype='<U7')],\n",
      "       [array(['wolf'], dtype='<U4')],\n",
      "       [array(['chihuahua'], dtype='<U9')],\n",
      "       [array(['rat'], dtype='<U3')],\n",
      "       [array(['weasel'], dtype='<U6')],\n",
      "       [array(['otter'], dtype='<U5')],\n",
      "       [array(['buffalo'], dtype='<U7')],\n",
      "       [array(['zebra'], dtype='<U5')],\n",
      "       [array(['giant+panda'], dtype='<U11')],\n",
      "       [array(['deer'], dtype='<U4')],\n",
      "       [array(['bobcat'], dtype='<U6')],\n",
      "       [array(['pig'], dtype='<U3')],\n",
      "       [array(['lion'], dtype='<U4')],\n",
      "       [array(['mouse'], dtype='<U5')],\n",
      "       [array(['polar+bear'], dtype='<U10')],\n",
      "       [array(['collie'], dtype='<U6')],\n",
      "       [array(['walrus'], dtype='<U6')],\n",
      "       [array(['raccoon'], dtype='<U7')],\n",
      "       [array(['cow'], dtype='<U3')],\n",
      "       [array(['dolphin'], dtype='<U7')]], dtype=object), 'att': array([[-0.00375358,  0.12045618,  0.26584459, ...,  0.22516498,\n",
      "         0.19613947,  0.03819588],\n",
      "       [-0.00375358,  0.00426584,  0.20652363, ...,  0.15266022,\n",
      "         0.1966714 ,  0.08046548],\n",
      "       [-0.00375358,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.10363715],\n",
      "       ...,\n",
      "       [ 0.00882092,  0.17996306,  0.05026822, ...,  0.12733492,\n",
      "         0.01787277,  0.01479997],\n",
      "       [ 0.03640974,  0.0618086 ,  0.04274552, ...,  0.10009694,\n",
      "         0.06698743,  0.05250999],\n",
      "       [ 0.03145501,  0.03495531,  0.04915256, ...,  0.01771   ,\n",
      "         0.25883601,  0.14194515]]), 'original_att': array([[-1.  , 39.25, 83.4 , ..., 63.57, 55.31, 10.22],\n",
      "       [-1.  ,  1.39, 64.79, ..., 43.1 , 55.46, 21.53],\n",
      "       [-1.  ,  0.  ,  0.  , ...,  0.  ,  0.  , 27.73],\n",
      "       ...,\n",
      "       [ 2.35, 58.64, 15.77, ..., 35.95,  5.04,  3.96],\n",
      "       [ 9.7 , 20.14, 13.41, ..., 28.26, 18.89, 14.05],\n",
      "       [ 8.38, 11.39, 15.42, ...,  5.  , 72.99, 37.98]]), 'test_seen_loc': array([[11061],\n",
      "       [ 6271],\n",
      "       [17298],\n",
      "       ...,\n",
      "       [ 4014],\n",
      "       [20615],\n",
      "       [33202]], dtype=uint16), 'test_unseen_loc': array([[ 1047],\n",
      "       [ 1048],\n",
      "       [ 1049],\n",
      "       ...,\n",
      "       [35289],\n",
      "       [35290],\n",
      "       [35291]], dtype=uint16), 'train_loc': array([[21483],\n",
      "       [11453],\n",
      "       [21449],\n",
      "       ...,\n",
      "       [19616],\n",
      "       [10389],\n",
      "       [ 5161]], dtype=uint16), 'trainval_loc': array([[21483],\n",
      "       [11453],\n",
      "       [21449],\n",
      "       ...,\n",
      "       [ 7993],\n",
      "       [10389],\n",
      "       [ 5161]], dtype=uint16), 'val_loc': array([[12751],\n",
      "       [15468],\n",
      "       [ 8275],\n",
      "       ...,\n",
      "       [28510],\n",
      "       [20800],\n",
      "       [ 7993]], dtype=uint16)}\n",
      "[21482 11452 21448 ...  7992 10388  5160] 23527\n",
      "[11060  6270 17297 ...  4013 20614 33201] 5882\n",
      "[ 1046  1047  1048 ... 35288 35289 35290] 7913\n",
      "[[-0.00375358 -0.00375358 -0.00375358 ...  0.00882092  0.03640974\n",
      "   0.03145501]\n",
      " [ 0.12045618  0.00426584  0.         ...  0.17996306  0.0618086\n",
      "   0.03495531]\n",
      " [ 0.26584459  0.20652363  0.         ...  0.05026822  0.04274552\n",
      "   0.04915256]\n",
      " ...\n",
      " [ 0.22516498  0.15266022  0.         ...  0.12733492  0.10009694\n",
      "   0.01771   ]\n",
      " [ 0.19613947  0.1966714   0.         ...  0.01787277  0.06698743\n",
      "   0.25883601]\n",
      " [ 0.03819588  0.08046548  0.10363715 ...  0.01479997  0.05250999\n",
      "   0.14194515]] (50, 85)\n",
      "[[-1.   -1.   -1.   ...  2.35  9.7   8.38]\n",
      " [39.25  1.39  0.   ... 58.64 20.14 11.39]\n",
      " [83.4  64.79  0.   ... 15.77 13.41 15.42]\n",
      " ...\n",
      " [63.57 43.1   0.   ... 35.95 28.26  5.  ]\n",
      " [55.31 55.46  0.   ...  5.04 18.89 72.99]\n",
      " [10.22 21.53 27.73 ...  3.96 14.05 37.98]] (50, 85)\n"
     ]
    }
   ],
   "source": [
    "def get_index_details(split_path):\n",
    "    matcontent = sio.loadmat(split_path)\n",
    "    print(matcontent)\n",
    "    \n",
    "    trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "    print(trainval_loc, len(trainval_loc))\n",
    "\n",
    "    test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "    print(test_seen_loc, len(test_seen_loc))\n",
    "\n",
    "    test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "    print(test_unseen_loc, len(test_unseen_loc))\n",
    "    \n",
    "    att = matcontent['att'].T\n",
    "    print(att, att.shape)\n",
    "    \n",
    "    original_att = matcontent['original_att'].T\n",
    "    print(original_att, original_att.shape)\n",
    "    return trainval_loc, test_seen_loc, test_unseen_loc, att, original_att\n",
    "    \n",
    "trainval_loc, test_seen_loc, test_unseen_loc, att, original_att = get_index_details(split_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea517643",
   "metadata": {},
   "source": [
    "# Save the feature map that includes ResNet50 features, labels, training and test (seen and unseen) data indexes, semantic attributes, and w2v attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a540d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = 'C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/feature_map_ResNet_101_AWA2.hdf5'\n",
    "\n",
    "f = h5py.File(save_path, \"w\")\n",
    "f.create_dataset('feature_map', data=all_features,compression=\"gzip\")\n",
    "f.create_dataset('labels', data=labels,compression=\"gzip\")\n",
    "f.create_dataset('trainval_loc', data=trainval_loc,compression=\"gzip\")\n",
    "#    f.create_dataset('train_loc', data=train_loc,compression=\"gzip\")\n",
    "#    f.create_dataset('val_unseen_loc', data=val_unseen_loc,compression=\"gzip\")\n",
    "f.create_dataset('test_seen_loc', data=test_seen_loc,compression=\"gzip\")\n",
    "f.create_dataset('test_unseen_loc', data=test_unseen_loc,compression=\"gzip\")\n",
    "f.create_dataset('att', data=att,compression=\"gzip\")\n",
    "f.create_dataset('original_att', data=original_att,compression=\"gzip\")\n",
    "f.create_dataset('w2v_att', data=w2v_att,compression=\"gzip\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/feature_map_ResNet_101_AWA2.hdf5', 'r')\n",
    "features = np.array(hf.get('feature_map'))\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb52765",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = np.array(hf.get('att'))\n",
    "print(att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647068b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86e6b3",
   "metadata": {},
   "source": [
    "# Train the DAZLE model for AWA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "046aff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.resnet as models\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import torch.optim as optim\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae3b3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DAZLE import DAZLE\n",
    "from AWA2DataLoader import AWA2DataLoader\n",
    "from helper_func import eval_zs_gzsl,visualize_attention#,get_attribute_attention_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b657e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Sushree/Jio_Institute/Dataset/\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "AWA2\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Balance dataloader\n",
      "_____\n",
      "C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/feature_map_ResNet_101_AWA2.hdf5\n",
      "Expert Attr\n",
      "threshold at zero attribute with negative value\n"
     ]
    }
   ],
   "source": [
    "data_path = 'C:/Sushree/Jio_Institute/Dataset/'\n",
    "feature_path = 'C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/'\n",
    "dataloader = AWA2DataLoader(data_path, feature_path, device = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "485c300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr.append(param_group['lr'])\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d81d8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sushree.Behera\\Sushree\\GZSL_Implementation\\DAZLE.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.init_w2v_att = F.normalize(torch.tensor(init_w2v_att))\n",
      "C:\\Users\\Sushree.Behera\\Sushree\\GZSL_Implementation\\DAZLE.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.att = nn.Parameter(F.normalize(torch.tensor(att)),requires_grad = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "normalize V\n",
      "normalize F\n",
      "training to exclude unseen class [seen upperbound]\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "Compute Pruning loss Parameter containing:\n",
      "tensor(0)\n",
      "Add one smoothing\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n",
      "{'pmp': {'init_lambda': 0.1, 'final_lambda': 0.1, 'phase': 0.8}, 'desired_mass': {'init_lambda': -1, 'final_lambda': -1, 'phase': 0.8}}\n",
      "\t V\n",
      "\t W_1\n",
      "\t W_2\n",
      "\t W_3\n",
      "default lr ['V'] 1x lr ['W_1', 'W_2', 'W_3']\n",
      "------------------------------\n",
      "learing rate 0.0001\n",
      "trainable V True\n",
      "lambda_ 0.1\n",
      "optimized seen only\n",
      "optimizer: RMSProp with momentum = 0.0 and weight_decay = 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "seed = 214\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "batch_size = 32\n",
    "nepoches = 100\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att # load the attribute features\n",
    "att = dataloader.att\n",
    "att[att<0] = 0\n",
    "normalize_att = dataloader.normalize_att\n",
    "\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_ = 0.1\n",
    "bias = 0\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = False\n",
    "\n",
    "seenclass = dataloader.seenclasses #load seen and unseen data\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1\n",
    "report_interval = niters//nepoches\n",
    "\n",
    "device = None\n",
    "\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_,\n",
    "            trainable_w2v,normalize_V=True,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=True)\n",
    "model.to(device)\n",
    "\n",
    "setup = {'pmp':{'init_lambda':0.1,'final_lambda':0.1,'phase':0.8},\n",
    "         'desired_mass':{'init_lambda':-1,'final_lambda':-1,'phase':0.8}}\n",
    "print(setup)\n",
    "\n",
    "params_to_update = []\n",
    "params_names = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        params_names.append(name)\n",
    "        print(\"\\t\",name)\n",
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.\n",
    "#%%\n",
    "lr_seperator = 1\n",
    "lr_factor = 1\n",
    "print('default lr {} {}x lr {}'.format(params_names[:lr_seperator],lr_factor,params_names[lr_seperator:]))\n",
    "optimizer  = optim.RMSprop( params_to_update ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_ {}'.format(lambda_))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef247f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 0, 'loss': 3.9689862728118896, 'loss_CE': 3.9225924015045166, 'loss_cal': 0.4639386534690857, 'acc_seen': 0, 'acc_novel': 0, 'H': 0, 'acc_zs': 0}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 735, 'loss': 1.085241675376892, 'loss_CE': 0.9712463617324829, 'loss_cal': 1.13995361328125, 'acc_seen': 0.6427987813949585, 'acc_novel': 0.5949987173080444, 'H': 0.6179758010788207, 'acc_zs': 0.651785671710968}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1470, 'loss': 0.6941002607345581, 'loss_CE': 0.5455842614173889, 'loss_cal': 1.4851598739624023, 'acc_seen': 0.7299371957778931, 'acc_novel': 0.602407693862915, 'H': 0.6600690049434114, 'acc_zs': 0.6647583246231079}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2205, 'loss': 0.9008108377456665, 'loss_CE': 0.7790248990058899, 'loss_cal': 1.2178595066070557, 'acc_seen': 0.7481359243392944, 'acc_novel': 0.5977089405059814, 'H': 0.6645157140644306, 'acc_zs': 0.6685706973075867}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2940, 'loss': 0.6499524712562561, 'loss_CE': 0.5297620892524719, 'loss_cal': 1.2019037008285522, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3675, 'loss': 0.6399571895599365, 'loss_CE': 0.5183020830154419, 'loss_cal': 1.2165508270263672, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 4410, 'loss': 0.7588014006614685, 'loss_CE': 0.6369753479957581, 'loss_cal': 1.2182605266571045, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 5145, 'loss': 0.66252201795578, 'loss_CE': 0.5387728214263916, 'loss_cal': 1.2374920845031738, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 5880, 'loss': 0.5241669416427612, 'loss_CE': 0.39665526151657104, 'loss_cal': 1.2751169204711914, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 6615, 'loss': 0.6039716005325317, 'loss_CE': 0.46853891015052795, 'loss_cal': 1.3543267250061035, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 7350, 'loss': 0.5079832077026367, 'loss_CE': 0.3691604435443878, 'loss_cal': 1.388227939605713, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 8085, 'loss': 0.6739691495895386, 'loss_CE': 0.5489320158958435, 'loss_cal': 1.2503714561462402, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 8820, 'loss': 0.826629102230072, 'loss_CE': 0.7109538316726685, 'loss_cal': 1.156752586364746, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 9555, 'loss': 0.5908403396606445, 'loss_CE': 0.4595819413661957, 'loss_cal': 1.3125836849212646, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 10290, 'loss': 0.6060450077056885, 'loss_CE': 0.482627809047699, 'loss_cal': 1.2341722249984741, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 11025, 'loss': 0.6947084665298462, 'loss_CE': 0.5705692768096924, 'loss_cal': 1.2413917779922485, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 11760, 'loss': 0.5528489351272583, 'loss_CE': 0.42514196038246155, 'loss_cal': 1.2770698070526123, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 12495, 'loss': 0.5500624179840088, 'loss_CE': 0.4140143394470215, 'loss_cal': 1.360480785369873, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 13230, 'loss': 0.5716777443885803, 'loss_CE': 0.4341990351676941, 'loss_cal': 1.3747869729995728, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 13965, 'loss': 0.7066172361373901, 'loss_CE': 0.5833863019943237, 'loss_cal': 1.2323092222213745, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 14700, 'loss': 0.787014901638031, 'loss_CE': 0.6699916124343872, 'loss_cal': 1.170233130455017, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 15435, 'loss': 0.7164785265922546, 'loss_CE': 0.5966459512710571, 'loss_cal': 1.1983259916305542, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 16170, 'loss': 0.6629340648651123, 'loss_CE': 0.5333308577537537, 'loss_cal': 1.2960318326950073, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 16905, 'loss': 0.6255937814712524, 'loss_CE': 0.5075184106826782, 'loss_cal': 1.1807537078857422, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 17640, 'loss': 0.6412826776504517, 'loss_CE': 0.5230245590209961, 'loss_cal': 1.1825814247131348, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 18375, 'loss': 0.6468738317489624, 'loss_CE': 0.5255739092826843, 'loss_cal': 1.2129992246627808, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 19110, 'loss': 0.5851658582687378, 'loss_CE': 0.44145894050598145, 'loss_cal': 1.4370689392089844, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 19845, 'loss': 0.5831039547920227, 'loss_CE': 0.4455137550830841, 'loss_cal': 1.3759018182754517, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 20580, 'loss': 0.6276482343673706, 'loss_CE': 0.48706167936325073, 'loss_cal': 1.4058656692504883, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iter': 21315, 'loss': 0.6828747391700745, 'loss_CE': 0.5592877864837646, 'loss_cal': 1.2358695268630981, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 22050, 'loss': 0.666007936000824, 'loss_CE': 0.5431318879127502, 'loss_cal': 1.2287604808807373, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 22785, 'loss': 0.5813158750534058, 'loss_CE': 0.4507203996181488, 'loss_cal': 1.3059544563293457, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 23520, 'loss': 0.5826154947280884, 'loss_CE': 0.4628904461860657, 'loss_cal': 1.1972506046295166, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 24255, 'loss': 0.9684376120567322, 'loss_CE': 0.8680579662322998, 'loss_cal': 1.0037965774536133, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 24990, 'loss': 0.7647511959075928, 'loss_CE': 0.6385321617126465, 'loss_cal': 1.262190341949463, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 25725, 'loss': 0.8253782987594604, 'loss_CE': 0.7102091312408447, 'loss_cal': 1.1516919136047363, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 26460, 'loss': 0.6415237188339233, 'loss_CE': 0.5076901316642761, 'loss_cal': 1.3383361101150513, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 27195, 'loss': 0.6969407796859741, 'loss_CE': 0.5759581923484802, 'loss_cal': 1.2098259925842285, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 27930, 'loss': 0.6553513407707214, 'loss_CE': 0.5233358144760132, 'loss_cal': 1.320155382156372, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 28665, 'loss': 0.8583074808120728, 'loss_CE': 0.7474902272224426, 'loss_cal': 1.1081727743148804, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 29400, 'loss': 0.5214744210243225, 'loss_CE': 0.3867512345314026, 'loss_cal': 1.3472316265106201, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 30135, 'loss': 0.6622234582901001, 'loss_CE': 0.5351371765136719, 'loss_cal': 1.2708628177642822, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 30870, 'loss': 0.6444138288497925, 'loss_CE': 0.5197778940200806, 'loss_cal': 1.2463593482971191, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 31605, 'loss': 0.5913358926773071, 'loss_CE': 0.45814335346221924, 'loss_cal': 1.331925630569458, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 32340, 'loss': 0.6938830018043518, 'loss_CE': 0.5700272917747498, 'loss_cal': 1.2385568618774414, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 33075, 'loss': 0.6929976344108582, 'loss_CE': 0.554681658744812, 'loss_cal': 1.3831597566604614, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 33810, 'loss': 0.5733815431594849, 'loss_CE': 0.4273425042629242, 'loss_cal': 1.460390329360962, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 34545, 'loss': 0.6441258788108826, 'loss_CE': 0.5173791646957397, 'loss_cal': 1.2674670219421387, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 35280, 'loss': 0.7185782790184021, 'loss_CE': 0.5977661609649658, 'loss_cal': 1.2081212997436523, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 36015, 'loss': 0.5712680220603943, 'loss_CE': 0.4406680762767792, 'loss_cal': 1.3059992790222168, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 36750, 'loss': 0.6685594916343689, 'loss_CE': 0.5378516316413879, 'loss_cal': 1.3070783615112305, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 37485, 'loss': 0.5670984983444214, 'loss_CE': 0.430400013923645, 'loss_cal': 1.3669846057891846, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 38220, 'loss': 0.7088315486907959, 'loss_CE': 0.5789312720298767, 'loss_cal': 1.2990028858184814, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 38955, 'loss': 0.5697323679924011, 'loss_CE': 0.4378383159637451, 'loss_cal': 1.3189406394958496, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 39690, 'loss': 0.6765795350074768, 'loss_CE': 0.5440755486488342, 'loss_cal': 1.3250396251678467, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 40425, 'loss': 0.8709467053413391, 'loss_CE': 0.7487199902534485, 'loss_cal': 1.2222673892974854, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 41160, 'loss': 0.5603394508361816, 'loss_CE': 0.434581458568573, 'loss_cal': 1.257580041885376, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 41895, 'loss': 0.7266391515731812, 'loss_CE': 0.5849424600601196, 'loss_cal': 1.4169669151306152, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iter': 42630, 'loss': 0.573358952999115, 'loss_CE': 0.4580504894256592, 'loss_cal': 1.1530847549438477, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 43365, 'loss': 0.7047705054283142, 'loss_CE': 0.5821986198425293, 'loss_cal': 1.2257190942764282, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 44100, 'loss': 0.8869787454605103, 'loss_CE': 0.7657586336135864, 'loss_cal': 1.2122012376785278, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 44835, 'loss': 0.8330092430114746, 'loss_CE': 0.7064053416252136, 'loss_cal': 1.2660387754440308, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 45570, 'loss': 0.726046085357666, 'loss_CE': 0.6043239831924438, 'loss_cal': 1.2172212600708008, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 46305, 'loss': 0.7466203570365906, 'loss_CE': 0.63118976354599, 'loss_cal': 1.1543058156967163, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 47040, 'loss': 0.7539873719215393, 'loss_CE': 0.6438696980476379, 'loss_cal': 1.1011767387390137, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 47775, 'loss': 0.6309660077095032, 'loss_CE': 0.5071120858192444, 'loss_cal': 1.238539457321167, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 48510, 'loss': 0.6301939487457275, 'loss_CE': 0.5050374269485474, 'loss_cal': 1.2515653371810913, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 49245, 'loss': 0.5895310044288635, 'loss_CE': 0.460480660200119, 'loss_cal': 1.2905032634735107, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 49980, 'loss': 0.6336081027984619, 'loss_CE': 0.5062099695205688, 'loss_cal': 1.2739815711975098, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 50715, 'loss': 0.6780703067779541, 'loss_CE': 0.5438230633735657, 'loss_cal': 1.3424726724624634, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 51450, 'loss': 0.6783418655395508, 'loss_CE': 0.5413939356803894, 'loss_cal': 1.3694790601730347, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 52185, 'loss': 0.9602841734886169, 'loss_CE': 0.8325144052505493, 'loss_cal': 1.2776975631713867, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 52920, 'loss': 0.6508755683898926, 'loss_CE': 0.5157302618026733, 'loss_cal': 1.351453185081482, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 53655, 'loss': 0.6580166816711426, 'loss_CE': 0.5213235020637512, 'loss_cal': 1.366931438446045, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 54390, 'loss': 0.6607312560081482, 'loss_CE': 0.53574138879776, 'loss_cal': 1.2498985528945923, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 55125, 'loss': 0.6871405839920044, 'loss_CE': 0.5783625245094299, 'loss_cal': 1.0877807140350342, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 55860, 'loss': 0.6439274549484253, 'loss_CE': 0.5101733207702637, 'loss_cal': 1.3375413417816162, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 56595, 'loss': 0.547288715839386, 'loss_CE': 0.415029376745224, 'loss_cal': 1.3225935697555542, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 57330, 'loss': 0.534245491027832, 'loss_CE': 0.39788660407066345, 'loss_cal': 1.363588571548462, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 58065, 'loss': 0.7382544279098511, 'loss_CE': 0.6115017533302307, 'loss_cal': 1.2675267457962036, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 58800, 'loss': 0.6137173175811768, 'loss_CE': 0.49098512530326843, 'loss_cal': 1.2273216247558594, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 59535, 'loss': 0.6856090426445007, 'loss_CE': 0.5540511012077332, 'loss_cal': 1.3155791759490967, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 60270, 'loss': 0.7368530631065369, 'loss_CE': 0.6137984991073608, 'loss_cal': 1.2305455207824707, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 61005, 'loss': 0.6038116812705994, 'loss_CE': 0.46180301904678345, 'loss_cal': 1.42008638381958, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 61740, 'loss': 0.6301363706588745, 'loss_CE': 0.5150882005691528, 'loss_cal': 1.1504818201065063, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 62475, 'loss': 0.7023540139198303, 'loss_CE': 0.5788047909736633, 'loss_cal': 1.235492467880249, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 63210, 'loss': 0.638043224811554, 'loss_CE': 0.49965906143188477, 'loss_cal': 1.383841633796692, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iter': 63945, 'loss': 0.5818604230880737, 'loss_CE': 0.44215258955955505, 'loss_cal': 1.3970786333084106, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 64680, 'loss': 0.6420921087265015, 'loss_CE': 0.5196096301078796, 'loss_cal': 1.2248250246047974, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 65415, 'loss': 0.6093083024024963, 'loss_CE': 0.466422975063324, 'loss_cal': 1.4288532733917236, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 66150, 'loss': 0.4841991066932678, 'loss_CE': 0.3433381915092468, 'loss_cal': 1.4086089134216309, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 66885, 'loss': 0.7261913418769836, 'loss_CE': 0.6001570820808411, 'loss_cal': 1.2603425979614258, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 67620, 'loss': 0.5268915295600891, 'loss_CE': 0.3848966360092163, 'loss_cal': 1.419948697090149, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 68355, 'loss': 0.5522363185882568, 'loss_CE': 0.4145481586456299, 'loss_cal': 1.3768815994262695, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 69090, 'loss': 0.5086340308189392, 'loss_CE': 0.3771724998950958, 'loss_cal': 1.314615249633789, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 69825, 'loss': 0.7429693341255188, 'loss_CE': 0.6281970739364624, 'loss_cal': 1.147722601890564, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 70560, 'loss': 0.5165588855743408, 'loss_CE': 0.36887824535369873, 'loss_cal': 1.476806402206421, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 71295, 'loss': 0.6929102540016174, 'loss_CE': 0.5680170655250549, 'loss_cal': 1.2489320039749146, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 72030, 'loss': 0.5977464914321899, 'loss_CE': 0.4660462737083435, 'loss_cal': 1.317002296447754, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 72765, 'loss': 0.6468745470046997, 'loss_CE': 0.5152096748352051, 'loss_cal': 1.3166486024856567, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 73500, 'loss': 0.606707751750946, 'loss_CE': 0.4820508360862732, 'loss_cal': 1.2465689182281494, 'acc_seen': 0.7545713782310486, 'acc_novel': 0.5993922352790833, 'H': 0.6680891872758474, 'acc_zs': 0.6729567050933838}\n"
     ]
    }
   ],
   "source": [
    "best_performance = [0,0,0,0]\n",
    "for i in range(0,niters):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "    out_package = model(batch_feature)\n",
    "    \n",
    "    in_package = out_package\n",
    "    in_package['batch_label'] = batch_label\n",
    "    \n",
    "    out_package=model.compute_loss(in_package)\n",
    "    loss,loss_CE,loss_cal = out_package['loss'],out_package['loss_CE'],out_package['loss_cal']\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%report_interval==0:\n",
    "        print('-'*30)\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss':loss.item(), 'loss_CE':loss_CE.item(),\n",
    "                         'loss_cal': loss_cal.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411c872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
