{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7057dd",
   "metadata": {},
   "source": [
    "# Implementation of the paper \"Fine-grained generalized zero-shot learning via dense attribute-based attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9ccf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.resnet as models\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import torch.optim as optim\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac6ac9",
   "metadata": {},
   "source": [
    "# CUB dataset\n",
    "#images = 17188\n",
    "\n",
    "#classes = 200, 150 seen classes and 50 unseen classes\n",
    "\n",
    "Each class has 200 number of attributes that represent the class infromation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb6dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Sushree/Jio_Institute/Dataset/\n",
      "C:/Sushree/Jio_Institute/Dataset/data/xlsa17/data/CUB/res101.mat\n"
     ]
    }
   ],
   "source": [
    "img_dir = 'C:/Sushree/Jio_Institute/Dataset/'\n",
    "print(img_dir)\n",
    "\n",
    "file_paths = 'C:/Sushree/Jio_Institute/Dataset/data/xlsa17/data/CUB/res101.mat'\n",
    "print(file_paths)\n",
    "\n",
    "#resNet101.mat includes the following fields:\n",
    "#-features: columns correspond to image instances\n",
    "#-labels: label number of a class is its row number in allclasses.txt\n",
    "#-image_files: image sources  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d3b51",
   "metadata": {},
   "source": [
    "# Let's visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd214aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_distribution(file_paths):    \n",
    "    matcontent = sio.loadmat(file_paths)\n",
    "    print(matcontent)\n",
    "\n",
    "    image_files = np.squeeze(matcontent['image_files'])\n",
    "    #print(image_files)\n",
    "\n",
    "    labels = np.squeeze(matcontent['labels'])\n",
    "    print(labels)\n",
    "    print(labels.size)  # 11788 for CUB\n",
    "\n",
    "    class_names = []\n",
    "    for idx in range(len(image_files)):\n",
    "        image_file = image_files[idx][0]\n",
    "        class_name = image_file.split('/')[5:][3]\n",
    "        class_names.append(class_name)\n",
    "\n",
    "    print(len(class_names))   \n",
    "    #print(class_names)\n",
    "    \n",
    "    num_bins = 200 # # for CUB\n",
    "    \n",
    "    plt.figure(figsize=(35,6))\n",
    "    \n",
    "    plt.title(\"Data Distribution: CUB\")\n",
    "    plt.xlabel(\"Categories\")\n",
    "    plt.ylabel(\"Number of Classes\")\n",
    "    \n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.grid(color = 'red', linestyle = '--', linewidth = 0.3)\n",
    "    plt.hist(class_names, num_bins, align=\"mid\")\n",
    "\n",
    "visualize_data_distribution(file_paths)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72caa267",
   "metadata": {},
   "source": [
    "# Let's extract deep features (consider pre-trained ResNet 101 with no fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28dc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomedDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir , file_paths, transform=None):\n",
    "        self.matcontent = sio.loadmat(file_paths)\n",
    "        self.image_files = np.squeeze(self.matcontent['image_files'])\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx][0]\n",
    "        image_file = os.path.join(self.img_dir, '/'.join(image_file.split('/')[5:]))\n",
    "        image = Image.open(image_file)\n",
    "        \n",
    "        if image.mode == 'L':\n",
    "            image=image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57bc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "CUBDataset = CustomedDataset(img_dir , file_paths, data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51604ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sushree.Behera\\anaconda3\\envs\\tf_sushree\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sushree.Behera\\anaconda3\\envs\\tf_sushree\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-313          [-1, 512, 14, 14]               0\n",
      "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-316            [-1, 512, 7, 7]               0\n",
      "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-321           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-322           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-325            [-1, 512, 7, 7]               0\n",
      "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-328            [-1, 512, 7, 7]               0\n",
      "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-331           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-332           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-335            [-1, 512, 7, 7]               0\n",
      "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-338            [-1, 512, 7, 7]               0\n",
      "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-341           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-342           [-1, 2048, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 42,500,160\n",
      "Trainable params: 0\n",
      "Non-trainable params: 42,500,160\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 429.71\n",
      "Params size (MB): 162.13\n",
      "Estimated Total Size (MB): 592.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_name = \"resnet\"\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "model_ref = models.resnet101(pretrained=True)\n",
    "model_ref.eval()\n",
    "\n",
    "model_f = nn.Sequential(*list(model_ref.children())[:-2])\n",
    "model_f.eval()\n",
    "\n",
    "\n",
    "for param in model_f.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(model_f)\n",
    "        \n",
    "from torchsummary import summary\n",
    "summary(model_f, (3, 224, 224))    \n",
    "\n",
    "dataset_loader = torch.utils.data.DataLoader(CUBDataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740abf7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "[[[[0.00000000e+00 1.23344541e-01 4.21795040e-01 ... 8.83068979e-01\n",
      "    4.78987157e-01 3.40790004e-02]\n",
      "   [3.98916721e-01 8.65673423e-01 1.91944945e+00 ... 2.04522371e+00\n",
      "    9.94292498e-01 3.35752785e-01]\n",
      "   [1.62883639e+00 1.88533747e+00 2.49308968e+00 ... 2.54128289e+00\n",
      "    2.07721472e+00 1.13727379e+00]\n",
      "   ...\n",
      "   [2.21705508e+00 2.69009900e+00 2.40386891e+00 ... 3.52886152e+00\n",
      "    2.16139841e+00 1.15703058e+00]\n",
      "   [1.61826468e+00 2.66649914e+00 2.61611414e+00 ... 2.47577143e+00\n",
      "    2.30320406e+00 1.44193637e+00]\n",
      "   [1.09783983e+00 1.36646819e+00 9.82558608e-01 ... 1.12874866e+00\n",
      "    1.96719456e+00 1.98620057e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.04329327e-01 6.98611215e-02 ... 1.40249580e-01\n",
      "    1.42701417e-02 0.00000000e+00]\n",
      "   [3.35342288e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 7.71514326e-03]\n",
      "   ...\n",
      "   [3.59844476e-01 1.07140690e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[3.75346041e+00 3.64417338e+00 2.83157635e+00 ... 1.72575533e+00\n",
      "    2.54983783e+00 3.33554864e+00]\n",
      "   [2.95257854e+00 3.07320738e+00 3.30664444e+00 ... 3.35704446e+00\n",
      "    4.14575052e+00 4.92940569e+00]\n",
      "   [2.41849494e+00 3.57774687e+00 5.48922443e+00 ... 5.79459667e+00\n",
      "    6.48801517e+00 7.70109797e+00]\n",
      "   ...\n",
      "   [1.85578632e+00 2.70677948e+00 4.66350412e+00 ... 7.30483961e+00\n",
      "    7.11566877e+00 6.29558182e+00]\n",
      "   [2.18888760e+00 2.61752439e+00 3.84370899e+00 ... 5.14817047e+00\n",
      "    7.96137714e+00 7.96865749e+00]\n",
      "   [2.18726873e+00 2.56882286e+00 2.62795329e+00 ... 4.11728954e+00\n",
      "    6.33051300e+00 5.60106468e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 1.01248845e-01 3.80741805e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.47778112e-01 5.12185335e-01 5.23400068e-01 ... 7.63493240e-01\n",
      "    9.36028361e-03 6.51458204e-02]\n",
      "   [2.37892270e-01 0.00000000e+00 6.85765207e-01 ... 4.38160121e-01\n",
      "    0.00000000e+00 3.61981660e-01]\n",
      "   ...\n",
      "   [1.57944351e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.96553338e-01 4.59979266e-01 1.82142556e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [4.74180400e-01 5.67082942e-01 5.54889917e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 3.43969129e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 2.27627754e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.51395991e-01 7.07046986e-01 1.03317130e+00 ... 7.07272291e-01\n",
      "    1.57026023e-01 0.00000000e+00]\n",
      "   [7.51218438e-01 6.15920901e-01 4.93486285e-01 ... 1.62529135e+00\n",
      "    2.50245905e+00 6.77129686e-01]\n",
      "   ...\n",
      "   [1.23136604e+00 1.05579925e+00 1.28499842e+00 ... 1.86028481e-02\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [6.55633152e-01 7.71340847e-01 7.74460673e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.30988821e-01 4.26023006e-01 2.50278533e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 4.54365686e-02 ... 1.27414525e-01\n",
      "    8.00419971e-02 1.42024666e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 7.09344000e-02 ... 5.38958132e-01\n",
      "    2.70261079e-01 2.69324750e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.28245860e-01 ... 0.00000000e+00\n",
      "    4.76176620e-01 0.00000000e+00]\n",
      "   ...\n",
      "   [1.73644125e-02 6.62654281e-01 1.53448510e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.52076483e-02]\n",
      "   [0.00000000e+00 3.79943967e-01 1.11918986e+00 ... 7.39583313e-01\n",
      "    4.45294470e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 6.36404008e-03 ... 7.29544759e-02\n",
      "    4.66245472e-01 6.55090511e-02]]]\n",
      "\n",
      "\n",
      " [[[0.00000000e+00 5.55512905e-02 1.21832326e-01 ... 1.95341498e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.16046622e-01 3.27587217e-01 2.82026917e-01 ... 3.87288392e-01\n",
      "    4.74050403e-01 5.70539951e-01]\n",
      "   [7.40075529e-01 6.94488764e-01 5.60813904e-01 ... 7.97301054e-01\n",
      "    1.00133324e+00 9.84840989e-01]\n",
      "   ...\n",
      "   [2.75124764e+00 3.82622290e+00 3.50927734e+00 ... 1.46784961e+00\n",
      "    1.64880013e+00 1.06901252e+00]\n",
      "   [2.41980839e+00 4.59412050e+00 4.16425467e+00 ... 8.68923128e-01\n",
      "    1.69764829e+00 1.38343191e+00]\n",
      "   [2.03918266e+00 2.59122181e+00 2.57117414e+00 ... 8.70916784e-01\n",
      "    5.02322912e-01 7.59068012e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 4.46224995e-02\n",
      "    4.60248739e-02 1.42843977e-01]\n",
      "   [1.80280894e-01 1.11720011e-01 1.03530325e-02 ... 3.60177517e-01\n",
      "    5.88244319e-01 5.48055708e-01]\n",
      "   [4.33086336e-01 3.81942928e-01 1.20741442e-01 ... 1.59049749e-01\n",
      "    1.87309116e-01 9.74878371e-02]\n",
      "   ...\n",
      "   [5.34982443e-01 4.16235358e-01 6.71987653e-01 ... 7.04398870e-01\n",
      "    5.32866359e-01 3.39570865e-02]\n",
      "   [6.59395814e-01 3.78882915e-01 3.48803639e-01 ... 1.99039310e-01\n",
      "    3.29580784e-01 0.00000000e+00]\n",
      "   [6.43873140e-02 0.00000000e+00 0.00000000e+00 ... 1.74069762e-01\n",
      "    1.95846900e-01 0.00000000e+00]]\n",
      "\n",
      "  [[1.99017322e+00 2.53189659e+00 2.61554909e+00 ... 3.77395725e+00\n",
      "    3.75050974e+00 5.02650690e+00]\n",
      "   [3.33304453e+00 4.02199221e+00 3.83909297e+00 ... 4.65760612e+00\n",
      "    5.74992418e+00 5.06978512e+00]\n",
      "   [2.96229935e+00 4.36833000e+00 4.78798819e+00 ... 5.90359354e+00\n",
      "    5.61335754e+00 4.58098984e+00]\n",
      "   ...\n",
      "   [3.41839504e+00 4.30401087e+00 4.00673676e+00 ... 7.31423473e+00\n",
      "    7.49174595e+00 7.19132519e+00]\n",
      "   [2.62928772e+00 4.12013102e+00 3.56735063e+00 ... 3.08509827e+00\n",
      "    4.13509226e+00 5.06550980e+00]\n",
      "   [2.64984369e+00 3.66536713e+00 4.37577295e+00 ... 2.37959194e+00\n",
      "    1.76572180e+00 2.45953679e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.94071198e-01\n",
      "    8.24070036e-01 6.00410104e-01]\n",
      "   [0.00000000e+00 1.82687640e-02 4.70556855e-01 ... 1.09519899e+00\n",
      "    1.09652197e+00 5.63392401e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.49974927e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.65383929e-01\n",
      "    9.20065939e-02 0.00000000e+00]\n",
      "   [5.44075482e-02 0.00000000e+00 3.42497490e-02 ... 2.99070477e-01\n",
      "    1.40833706e-01 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 1.05622113e-02 ... 9.58454013e-02\n",
      "    0.00000000e+00 6.94755986e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 7.92712748e-01\n",
      "    9.95317042e-01 8.82099092e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 7.52023637e-01\n",
      "    6.30104542e-01 1.02457833e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 6.75854087e-01 ... 8.07358384e-01\n",
      "    8.99141014e-01 6.53136611e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.07828997e-01 ... 5.96834958e-01\n",
      "    1.05742455e+00 4.90641475e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.23228797e-01\n",
      "    4.34123784e-01 0.00000000e+00]]\n",
      "\n",
      "  [[1.12536728e-01 1.85717225e-01 8.84975567e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.02121457e-01 3.37889880e-01 7.01524198e-01 ... 4.38047290e-01\n",
      "    3.86167347e-01 5.87922335e-01]\n",
      "   [5.94430745e-01 9.56499338e-01 1.02932954e+00 ... 1.10222089e+00\n",
      "    1.29932678e+00 1.97966933e+00]\n",
      "   ...\n",
      "   [6.55581355e-01 1.37303472e+00 1.30771661e+00 ... 1.37036180e+00\n",
      "    1.56573224e+00 2.27161312e+00]\n",
      "   [1.05480731e+00 8.93914342e-01 3.17737252e-01 ... 4.73452836e-01\n",
      "    7.65421808e-01 1.36658013e+00]\n",
      "   [1.24122918e+00 8.84173989e-01 3.55686367e-01 ... 3.87819558e-01\n",
      "    7.18837976e-01 4.32090998e-01]]]\n",
      "\n",
      "\n",
      " [[[0.00000000e+00 0.00000000e+00 5.14609098e-01 ... 1.74660361e+00\n",
      "    1.73377085e+00 1.38192451e+00]\n",
      "   [0.00000000e+00 4.35321659e-01 1.50430894e+00 ... 2.64416122e+00\n",
      "    3.05327272e+00 1.63199258e+00]\n",
      "   [1.13033652e+00 1.79851842e+00 2.27450991e+00 ... 3.07588816e+00\n",
      "    3.09264660e+00 1.80212450e+00]\n",
      "   ...\n",
      "   [1.44788253e+00 1.94485545e+00 2.36521006e+00 ... 2.87604141e+00\n",
      "    1.36609232e+00 4.50808585e-01]\n",
      "   [1.39853799e+00 2.61941385e+00 4.42756605e+00 ... 1.83830643e+00\n",
      "    4.70707983e-01 8.25710371e-02]\n",
      "   [3.98768544e-01 1.02479243e+00 2.53090119e+00 ... 9.44443345e-01\n",
      "    1.20840728e-01 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 6.28843784e-01\n",
      "    5.08461237e-01 8.22339728e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    5.62002301e-01 6.33501112e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.66646630e-01 6.08619824e-02]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 2.23245710e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[3.08519363e+00 4.13730335e+00 2.11681342e+00 ... 7.74318337e-01\n",
      "    7.77671337e-01 1.45249367e+00]\n",
      "   [2.42382431e+00 3.12841010e+00 2.60485888e+00 ... 7.09272563e-01\n",
      "    9.55441654e-01 1.20019257e+00]\n",
      "   [1.69484997e+00 3.12121916e+00 4.12403107e+00 ... 1.60222304e+00\n",
      "    1.09781337e+00 1.35860634e+00]\n",
      "   ...\n",
      "   [3.53537989e+00 7.37788582e+00 8.59177113e+00 ... 5.99820042e+00\n",
      "    3.76717043e+00 2.12989330e+00]\n",
      "   [3.74528551e+00 7.33026981e+00 8.57229328e+00 ... 7.78991604e+00\n",
      "    4.49117661e+00 3.13209176e+00]\n",
      "   [3.17805910e+00 6.62985086e+00 8.59153748e+00 ... 6.25169754e+00\n",
      "    3.92545700e+00 3.09998369e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[4.07315418e-02 2.94651091e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.50065947e-01 4.54916358e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.56395841e-01 1.72396749e-02 7.85625428e-02 ... 0.00000000e+00\n",
      "    4.35394049e-02 5.44649661e-02]\n",
      "   ...\n",
      "   [5.13114274e-01 2.62079060e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.87500894e-01 5.99996209e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 2.38595814e-01]\n",
      "   [5.73775172e-02 4.70866710e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 5.85133135e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.74833763e+00\n",
      "    1.60305834e+00 1.39611930e-01]\n",
      "   [1.46919027e-01 3.40642989e-01 7.38825142e-01 ... 2.89023685e+00\n",
      "    1.15003014e+00 2.56190896e-01]\n",
      "   [1.19329464e+00 1.16351068e+00 9.75439191e-01 ... 1.74720836e+00\n",
      "    1.43983150e+00 5.07670522e-01]\n",
      "   ...\n",
      "   [8.39505672e-01 8.59709203e-01 9.58561897e-04 ... 2.08582103e-01\n",
      "    3.89855981e-01 1.02021344e-01]\n",
      "   [5.45158565e-01 0.00000000e+00 0.00000000e+00 ... 1.03454292e-02\n",
      "    5.72227538e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 2.64874250e-02 0.00000000e+00 ... 4.84409571e-01\n",
      "    7.28514254e-01 7.80541152e-02]]\n",
      "\n",
      "  [[9.29011311e-03 8.19072798e-02 1.17494231e-02 ... 1.30910110e+00\n",
      "    1.53211069e+00 9.11279798e-01]\n",
      "   [4.12151128e-01 6.20241761e-01 3.23496044e-01 ... 9.12613988e-01\n",
      "    1.38501406e+00 1.08084297e+00]\n",
      "   [7.14405894e-01 1.55567765e+00 1.31455052e+00 ... 7.01306760e-01\n",
      "    1.07731843e+00 6.14239693e-01]\n",
      "   ...\n",
      "   [2.46584579e-01 1.73007929e+00 1.49716949e+00 ... 1.71258390e+00\n",
      "    1.36866415e+00 1.72431469e-01]\n",
      "   [1.45635262e-01 1.07985544e+00 4.42533731e-01 ... 1.36447024e+00\n",
      "    1.08928144e+00 2.53813386e-01]\n",
      "   [2.18597800e-02 8.22420597e-01 8.59650493e-01 ... 8.29530716e-01\n",
      "    5.40445268e-01 2.23672539e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.38703203e+00\n",
      "    1.76032400e+00 4.00436111e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 2.93784887e-01 ... 4.79732418e+00\n",
      "    2.38045144e+00 3.27828228e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 8.62688124e-01 ... 2.67858362e+00\n",
      "    2.13050270e+00 3.51154774e-01]\n",
      "   ...\n",
      "   [1.19607612e-01 1.19773293e+00 2.91056633e+00 ... 2.45064139e+00\n",
      "    1.21959674e+00 1.73290923e-01]\n",
      "   [1.05191171e-01 8.28289151e-01 2.10831070e+00 ... 1.40767157e+00\n",
      "    2.96408832e-01 0.00000000e+00]\n",
      "   [3.70199502e-01 8.06755185e-01 5.66424072e-01 ... 2.60193292e-02\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[2.21496850e-01 0.00000000e+00 0.00000000e+00 ... 2.17734623e+00\n",
      "    1.64285445e+00 5.71742177e-01]\n",
      "   [8.56714845e-01 3.94289464e-01 0.00000000e+00 ... 3.84377861e+00\n",
      "    2.06818151e+00 9.30971682e-01]\n",
      "   [9.27684486e-01 1.30823660e+00 1.13343549e+00 ... 2.36758995e+00\n",
      "    1.99752176e+00 1.42293882e+00]\n",
      "   ...\n",
      "   [1.25069892e+00 1.59929514e+00 1.24791217e+00 ... 2.35186553e+00\n",
      "    1.30400217e+00 9.81752098e-01]\n",
      "   [8.76262128e-01 6.61035717e-01 1.76966667e+00 ... 1.96029997e+00\n",
      "    9.15546596e-01 1.04478741e+00]\n",
      "   [0.00000000e+00 2.74956971e-01 6.78531528e-01 ... 7.23664165e-01\n",
      "    6.28796101e-01 4.28803712e-01]]\n",
      "\n",
      "  [[1.18504119e+00 2.31038666e+00 3.03925371e+00 ... 2.57731915e-01\n",
      "    0.00000000e+00 6.10086203e-01]\n",
      "   [1.25441980e+00 2.10903263e+00 2.49170732e+00 ... 6.29946351e-01\n",
      "    2.11120725e-01 3.97609472e-02]\n",
      "   [1.25105953e+00 1.02361107e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    5.99486649e-01 0.00000000e+00]\n",
      "   ...\n",
      "   [1.08315766e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.24295330e+00 1.34480810e+00]\n",
      "   [1.66198909e+00 0.00000000e+00 0.00000000e+00 ... 3.45839560e-01\n",
      "    1.18279445e+00 1.08507609e+00]\n",
      "   [1.94972599e+00 1.80321002e+00 2.03751183e+00 ... 1.30760813e+00\n",
      "    1.36173010e+00 6.86017990e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.12762094e-01 3.51024866e-01 0.00000000e+00 ... 1.24972187e-01\n",
      "    0.00000000e+00 5.95119953e-01]\n",
      "   [0.00000000e+00 4.39763725e-01 4.14060771e-01 ... 9.52929258e-01\n",
      "    3.08764875e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 7.47586191e-01 1.56700158e+00 ... 1.19419312e+00\n",
      "    6.80270791e-01 0.00000000e+00]\n",
      "   ...\n",
      "   [4.17915493e-01 6.25357032e-01 8.40183020e-01 ... 5.74439526e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.01757252e-01 5.64145207e-01 8.41121197e-01 ... 1.56547248e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.12547356e-01 3.03065293e-02 3.76400858e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 2.73540050e-01 ... 8.75891447e-01\n",
      "    7.71571875e-01 3.48998189e-01]\n",
      "   [1.47167727e-01 6.21304154e-01 1.61076581e+00 ... 1.41601825e+00\n",
      "    1.22400486e+00 4.92221117e-01]\n",
      "   [3.53185296e-01 1.07992578e+00 2.17329907e+00 ... 2.08184266e+00\n",
      "    1.24917209e+00 3.29913318e-01]\n",
      "   ...\n",
      "   [1.15030444e+00 2.39922571e+00 1.47118330e+00 ... 3.24160993e-01\n",
      "    2.12125465e-01 0.00000000e+00]\n",
      "   [1.00848889e+00 1.78653860e+00 3.68232667e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [7.49280691e-01 6.36837363e-01 4.49407846e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.83241367e-02]\n",
      "   [3.19202185e-01 1.65868700e-01 0.00000000e+00 ... 3.53625089e-01\n",
      "    0.00000000e+00 5.93797028e-01]\n",
      "   [7.77375460e-01 7.51640975e-01 7.26850510e-01 ... 1.31712103e+00\n",
      "    7.59312928e-01 2.98873752e-01]\n",
      "   ...\n",
      "   [2.59700388e-01 7.78411269e-01 1.01604748e+00 ... 2.07247138e+00\n",
      "    1.67154956e+00 1.44249633e-01]\n",
      "   [0.00000000e+00 4.38700348e-01 1.13465834e+00 ... 9.04989958e-01\n",
      "    4.39739168e-01 1.06205702e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [9.00125027e-01 5.87229311e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.85352600e+00 1.46896160e+00 1.12536120e+00 ... 3.90386611e-01\n",
      "    3.06656450e-01 1.92151457e-01]\n",
      "   ...\n",
      "   [1.61894226e+00 1.65214825e+00 2.13942099e+00 ... 6.63163126e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.47468746e-02 4.44570221e-02 1.09106436e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    9.95407999e-02 8.24614614e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [6.68560147e-01 5.80054700e-01 3.92742962e-01 ... 1.10411501e+00\n",
      "    9.14896786e-01 6.26444280e-01]\n",
      "   ...\n",
      "   [5.29324472e-01 3.34813595e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    2.18702018e-01 3.28336239e-01]\n",
      "   [4.30800095e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[3.18110406e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 6.43730164e-05 ... 7.29410127e-02\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.13350767e-01 1.79054990e-01 5.62665761e-01 ... 1.29600906e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.26433957e-01 5.67229986e-01 1.08279634e+00 ... 1.24453735e+00\n",
      "    4.45118845e-02 0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.26866817e-01 1.71618700e+00 3.26261282e+00 ... 2.19200754e+00\n",
      "    2.13297844e+00 1.86522269e+00]\n",
      "   [6.57636046e-01 1.92750120e+00 2.90067339e+00 ... 2.60911989e+00\n",
      "    3.41968727e+00 2.89302969e+00]\n",
      "   [1.11211240e+00 3.78539830e-01 1.34016740e+00 ... 2.56833124e+00\n",
      "    3.00477672e+00 1.86588359e+00]\n",
      "   ...\n",
      "   [2.84223747e+00 1.72774696e+00 3.17031920e-01 ... 6.41719460e-01\n",
      "    1.52587986e+00 1.69126976e+00]\n",
      "   [2.66106558e+00 2.30574703e+00 1.43773353e+00 ... 1.47520244e+00\n",
      "    1.98484433e+00 1.35650694e+00]\n",
      "   [1.74500263e+00 2.30503798e+00 9.15091932e-01 ... 2.01893854e+00\n",
      "    9.22176659e-01 7.47826278e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 1.37398273e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [5.68873733e-02 1.53779447e-01 1.88760996e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.66777039e-01\n",
      "    3.61422956e-01 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.36945748e+00 1.01915574e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.29171664e-01\n",
      "    1.15578544e+00 6.55931354e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.72103927e-01 ... 2.18138561e-01\n",
      "    5.40704489e-01 2.27018416e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    6.59787841e-03 1.31200448e-01]\n",
      "   [4.66722190e-01 8.90830100e-01 7.86777198e-01 ... 4.67431098e-02\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.71022868e+00 1.70205688e+00 2.70606232e+00 ... 1.44933271e+00\n",
      "    9.94663984e-02 0.00000000e+00]\n",
      "   ...\n",
      "   [1.64259005e+00 1.16157365e+00 1.52915061e+00 ... 7.15507925e-01\n",
      "    4.66894627e-01 5.16050339e-01]\n",
      "   [3.58681619e-01 4.20237154e-01 4.85955805e-01 ... 1.19052552e-01\n",
      "    3.31622094e-01 6.34687245e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    3.43105078e-01 1.80010825e-01]]]\n",
      "\n",
      "\n",
      " [[[3.75678599e-01 6.23176873e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 5.13247252e-02]\n",
      "   [1.02346659e+00 8.41145158e-01 2.41493270e-01 ... 3.25752556e-01\n",
      "    3.91974390e-01 1.75023764e-01]\n",
      "   [8.19678426e-01 4.04689252e-01 2.90538549e-01 ... 1.03628528e+00\n",
      "    1.57969666e+00 1.54522491e+00]\n",
      "   ...\n",
      "   [3.68412554e-01 7.53560305e-01 4.38117355e-01 ... 1.06537747e+00\n",
      "    1.42509174e+00 1.29873717e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.25358531e-01\n",
      "    1.05862156e-01 1.86739132e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    3.14494371e-02 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 1.85491264e-01 5.14499009e-01 ... 4.37951386e-01\n",
      "    2.43855745e-01 1.34231299e-01]\n",
      "   ...\n",
      "   [6.05657697e-05 8.45985532e-01 9.49050844e-01 ... 1.56624603e+00\n",
      "    1.22473574e+00 7.54031301e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.02780473e+00\n",
      "    9.39171910e-01 1.03748822e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    6.30821809e-02 9.27882791e-02]]\n",
      "\n",
      "  [[2.14229584e+00 1.33418393e+00 3.39271665e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.97937763e+00 1.59541643e+00 1.85492903e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.80638337e+00 1.14853513e+00 1.07631338e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [4.15570617e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 8.20807517e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.90916628e-01]\n",
      "   [9.79159534e-01 1.33588016e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    3.49074781e-01 1.41029447e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.34489030e-01 5.48053645e-02 2.88178399e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.58378649e-01 1.27784193e+00 1.05326462e+00 ... 6.33899927e-01\n",
      "    4.55242962e-01 0.00000000e+00]\n",
      "   [9.66873050e-01 6.17526054e-01 5.11544287e-01 ... 7.24110365e-01\n",
      "    4.79640484e-01 3.52772653e-01]\n",
      "   ...\n",
      "   [2.25502878e-01 7.37333298e-01 6.89847827e-01 ... 7.94446409e-01\n",
      "    9.48933542e-01 4.58682626e-01]\n",
      "   [1.60477296e-01 3.07252884e-01 5.77374041e-01 ... 6.07259750e-01\n",
      "    4.23939884e-01 9.38916206e-01]\n",
      "   [7.01187477e-02 3.65321904e-01 3.34585905e-01 ... 5.04749060e-01\n",
      "    1.36962843e+00 5.97665846e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.27401933e-01 2.75657296e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.28324485e+00\n",
      "    1.33107507e+00 1.42223716e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 9.21885431e-01 ... 1.01058507e+00\n",
      "    1.56290460e+00 1.91113734e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 6.93976998e-01 1.30653024e+00 ... 8.76341343e-01\n",
      "    1.72476351e-01 6.96518064e-01]\n",
      "   [0.00000000e+00 3.03519785e-01 8.39614570e-02 ... 3.35501730e-02\n",
      "    3.65525633e-02 3.96014154e-01]\n",
      "   [0.00000000e+00 7.61310399e-01 9.50142205e-01 ... 4.29477036e-01\n",
      "    1.89235538e-01 1.28751293e-01]]\n",
      "\n",
      "  [[1.09963566e-01 0.00000000e+00 0.00000000e+00 ... 7.09319830e-01\n",
      "    6.11239016e-01 5.56220174e-01]\n",
      "   [2.26894766e-03 3.71787757e-01 5.90598583e-01 ... 1.40480638e+00\n",
      "    1.47905266e+00 7.99979210e-01]\n",
      "   [6.70535043e-02 3.18579853e-01 8.58125031e-01 ... 2.18514276e+00\n",
      "    2.20248079e+00 1.44863033e+00]\n",
      "   ...\n",
      "   [3.00460076e-03 6.75668955e-01 8.41737092e-01 ... 1.42015314e+00\n",
      "    1.55290186e+00 9.44043815e-01]\n",
      "   [2.95199156e-01 5.47617495e-01 1.12101269e+00 ... 1.96151507e+00\n",
      "    9.87273514e-01 3.96981984e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 3.35563928e-01 ... 6.27448916e-01\n",
      "    7.89714679e-02 0.00000000e+00]]]] (11788, 2048, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "for i_batch, imgs in enumerate(dataset_loader):\n",
    "    print(i_batch)\n",
    "    #pdb.set_trace()\n",
    "    #imgs = imgs.to(device)\n",
    "    features = model_f(imgs)\n",
    "    all_features.append(features.numpy())\n",
    "    \n",
    "all_features = np.concatenate(all_features, axis=0)\n",
    "print(all_features, all_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee639a91",
   "metadata": {},
   "source": [
    "# Let's extract semantic attributes of each category (consider pre-trained word2vec model with no fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566d27b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m des \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#%% filter\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m new_des \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m des]\n\u001b[0;32m     18\u001b[0m new_des \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m new_des]\n\u001b[0;32m     19\u001b[0m new_des \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m new_des]\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m des \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#%% filter\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m new_des \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m des]\n\u001b[0;32m     18\u001b[0m new_des \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m new_des]\n\u001b[0;32m     19\u001b[0m new_des \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m new_des]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "print('Load pretrain w2v model')\n",
    "\n",
    "model_name = 'word2vec-google-news-300'#best model\n",
    "model = api.load(model_name)\n",
    "\n",
    "dim_w2v = 300\n",
    "\n",
    "#%%\n",
    "replace_word = [('spatulate','broad'),('upperparts','upper parts'),('grey','gray')] # for CUB\n",
    "\n",
    "\n",
    "path = 'C:/Sushree/Jio_Institute/Dataset/CUB_200_2011/CUB_200_2011/attributes/attributes.txt'\n",
    "df=pd.read_csv(path,sep=' ',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% filter\n",
    "new_des = [' '.join(i.split('_')) for i in des]\n",
    "new_des = [' '.join(i.split('-')) for i in new_des]\n",
    "new_des = [' '.join(i.split('::')) for i in new_des]\n",
    "new_des = [i.split('(')[0] for i in new_des]\n",
    "new_des = [i[4:] for i in new_des]\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('C:/Sushree/Jio_Institute/Dataset/CUB_200_2011/CUB_200_2011/attributes/new_des.csv')\n",
    "print('Done preprocessing attribute des')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426bda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_err = 0\n",
    "\n",
    "all_w2v = []\n",
    "for s in des:\n",
    "    print(s)\n",
    "    words = s.split(' ')\n",
    "    if words[-1] == '':     #remove empty element\n",
    "        words = words[:-1]\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    for w in words:\n",
    "        try:\n",
    "            w2v += model[w]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            counter_err += 1\n",
    "    all_w2v.append(w2v[np.newaxis,:])\n",
    "    \n",
    "print('counter_err ',counter_err)\n",
    "\n",
    "#%%\n",
    "all_w2v=np.concatenate(all_w2v,axis=0)\n",
    "#pdb.set_trace()\n",
    "#%%\n",
    "\n",
    "with open('C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/w2v/AWA2_attribute.pkl','wb') as f:\n",
    "    pickle.dump(all_w2v,f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02655a5",
   "metadata": {},
   "source": [
    "# Read the attributes and save as \"w2v_att\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_path = 'C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/w2v/AWA2_attribute.pkl'\n",
    "\n",
    "with open(attribute_path,'rb') as f:\n",
    "    w2v_att = pickle.load(f)\n",
    "assert w2v_att.shape == (85,300) # for AWA2\n",
    "print('save w2v_att')\n",
    "\n",
    "print(w2v_att, w2v_att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633a585",
   "metadata": {},
   "source": [
    "# Let's gather additional information (training, validation, and test indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% get remaining metadata\n",
    "matcontent = AWA2Dataset.matcontent\n",
    "labels = matcontent['labels'].astype(int).squeeze() - 1\n",
    "\n",
    "split_path = 'C:/Sushree/Jio_Institute/Dataset/data/xlsa17/data/AWA2/att_splits.mat'\n",
    "print(split_path)\n",
    "    \n",
    "#att_splits.mat includes the following fields:\n",
    "#-att: columns correpond to class attribute vectors normalized to have unit l2 norm, following the classes order in allclasses.txt \n",
    "#-original_att: the original class attribute vectors without normalization\n",
    "#-trainval_loc: instances indexes of train+val set features (for only seen classes) in resNet101.mat\n",
    "#-test_seen_loc: instances indexes of test set features for seen classes\n",
    "#-test_unseen_loc: instances indexes of test set features for unseen classes    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03737527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_details(split_path):\n",
    "    matcontent = sio.loadmat(split_path)\n",
    "    print(matcontent)\n",
    "    \n",
    "    trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "    print(trainval_loc, len(trainval_loc))\n",
    "\n",
    "    test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "    print(test_seen_loc, len(test_seen_loc))\n",
    "\n",
    "    test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "    print(test_unseen_loc, len(test_unseen_loc))\n",
    "    \n",
    "    att = matcontent['att'].T\n",
    "    print(att, att.shape)\n",
    "    \n",
    "    original_att = matcontent['original_att'].T\n",
    "    print(original_att, original_att.shape)\n",
    "    return trainval_loc, test_seen_loc, test_unseen_loc, att, original_att\n",
    "    \n",
    "trainval_loc, test_seen_loc, test_unseen_loc, att, original_att = get_index_details(split_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea517643",
   "metadata": {},
   "source": [
    "# Save the feature map that includes ResNet50 features, labels, training and test (seen and unseen) data indexes, semantic attributes, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = 'C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/feature_map_ResNet_101_AWA2.hdf5'\n",
    "\n",
    "f = h5py.File(save_path, \"w\")\n",
    "f.create_dataset('feature_map', data=all_features,compression=\"gzip\")\n",
    "f.create_dataset('labels', data=labels,compression=\"gzip\")\n",
    "f.create_dataset('trainval_loc', data=trainval_loc,compression=\"gzip\")\n",
    "#    f.create_dataset('train_loc', data=train_loc,compression=\"gzip\")\n",
    "#    f.create_dataset('val_unseen_loc', data=val_unseen_loc,compression=\"gzip\")\n",
    "f.create_dataset('test_seen_loc', data=test_seen_loc,compression=\"gzip\")\n",
    "f.create_dataset('test_unseen_loc', data=test_unseen_loc,compression=\"gzip\")\n",
    "f.create_dataset('att', data=att,compression=\"gzip\")\n",
    "f.create_dataset('original_att', data=original_att,compression=\"gzip\")\n",
    "f.create_dataset('w2v_att', data=w2v_att,compression=\"gzip\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/feature_map_ResNet_101_AWA2.hdf5', 'r')\n",
    "features = np.array(hf.get('feature_map'))\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb52765",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = np.array(hf.get('att'))\n",
    "print(att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647068b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bfe35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865fbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47d34245",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42717d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9e1bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e79109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb3d3925",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644fcfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f86e6b3",
   "metadata": {},
   "source": [
    "# Train the DAZLE model for AWA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046aff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.resnet as models\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import torch.optim as optim\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DAZLE import DAZLE\n",
    "from AWA2DataLoader import AWA2DataLoader\n",
    "from helper_func import eval_zs_gzsl,visualize_attention#,get_attribute_attention_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b657e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Sushree/Jio_Institute/Dataset/'\n",
    "feature_path = 'C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/'\n",
    "dataloader = AWA2DataLoader(data_path, feature_path, device = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr.append(param_group['lr'])\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 214\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "batch_size = 32\n",
    "nepoches = 100\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att # load the attribute features\n",
    "att = dataloader.att\n",
    "att[att<0] = 0\n",
    "normalize_att = dataloader.normalize_att\n",
    "\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_ = 0.1\n",
    "bias = 0\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = False\n",
    "\n",
    "seenclass = dataloader.seenclasses #load seen and unseen data\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1\n",
    "report_interval = niters//nepoches\n",
    "\n",
    "device = None\n",
    "\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_,\n",
    "            trainable_w2v,normalize_V=True,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=True)\n",
    "model.to(device)\n",
    "\n",
    "setup = {'pmp':{'init_lambda':0.1,'final_lambda':0.1,'phase':0.8},\n",
    "         'desired_mass':{'init_lambda':-1,'final_lambda':-1,'phase':0.8}}\n",
    "print(setup)\n",
    "\n",
    "params_to_update = []\n",
    "params_names = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        params_names.append(name)\n",
    "        print(\"\\t\",name)\n",
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.\n",
    "#%%\n",
    "lr_seperator = 1\n",
    "lr_factor = 1\n",
    "print('default lr {} {}x lr {}'.format(params_names[:lr_seperator],lr_factor,params_names[lr_seperator:]))\n",
    "optimizer  = optim.RMSprop( params_to_update ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_ {}'.format(lambda_))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef247f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_performance = [0,0,0,0]\n",
    "for i in range(0,niters):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "    out_package = model(batch_feature)\n",
    "    \n",
    "    in_package = out_package\n",
    "    in_package['batch_label'] = batch_label\n",
    "    \n",
    "    out_package=model.compute_loss(in_package)\n",
    "    loss,loss_CE,loss_cal = out_package['loss'],out_package['loss_CE'],out_package['loss_cal']\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%report_interval==0:\n",
    "        print('-'*30)\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss':loss.item(), 'loss_CE':loss_CE.item(),\n",
    "                         'loss_cal': loss_cal.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411c872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
