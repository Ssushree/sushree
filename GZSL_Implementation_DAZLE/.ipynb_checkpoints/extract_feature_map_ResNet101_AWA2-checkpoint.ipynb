{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "pwd = os.getcwd()\n",
    "sys.path.insert(0,pwd)\n",
    "\n",
    "print('-'*30)\n",
    "print(os.getcwd())\n",
    "print('-'*30)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.resnet as models\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c874b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8bb04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFS_path = 'C:/Sushree/Jio_Institute/Dataset/'\n",
    "\n",
    "dataset = 'AWA2'\n",
    "\n",
    "img_dir = os.path.join(NFS_path,'data/{}/'.format(dataset))\n",
    "print(img_dir)\n",
    "file_paths = os.path.join(NFS_path,'data/xlsa17/data/{}/res101.mat'.format(dataset))\n",
    "print(file_paths)\n",
    "save_path = os.path.join(NFS_path,'data/{}/feature_map_ResNet_101_{}.hdf5'.format(dataset,dataset))\n",
    "print(save_path)\n",
    "attribute_path = './w2v/{}_attribute.pkl'.format(dataset)\n",
    "print(attribute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399939ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet\"\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda:{}\".format(idx_GPU) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ref = models.resnet101(pretrained=True)\n",
    "model_ref.eval()\n",
    "\n",
    "model_f = nn.Sequential(*list(model_ref.children())[:-2])\n",
    "model_f.to(device)\n",
    "model_f.eval()\n",
    "\n",
    "for param in model_f.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "class CustomedDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir , file_paths, transform=None):\n",
    "        self.matcontent = sio.loadmat(file_paths)\n",
    "        self.image_files = np.squeeze(self.matcontent['image_files'])\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx][0]\n",
    "        image_file = os.path.join(self.img_dir, '/'.join(image_file.split('/')[5:]))\n",
    "        image = Image.open(image_file)\n",
    "        if image.mode == 'L':\n",
    "            image=image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image   \n",
    "\n",
    "input_size = 224\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "AWA2Dataset = CustomedDataset(img_dir , file_paths, data_transforms)\n",
    "dataset_loader = torch.utils.data.DataLoader(AWA2Dataset, batch_size=batch_size, shuffle=False, num_workers=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64258447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_f)\n",
    "        \n",
    "from torchsummary import summary\n",
    "summary(model_f, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0802fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd90b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = next(iter(dataset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model_f(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5827be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for i_batch, imgs in enumerate(dataset_loader):\n",
    "    print(i_batch)\n",
    "    pdb.set_trace()\n",
    "    imgs=imgs.to(device)\n",
    "    features = model_f(imgs)\n",
    "    all_features.append(features.cpu().numpy())\n",
    "all_features = np.concatenate(all_features,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0084aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#%% get remaining metadata\n",
    "matcontent = AWA2Dataset.matcontent\n",
    "labels = matcontent['labels'].astype(int).squeeze() - 1\n",
    "\n",
    "split_path = os.path.join(NFS_path,'data/xlsa17/data/{}/att_splits.mat'.format(dataset))\n",
    "matcontent = sio.loadmat(split_path)\n",
    "trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "#train_loc = matcontent['train_loc'].squeeze() - 1 #--> train_feature = TRAIN SEEN\n",
    "#val_unseen_loc = matcontent['val_loc'].squeeze() - 1 #--> test_unseen_feature = TEST UNSEEN\n",
    "test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "att = matcontent['att'].T\n",
    "original_att = matcontent['original_att'].T\n",
    "#%% construct attribute w2v\n",
    "with open(attribute_path,'rb') as f:\n",
    "    w2v_att = pickle.load(f)\n",
    "assert w2v_att.shape == (85,300)\n",
    "print('save w2v_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ed420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
