{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a150b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.resnet as models\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import pdb\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c334cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = 'C:/Sushree/Jio_Institute/Dataset/Animals_with_Attributes2/JPEGImages/'\n",
    "\n",
    "or_img = cv2.imread(os.path.join(data_path, 'tiger/tiger_10843.jpg'))\n",
    "#cv2.imshow('original image', or_img)\n",
    "\n",
    "resized_img = cv2.resize(or_img, [224, 224])\n",
    "print(np.shape(resized_img))\n",
    "\n",
    "gray_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "#cv2.imshow('grayscale image', gray_img)\n",
    "print(np.shape(gray_img))\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "# load the image\n",
    "or_img = Image.open(os.path.join(data_path, 'tiger/tiger_10843.jpg'))\n",
    "\n",
    "resized_img = or_img.resize((224, 224))\n",
    "print(np.shape(resized_img))\n",
    "\n",
    "# convert the image to grayscale\n",
    "gray_img = resized_img.convert(mode='L')\n",
    "gray_img = asarray(gray_img)\n",
    "\n",
    "# convert to numpy array\n",
    "data = asarray(resized_img)\n",
    "print(data.shape)\n",
    "\n",
    "# add channels first\n",
    "data = data.reshape((3, 224, 224))\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68283285",
   "metadata": {},
   "source": [
    "# Extract gabor features: approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72592694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as nd        \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta = theta, sigma_x = sigma, sigma_y = sigma))\n",
    "            kernels.append(kernel)\n",
    "            \n",
    "def compute_feats(image, kernels):\n",
    "    feats = np.zeros((len(kernels), 2), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = nd.convolve(image, kernel, mode='wrap')\n",
    "        feats[k, 0] = filtered.mean()\n",
    "        feats[k, 1] = filtered.var()\n",
    "    return feats    \n",
    "\n",
    "\n",
    "result = compute_feats(gray_img, kernels)\n",
    "print(np.shape(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52beb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def power(image, kernel):\n",
    "    # Normalize images for better comparison.\n",
    "    image = (image - image.mean()) / image.std()\n",
    "    print(np.shape(image))\n",
    "    return np.sqrt(nd.convolve(image, np.real(kernel), mode='wrap')**2 + nd.convolve(image, np.imag(kernel), mode='wrap')**2)\n",
    "\n",
    "# Plot a selection of the filter bank kernels and their responses.\n",
    "results = []\n",
    "kernel_params = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = gabor_kernel(frequency, theta = theta, sigma_x = sigma, sigma_y = sigma)\n",
    "            params = 'theta=%d,\\nfrequency=%.2f, \\nsigma_x=%d, \\nsigma_y=%d' % (theta * 180 / np.pi, frequency, sigma, sigma)\n",
    "            kernel_params.append(params)\n",
    "            res_pow = power(gray_img, kernel)\n",
    "            # Save kernel and the power image for each image\n",
    "            results.append((kernel, res_pow))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(results))\n",
    "print(np.shape(kernel_params))\n",
    "print(np.shape(kernel))\n",
    "print(np.shape(res_pow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0837d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resized_img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(gray_img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7185b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=16, figsize=(40, 48))\n",
    "plt.gray()\n",
    "for label, (kernel, powers), ax in zip(kernel_params, results, axes[0:]):\n",
    "    #print(np.shape(kernel))\n",
    "    #print(np.shape(powers))\n",
    "    # Plot Gabor kernel\n",
    "    ax.imshow(np.real(kernel), interpolation='nearest')\n",
    "    ax.set_xlabel(label, fontsize=7)\n",
    "    #ax.set_xticks([])\n",
    "    #ax.set_yticks([])\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=1, ncols=16, figsize=(40, 48))\n",
    "plt.gray()   \n",
    "for label, (kernel, powers), ax in zip(kernel_params, results, axes[0:]):\n",
    "    # Plot Gabor responses with the contrast normalized for each filter\n",
    "    vmin = np.min(powers)\n",
    "    print(vmin)\n",
    "    vmax = np.max(powers)\n",
    "    print(vmax)\n",
    "    ax.imshow(powers, vmin=vmin, vmax=vmax)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_tot = np.zeros(powers.shape)\n",
    "for (kernel, powers) in results:\n",
    "    power_tot = np.add(power_tot, powers)\n",
    "\n",
    "fig = plt.plot()    \n",
    "vmin = np.min(power_tot)\n",
    "print(vmin)\n",
    "vmax = np.max(power_tot)\n",
    "print(vmax)\n",
    "print(np.shape(power_tot))\n",
    "plt.imshow(power_tot, vmin=vmin, vmax=vmax)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ffc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transform_handcrafted = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(50176, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model_transform_handcrafted, (224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "to_tensor = transforms.ToTensor()\n",
    "t_power_tot = Variable(to_tensor(power_tot))\n",
    "t_power_tot = t_power_tot.to(torch.float32)\n",
    "print(t_power_tot, t_power_tot.shape)\n",
    "\n",
    "transformed_handcrafted_features = model_transform_handcrafted(t_power_tot)\n",
    "print(transformed_handcrafted_features, transformed_handcrafted_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88fc3d",
   "metadata": {},
   "source": [
    "# Find Deep visual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ef1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "model_ref = models.resnet101(pretrained=True)\n",
    "model_ref.eval()\n",
    "summary(model_ref, (3, 224, 224))\n",
    "\n",
    "model_f = nn.Sequential(*list(model_ref.children())[:-2])\n",
    "model_f.eval()\n",
    "\n",
    "for param in model_f.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "summary(model_f, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77cc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "or_img = Image.open(os.path.join(data_path, 'tiger/tiger_10843.jpg'))\n",
    "\n",
    "scaler = transforms.Resize((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "t_img = Variable(normalize(to_tensor(scaler(or_img))).unsqueeze(0))\n",
    "\n",
    "print(t_img, t_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4c997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = model_f(t_img)\n",
    "print(features, features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transform_visual = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(100352, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model_transform_visual, (2048, 7, 7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a2fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_visual_features = model_transform_visual(features)\n",
    "print(transformed_visual_features, transformed_visual_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64dfd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2f0c7d7",
   "metadata": {},
   "source": [
    "# Find correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel = gabor_kernel(0.5, theta = theta / 4. * np.pi, sigma_x = 0.1, sigma_y = 0.9)\n",
    "#result = power(gray_img, kernel)\n",
    "#print(np.shape(result))\n",
    "#result_vec = np.reshape(result, [1, np.shape(result)[1]*np.shape(result)[0]])\n",
    "#print(np.shape(result_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "#transformed_visual_features = transformed_visual_features.numpy()\n",
    "transformed_handcrafted_features = Tensor.detach(transformed_handcrafted_features).numpy()\n",
    "\n",
    "corr_coeff = np.corrcoef(transformed_visual_features, transformed_handcrafted_features)\n",
    "print(corr_coeff)\n",
    "print(np.shape(corr_coeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc2388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(corr_coeff, vmin=-1, vmax=1, annot=True,cmap=\"rocket_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b08af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de21395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93bd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ab8a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c7b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
