{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d758d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as nd        \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt    \n",
    "from numpy.linalg import norm\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "import graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490b32c",
   "metadata": {},
   "source": [
    "# Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "018b6cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/Sushree/Dataset/Animals_with_Attributes2/JPEGImages/\n",
      "50\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "37322\n"
     ]
    }
   ],
   "source": [
    "data_path = 'E:/Sushree/Dataset/'\n",
    "\n",
    "img_path = os.path.join(data_path,'Animals_with_Attributes2/JPEGImages/')\n",
    "print(img_path)\n",
    "\n",
    "print(len(os.listdir(img_path)))\n",
    "\n",
    "def get_imlist(path, option):\n",
    "    if option == 'jpg':\n",
    "        return[os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "    \n",
    "def prepare_labels(img_path, option):\n",
    "    folder_list = os.listdir(img_path)\n",
    "    num_classes = len(folder_list)\n",
    "    labels = []\n",
    "    for i in range(len(folder_list)):\n",
    "        print(i)\n",
    "        img_list = get_imlist(os.path.join(img_path, folder_list[i]), option)\n",
    "        for j in range(len(img_list)):\n",
    "            labels.append(i)\n",
    "    labels = to_categorical(labels, num_classes)                             \n",
    "    return labels  \n",
    "\n",
    "width, height, ch = 224, 224, 3\n",
    "labels = prepare_labels(img_path, 'jpg')\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cefdf",
   "metadata": {},
   "source": [
    "# load gabor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e77aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.6899301e-01  9.8722791e-03 -1.6094959e-01 ...  1.4629345e-02\n",
      "  -4.3485691e-03  4.5536556e-03]\n",
      " [-1.5022326e-01 -1.0419858e-01  6.6463433e-02 ...  1.3577098e-02\n",
      "  -2.2116555e-03 -4.4724456e-04]\n",
      " [ 6.1230052e-01  4.7015655e-01 -1.7734341e-01 ...  2.6094878e-02\n",
      "   1.4556493e-02 -5.9137302e-03]\n",
      " ...\n",
      " [ 6.6859767e-02 -3.9864212e-01 -2.4804704e-01 ...  2.2252318e-03\n",
      "  -2.0175776e-03  9.7561618e-03]\n",
      " [-7.0907420e-01  3.4373632e-01 -4.1183540e-01 ...  1.7656519e-03\n",
      "   2.6551452e-03  7.2244145e-03]\n",
      " [-3.1808448e-01 -6.1351520e-01 -2.0063773e-01 ...  4.4152541e-03\n",
      "  -3.2643334e-03 -9.9789612e-03]] (23527, 4096)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_gabor_feature_transformed = np.load('C:/Users/Admin/Sushree_Codes/Sush/Results/train_gabor_feature_8_kernels_transformed_poly_4096.npy')\n",
    "print(train_gabor_feature_transformed, train_gabor_feature_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "322a60e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.51186728e-01 -9.88067091e-02 -2.52698511e-01 ...  9.77901891e-02\n",
      "  -5.02952114e-02 -3.63892764e-02]\n",
      " [-4.33573425e-01 -1.64868236e-01 -8.72340277e-02 ...  8.97908807e-02\n",
      "  -3.47408950e-02 -7.32663646e-02]\n",
      " [ 3.29547226e-02  1.67756379e-01 -2.64626563e-01 ...  1.84952453e-01\n",
      "   8.73122662e-02 -1.13576725e-01]\n",
      " ...\n",
      " [-3.00757438e-01 -3.35388482e-01 -3.16070139e-01 ...  3.49272229e-03\n",
      "  -3.33282277e-02  1.97447091e-03]\n",
      " [-7.75490224e-01  9.45429951e-02 -4.35241669e-01 ... -1.05705112e-06\n",
      "   6.83911145e-04 -1.66948624e-02]\n",
      " [-5.36274433e-01 -4.59827274e-01 -2.81575382e-01 ...  2.01415159e-02\n",
      "  -4.24032025e-02 -1.43554091e-01]] (23527, 4096)\n",
      "1.0000002\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_gabor_feature_transformed = scaler.fit_transform(train_gabor_feature_transformed)\n",
    "print(train_gabor_feature_transformed, train_gabor_feature_transformed.shape)\n",
    "print(np.max(train_gabor_feature_transformed))\n",
    "print(np.min(train_gabor_feature_transformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f88921",
   "metadata": {},
   "source": [
    "# Define classifier 1 (Dense layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7aa67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4096)]            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                51250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,321,394\n",
      "Trainable params: 27,321,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_features, feat_length = train_gabor_feature_transformed.shape\n",
    "\n",
    "feat = Input(shape = (feat_length))\n",
    "\n",
    "dense1 = Dense(4096, activation='tanh')(feat)\n",
    "#drop1 = Dropout(0.1)(dense1)\n",
    "dense2 = Dense(2048, activation='tanh')(dense1)\n",
    "#drop2 = Dropout(0.1)(dense2)\n",
    "dense3 = Dense(1024, activation='tanh')(dense2)\n",
    "#drop3 = Dropout(0.1)(dense3)\n",
    "output = Dense(50, activation='softmax')(dense3)\n",
    "\n",
    "\n",
    "classify_gabor = Model(inputs = feat, outputs = output)\n",
    "classify_gabor.summary()\n",
    "\n",
    "opt = SGD(learning_rate = 0.001, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "classify_gabor.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6121455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21482 11452 21448 ...  7992 10388  5160] 23527\n",
      "(23527, 4096)\n",
      "23527\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_path = os.path.join(data_path,'data/xlsa17/data/AWA2/att_splits.mat')\n",
    "matcontent = sio.loadmat(split_path)\n",
    "trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "\n",
    "#print(matcontent)\n",
    "\n",
    "print(trainval_loc, len(trainval_loc))\n",
    "\n",
    "train_features = train_gabor_feature_transformed\n",
    "train_labels = labels[trainval_loc]\n",
    "print(train_features.shape)\n",
    "print(len(train_labels))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "batch_size = 16\n",
    "train_gen = DataGenerator(X_train, y_train, batch_size)   \n",
    "val_gen = DataGenerator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f6a1451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 3.8519 - accuracy: 0.0368 - val_loss: 3.7959 - val_accuracy: 0.0347\n",
      "Epoch 2/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7412 - accuracy: 0.0445 - val_loss: 3.7248 - val_accuracy: 0.0486\n",
      "Epoch 3/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6862 - accuracy: 0.0420 - val_loss: 3.7033 - val_accuracy: 0.0486\n",
      "Epoch 4/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6575 - accuracy: 0.0522 - val_loss: 3.6955 - val_accuracy: 0.0486\n",
      "Epoch 5/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6476 - accuracy: 0.0428 - val_loss: 3.6699 - val_accuracy: 0.0556\n",
      "Epoch 6/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6311 - accuracy: 0.0591 - val_loss: 3.6576 - val_accuracy: 0.0451\n",
      "Epoch 7/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6300 - accuracy: 0.0454 - val_loss: 3.6459 - val_accuracy: 0.0451\n",
      "Epoch 8/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6139 - accuracy: 0.0616 - val_loss: 3.6474 - val_accuracy: 0.0486\n",
      "Epoch 9/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5831 - accuracy: 0.0565 - val_loss: 3.6438 - val_accuracy: 0.0451\n",
      "Epoch 10/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6614 - accuracy: 0.0462 - val_loss: 3.6391 - val_accuracy: 0.0660\n",
      "Epoch 11/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5934 - accuracy: 0.0493 - val_loss: 3.6363 - val_accuracy: 0.0347\n",
      "Epoch 12/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5941 - accuracy: 0.0479 - val_loss: 3.6367 - val_accuracy: 0.0243\n",
      "Epoch 13/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5889 - accuracy: 0.0497 - val_loss: 3.6349 - val_accuracy: 0.0486\n",
      "Epoch 14/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6072 - accuracy: 0.0659 - val_loss: 3.6332 - val_accuracy: 0.0590\n",
      "Epoch 15/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6031 - accuracy: 0.0539 - val_loss: 3.6452 - val_accuracy: 0.0590\n",
      "Epoch 16/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5943 - accuracy: 0.0428 - val_loss: 3.6287 - val_accuracy: 0.0660\n",
      "Epoch 17/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5905 - accuracy: 0.0565 - val_loss: 3.6248 - val_accuracy: 0.0486\n",
      "Epoch 18/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5626 - accuracy: 0.0616 - val_loss: 3.6258 - val_accuracy: 0.0556\n",
      "Epoch 19/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5969 - accuracy: 0.0599 - val_loss: 3.6137 - val_accuracy: 0.0486\n",
      "Epoch 20/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5997 - accuracy: 0.0539 - val_loss: 3.6146 - val_accuracy: 0.0486\n",
      "Epoch 21/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5920 - accuracy: 0.0488 - val_loss: 3.6131 - val_accuracy: 0.0556\n",
      "Epoch 22/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5921 - accuracy: 0.0616 - val_loss: 3.6176 - val_accuracy: 0.0486\n",
      "Epoch 23/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6002 - accuracy: 0.0548 - val_loss: 3.6220 - val_accuracy: 0.0278\n",
      "Epoch 24/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5822 - accuracy: 0.0548 - val_loss: 3.6162 - val_accuracy: 0.0278\n",
      "Epoch 25/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5773 - accuracy: 0.0488 - val_loss: 3.6155 - val_accuracy: 0.0556\n",
      "Epoch 26/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5700 - accuracy: 0.0651 - val_loss: 3.6116 - val_accuracy: 0.0556\n",
      "Epoch 27/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5715 - accuracy: 0.0582 - val_loss: 3.6142 - val_accuracy: 0.0486\n",
      "Epoch 28/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5902 - accuracy: 0.0616 - val_loss: 3.6132 - val_accuracy: 0.0451\n",
      "Epoch 29/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5853 - accuracy: 0.0596 - val_loss: 3.6218 - val_accuracy: 0.0486\n",
      "Epoch 30/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5709 - accuracy: 0.0693 - val_loss: 3.6262 - val_accuracy: 0.0521\n",
      "Epoch 31/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5659 - accuracy: 0.0591 - val_loss: 3.6228 - val_accuracy: 0.0521\n",
      "Epoch 32/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5742 - accuracy: 0.0719 - val_loss: 3.6244 - val_accuracy: 0.0729\n",
      "Epoch 33/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5622 - accuracy: 0.0642 - val_loss: 3.6209 - val_accuracy: 0.0451\n",
      "Epoch 34/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5565 - accuracy: 0.0674 - val_loss: 3.6209 - val_accuracy: 0.0556\n",
      "Epoch 35/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5591 - accuracy: 0.0574 - val_loss: 3.6118 - val_accuracy: 0.0486\n",
      "Epoch 36/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5612 - accuracy: 0.0668 - val_loss: 3.6108 - val_accuracy: 0.0660\n",
      "Epoch 37/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5532 - accuracy: 0.0622 - val_loss: 3.6126 - val_accuracy: 0.0590\n",
      "Epoch 38/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5448 - accuracy: 0.0745 - val_loss: 3.6150 - val_accuracy: 0.0660\n",
      "Epoch 39/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5503 - accuracy: 0.0719 - val_loss: 3.6096 - val_accuracy: 0.0590\n",
      "Epoch 40/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5521 - accuracy: 0.0771 - val_loss: 3.6010 - val_accuracy: 0.0660\n",
      "Epoch 41/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5546 - accuracy: 0.0608 - val_loss: 3.6079 - val_accuracy: 0.0729\n",
      "Epoch 42/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5535 - accuracy: 0.0676 - val_loss: 3.6042 - val_accuracy: 0.0417\n",
      "Epoch 43/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5577 - accuracy: 0.0625 - val_loss: 3.6015 - val_accuracy: 0.0382\n",
      "Epoch 44/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5319 - accuracy: 0.0676 - val_loss: 3.5942 - val_accuracy: 0.0486\n",
      "Epoch 45/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5543 - accuracy: 0.0599 - val_loss: 3.5934 - val_accuracy: 0.0590\n",
      "Epoch 46/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5198 - accuracy: 0.0745 - val_loss: 3.6061 - val_accuracy: 0.0451\n",
      "Epoch 47/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5384 - accuracy: 0.0882 - val_loss: 3.5994 - val_accuracy: 0.0556\n",
      "Epoch 48/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5464 - accuracy: 0.0685 - val_loss: 3.5979 - val_accuracy: 0.0625\n",
      "Epoch 49/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5354 - accuracy: 0.0838 - val_loss: 3.6009 - val_accuracy: 0.0417\n",
      "Epoch 50/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5420 - accuracy: 0.0771 - val_loss: 3.5915 - val_accuracy: 0.0590\n",
      "Epoch 51/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5314 - accuracy: 0.0736 - val_loss: 3.6011 - val_accuracy: 0.0729\n",
      "Epoch 52/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5632 - accuracy: 0.0762 - val_loss: 3.5946 - val_accuracy: 0.0660\n",
      "Epoch 53/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5334 - accuracy: 0.0779 - val_loss: 3.5907 - val_accuracy: 0.0590\n",
      "Epoch 54/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5264 - accuracy: 0.0702 - val_loss: 3.6011 - val_accuracy: 0.0556\n",
      "Epoch 55/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5411 - accuracy: 0.0693 - val_loss: 3.5944 - val_accuracy: 0.0694\n",
      "Epoch 56/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5122 - accuracy: 0.0856 - val_loss: 3.5959 - val_accuracy: 0.0590\n",
      "Epoch 57/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5304 - accuracy: 0.0805 - val_loss: 3.5899 - val_accuracy: 0.0521\n",
      "Epoch 58/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5138 - accuracy: 0.0788 - val_loss: 3.5960 - val_accuracy: 0.0660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5306 - accuracy: 0.0813 - val_loss: 3.5977 - val_accuracy: 0.0451\n",
      "Epoch 60/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5205 - accuracy: 0.0830 - val_loss: 3.5968 - val_accuracy: 0.0625\n",
      "Epoch 61/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5073 - accuracy: 0.0908 - val_loss: 3.5969 - val_accuracy: 0.0556\n",
      "Epoch 62/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5453 - accuracy: 0.0762 - val_loss: 3.5819 - val_accuracy: 0.0451\n",
      "Epoch 63/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4960 - accuracy: 0.0830 - val_loss: 3.5916 - val_accuracy: 0.0417\n",
      "Epoch 64/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5340 - accuracy: 0.0873 - val_loss: 3.5870 - val_accuracy: 0.0556\n",
      "Epoch 65/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5099 - accuracy: 0.0736 - val_loss: 3.5899 - val_accuracy: 0.0521\n",
      "Epoch 66/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5132 - accuracy: 0.0813 - val_loss: 3.5886 - val_accuracy: 0.0451\n",
      "Epoch 67/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5075 - accuracy: 0.0762 - val_loss: 3.5945 - val_accuracy: 0.0625\n",
      "Epoch 68/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5067 - accuracy: 0.0925 - val_loss: 3.5960 - val_accuracy: 0.0590\n",
      "Epoch 69/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4799 - accuracy: 0.0873 - val_loss: 3.5991 - val_accuracy: 0.0625\n",
      "Epoch 70/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5053 - accuracy: 0.0882 - val_loss: 3.5912 - val_accuracy: 0.0729\n",
      "Epoch 71/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5065 - accuracy: 0.0933 - val_loss: 3.5928 - val_accuracy: 0.0660\n",
      "Epoch 72/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4824 - accuracy: 0.1029 - val_loss: 3.6048 - val_accuracy: 0.0590\n",
      "Epoch 73/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4945 - accuracy: 0.0950 - val_loss: 3.5906 - val_accuracy: 0.0486\n",
      "Epoch 74/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5006 - accuracy: 0.1019 - val_loss: 3.5805 - val_accuracy: 0.0660\n",
      "Epoch 75/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5132 - accuracy: 0.0779 - val_loss: 3.5806 - val_accuracy: 0.0660\n",
      "Epoch 76/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4654 - accuracy: 0.1053 - val_loss: 3.5922 - val_accuracy: 0.0764\n",
      "Epoch 77/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4870 - accuracy: 0.0830 - val_loss: 3.5835 - val_accuracy: 0.0660\n",
      "Epoch 78/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4952 - accuracy: 0.0916 - val_loss: 3.5864 - val_accuracy: 0.0625\n",
      "Epoch 79/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4869 - accuracy: 0.0993 - val_loss: 3.5833 - val_accuracy: 0.0694\n",
      "Epoch 80/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4716 - accuracy: 0.0976 - val_loss: 3.5951 - val_accuracy: 0.0451\n",
      "Epoch 81/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4770 - accuracy: 0.1098 - val_loss: 3.5963 - val_accuracy: 0.0660\n",
      "Epoch 82/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4869 - accuracy: 0.0864 - val_loss: 3.5883 - val_accuracy: 0.0382\n",
      "Epoch 83/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4786 - accuracy: 0.0890 - val_loss: 3.5688 - val_accuracy: 0.0625\n",
      "Epoch 84/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4650 - accuracy: 0.0925 - val_loss: 3.5668 - val_accuracy: 0.0799\n",
      "Epoch 85/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4816 - accuracy: 0.1027 - val_loss: 3.5697 - val_accuracy: 0.0625\n",
      "Epoch 86/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4828 - accuracy: 0.1079 - val_loss: 3.5702 - val_accuracy: 0.0521\n",
      "Epoch 87/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4685 - accuracy: 0.0993 - val_loss: 3.5724 - val_accuracy: 0.0590\n",
      "Epoch 88/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4837 - accuracy: 0.0976 - val_loss: 3.5711 - val_accuracy: 0.0556\n",
      "Epoch 89/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4873 - accuracy: 0.0916 - val_loss: 3.5783 - val_accuracy: 0.0799\n",
      "Epoch 90/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4631 - accuracy: 0.1156 - val_loss: 3.5734 - val_accuracy: 0.0799\n",
      "Epoch 91/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4742 - accuracy: 0.0899 - val_loss: 3.5856 - val_accuracy: 0.0451\n",
      "Epoch 92/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4675 - accuracy: 0.1010 - val_loss: 3.5891 - val_accuracy: 0.0556\n",
      "Epoch 93/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4435 - accuracy: 0.1113 - val_loss: 3.5771 - val_accuracy: 0.0625\n",
      "Epoch 94/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4708 - accuracy: 0.0908 - val_loss: 3.5759 - val_accuracy: 0.0729\n",
      "Epoch 95/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4739 - accuracy: 0.0925 - val_loss: 3.5785 - val_accuracy: 0.0660\n",
      "Epoch 96/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4840 - accuracy: 0.0933 - val_loss: 3.5653 - val_accuracy: 0.0729\n",
      "Epoch 97/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4468 - accuracy: 0.1344 - val_loss: 3.5723 - val_accuracy: 0.0799\n",
      "Epoch 98/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4807 - accuracy: 0.1036 - val_loss: 3.5723 - val_accuracy: 0.0833\n",
      "Epoch 99/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4603 - accuracy: 0.1156 - val_loss: 3.5728 - val_accuracy: 0.0764\n",
      "Epoch 100/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4402 - accuracy: 0.1190 - val_loss: 3.5653 - val_accuracy: 0.0799\n",
      "Epoch 101/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4514 - accuracy: 0.1070 - val_loss: 3.5615 - val_accuracy: 0.0833\n",
      "Epoch 102/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4590 - accuracy: 0.1276 - val_loss: 3.5636 - val_accuracy: 0.0625\n",
      "Epoch 103/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4402 - accuracy: 0.1184 - val_loss: 3.5630 - val_accuracy: 0.0694\n",
      "Epoch 104/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4463 - accuracy: 0.1062 - val_loss: 3.5639 - val_accuracy: 0.0625\n",
      "Epoch 105/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4467 - accuracy: 0.1130 - val_loss: 3.5629 - val_accuracy: 0.0938\n",
      "Epoch 106/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4270 - accuracy: 0.1224 - val_loss: 3.5569 - val_accuracy: 0.0729\n",
      "Epoch 107/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4324 - accuracy: 0.1010 - val_loss: 3.5725 - val_accuracy: 0.0799\n",
      "Epoch 108/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4546 - accuracy: 0.1096 - val_loss: 3.5576 - val_accuracy: 0.0729\n",
      "Epoch 109/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4299 - accuracy: 0.1139 - val_loss: 3.5612 - val_accuracy: 0.0694\n",
      "Epoch 110/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4409 - accuracy: 0.1259 - val_loss: 3.5630 - val_accuracy: 0.0903\n",
      "Epoch 111/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4389 - accuracy: 0.1276 - val_loss: 3.5609 - val_accuracy: 0.0729\n",
      "Epoch 112/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4467 - accuracy: 0.1173 - val_loss: 3.5517 - val_accuracy: 0.0556\n",
      "Epoch 113/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4615 - accuracy: 0.1156 - val_loss: 3.5625 - val_accuracy: 0.0938\n",
      "Epoch 114/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4217 - accuracy: 0.1438 - val_loss: 3.5609 - val_accuracy: 0.0729\n",
      "Epoch 115/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4484 - accuracy: 0.1404 - val_loss: 3.5573 - val_accuracy: 0.0694\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4259 - accuracy: 0.1344 - val_loss: 3.5603 - val_accuracy: 0.0799\n",
      "Epoch 117/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4167 - accuracy: 0.1396 - val_loss: 3.5652 - val_accuracy: 0.0729\n",
      "Epoch 118/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4118 - accuracy: 0.1344 - val_loss: 3.5761 - val_accuracy: 0.0764\n",
      "Epoch 119/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4481 - accuracy: 0.1322 - val_loss: 3.5625 - val_accuracy: 0.0625\n",
      "Epoch 120/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4194 - accuracy: 0.1224 - val_loss: 3.5499 - val_accuracy: 0.0556\n",
      "Epoch 121/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4250 - accuracy: 0.1430 - val_loss: 3.5530 - val_accuracy: 0.0868\n",
      "Epoch 122/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4203 - accuracy: 0.1396 - val_loss: 3.5455 - val_accuracy: 0.0903\n",
      "Epoch 123/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4028 - accuracy: 0.1310 - val_loss: 3.5544 - val_accuracy: 0.0694\n",
      "Epoch 124/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4218 - accuracy: 0.1344 - val_loss: 3.5598 - val_accuracy: 0.0660\n",
      "Epoch 125/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4086 - accuracy: 0.1182 - val_loss: 3.5549 - val_accuracy: 0.0625\n",
      "Epoch 126/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4345 - accuracy: 0.1396 - val_loss: 3.5424 - val_accuracy: 0.0694\n",
      "Epoch 127/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4262 - accuracy: 0.1276 - val_loss: 3.5383 - val_accuracy: 0.0764\n",
      "Epoch 128/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3927 - accuracy: 0.1541 - val_loss: 3.5350 - val_accuracy: 0.0938\n",
      "Epoch 129/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4026 - accuracy: 0.1550 - val_loss: 3.5420 - val_accuracy: 0.0764\n",
      "Epoch 130/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3960 - accuracy: 0.1413 - val_loss: 3.5491 - val_accuracy: 0.0764\n",
      "Epoch 131/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4169 - accuracy: 0.1310 - val_loss: 3.5356 - val_accuracy: 0.0903\n",
      "Epoch 132/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4005 - accuracy: 0.1490 - val_loss: 3.5434 - val_accuracy: 0.0521\n",
      "Epoch 133/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4032 - accuracy: 0.1378 - val_loss: 3.5482 - val_accuracy: 0.0868\n",
      "Epoch 134/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4058 - accuracy: 0.1447 - val_loss: 3.5355 - val_accuracy: 0.1042\n",
      "Epoch 135/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4087 - accuracy: 0.1498 - val_loss: 3.5562 - val_accuracy: 0.0868\n",
      "Epoch 136/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3947 - accuracy: 0.1413 - val_loss: 3.5452 - val_accuracy: 0.0764\n",
      "Epoch 137/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3952 - accuracy: 0.1447 - val_loss: 3.5437 - val_accuracy: 0.0868\n",
      "Epoch 138/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3733 - accuracy: 0.1498 - val_loss: 3.5486 - val_accuracy: 0.0833\n",
      "Epoch 139/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3939 - accuracy: 0.1276 - val_loss: 3.5518 - val_accuracy: 0.0799\n",
      "Epoch 140/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3817 - accuracy: 0.1396 - val_loss: 3.5426 - val_accuracy: 0.0694\n",
      "Epoch 141/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3733 - accuracy: 0.1515 - val_loss: 3.5403 - val_accuracy: 0.0903\n",
      "Epoch 142/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3567 - accuracy: 0.1575 - val_loss: 3.5411 - val_accuracy: 0.0833\n",
      "Epoch 143/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4160 - accuracy: 0.1190 - val_loss: 3.5315 - val_accuracy: 0.0868\n",
      "Epoch 144/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3595 - accuracy: 0.1668 - val_loss: 3.5232 - val_accuracy: 0.0833\n",
      "Epoch 145/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3482 - accuracy: 0.1524 - val_loss: 3.5288 - val_accuracy: 0.0764\n",
      "Epoch 146/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3781 - accuracy: 0.1575 - val_loss: 3.5297 - val_accuracy: 0.0764\n",
      "Epoch 147/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3938 - accuracy: 0.1447 - val_loss: 3.5323 - val_accuracy: 0.0903\n",
      "Epoch 148/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3756 - accuracy: 0.1413 - val_loss: 3.5348 - val_accuracy: 0.0903\n",
      "Epoch 149/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3529 - accuracy: 0.1695 - val_loss: 3.5396 - val_accuracy: 0.0833\n",
      "Epoch 150/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3736 - accuracy: 0.1651 - val_loss: 3.5297 - val_accuracy: 0.0938\n",
      "Epoch 151/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3517 - accuracy: 0.1781 - val_loss: 3.5368 - val_accuracy: 0.0903\n",
      "Epoch 152/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3722 - accuracy: 0.1481 - val_loss: 3.5267 - val_accuracy: 0.1042\n",
      "Epoch 153/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3753 - accuracy: 0.1507 - val_loss: 3.5354 - val_accuracy: 0.0833\n",
      "Epoch 154/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3420 - accuracy: 0.1498 - val_loss: 3.5464 - val_accuracy: 0.0938\n",
      "Epoch 155/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3422 - accuracy: 0.1601 - val_loss: 3.5389 - val_accuracy: 0.0764\n",
      "Epoch 156/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3443 - accuracy: 0.1634 - val_loss: 3.5231 - val_accuracy: 0.0903\n",
      "Epoch 157/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3681 - accuracy: 0.1584 - val_loss: 3.5256 - val_accuracy: 0.0799\n",
      "Epoch 158/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3539 - accuracy: 0.1764 - val_loss: 3.5201 - val_accuracy: 0.1007\n",
      "Epoch 159/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3058 - accuracy: 0.1772 - val_loss: 3.5208 - val_accuracy: 0.0799\n",
      "Epoch 160/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3738 - accuracy: 0.1729 - val_loss: 3.5218 - val_accuracy: 0.0972\n",
      "Epoch 161/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3390 - accuracy: 0.1455 - val_loss: 3.5221 - val_accuracy: 0.0764\n",
      "Epoch 162/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3546 - accuracy: 0.1524 - val_loss: 3.5259 - val_accuracy: 0.0938\n",
      "Epoch 163/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3291 - accuracy: 0.1592 - val_loss: 3.5272 - val_accuracy: 0.0833\n",
      "Epoch 164/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3385 - accuracy: 0.1772 - val_loss: 3.5186 - val_accuracy: 0.0938\n",
      "Epoch 165/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3182 - accuracy: 0.1772 - val_loss: 3.5244 - val_accuracy: 0.0868\n",
      "Epoch 166/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3328 - accuracy: 0.1533 - val_loss: 3.5266 - val_accuracy: 0.0694\n",
      "Epoch 167/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3343 - accuracy: 0.1712 - val_loss: 3.5147 - val_accuracy: 0.0729\n",
      "Epoch 168/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3235 - accuracy: 0.2038 - val_loss: 3.5265 - val_accuracy: 0.0868\n",
      "Epoch 169/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3355 - accuracy: 0.1721 - val_loss: 3.5212 - val_accuracy: 0.0868\n",
      "Epoch 170/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3145 - accuracy: 0.1866 - val_loss: 3.5183 - val_accuracy: 0.0972\n",
      "Epoch 171/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3047 - accuracy: 0.1678 - val_loss: 3.5164 - val_accuracy: 0.0799\n",
      "Epoch 172/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2991 - accuracy: 0.1610 - val_loss: 3.5070 - val_accuracy: 0.0903\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3131 - accuracy: 0.1772 - val_loss: 3.5141 - val_accuracy: 0.0972\n",
      "Epoch 174/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3030 - accuracy: 0.1832 - val_loss: 3.5193 - val_accuracy: 0.0764\n",
      "Epoch 175/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3148 - accuracy: 0.1661 - val_loss: 3.5099 - val_accuracy: 0.0660\n",
      "Epoch 176/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3003 - accuracy: 0.1961 - val_loss: 3.5151 - val_accuracy: 0.0799\n",
      "Epoch 177/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3028 - accuracy: 0.1704 - val_loss: 3.5087 - val_accuracy: 0.0764\n",
      "Epoch 178/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2909 - accuracy: 0.1986 - val_loss: 3.5127 - val_accuracy: 0.0868\n",
      "Epoch 179/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3137 - accuracy: 0.1824 - val_loss: 3.5054 - val_accuracy: 0.0799\n",
      "Epoch 180/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2692 - accuracy: 0.2038 - val_loss: 3.4978 - val_accuracy: 0.0868\n",
      "Epoch 181/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2915 - accuracy: 0.1995 - val_loss: 3.4979 - val_accuracy: 0.0903\n",
      "Epoch 182/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2745 - accuracy: 0.1798 - val_loss: 3.5048 - val_accuracy: 0.0868\n",
      "Epoch 183/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3087 - accuracy: 0.1909 - val_loss: 3.5014 - val_accuracy: 0.1042\n",
      "Epoch 184/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2917 - accuracy: 0.1712 - val_loss: 3.4999 - val_accuracy: 0.1007\n",
      "Epoch 185/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2808 - accuracy: 0.2014 - val_loss: 3.4920 - val_accuracy: 0.0903\n",
      "Epoch 186/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3039 - accuracy: 0.1901 - val_loss: 3.4911 - val_accuracy: 0.0972\n",
      "Epoch 187/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2690 - accuracy: 0.2021 - val_loss: 3.5018 - val_accuracy: 0.0799\n",
      "Epoch 188/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2982 - accuracy: 0.1781 - val_loss: 3.4928 - val_accuracy: 0.0938\n",
      "Epoch 189/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2660 - accuracy: 0.1918 - val_loss: 3.5060 - val_accuracy: 0.0868\n",
      "Epoch 190/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2609 - accuracy: 0.1875 - val_loss: 3.5109 - val_accuracy: 0.0833\n",
      "Epoch 191/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2925 - accuracy: 0.1738 - val_loss: 3.5033 - val_accuracy: 0.0868\n",
      "Epoch 192/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2371 - accuracy: 0.1969 - val_loss: 3.4949 - val_accuracy: 0.0903\n",
      "Epoch 193/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2750 - accuracy: 0.1978 - val_loss: 3.4820 - val_accuracy: 0.0868\n",
      "Epoch 194/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2795 - accuracy: 0.1746 - val_loss: 3.4896 - val_accuracy: 0.0764\n",
      "Epoch 195/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2829 - accuracy: 0.1953 - val_loss: 3.4888 - val_accuracy: 0.0799\n",
      "Epoch 196/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2857 - accuracy: 0.1806 - val_loss: 3.4967 - val_accuracy: 0.0729\n",
      "Epoch 197/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2534 - accuracy: 0.1943 - val_loss: 3.4963 - val_accuracy: 0.0833\n",
      "Epoch 198/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2760 - accuracy: 0.1978 - val_loss: 3.4767 - val_accuracy: 0.0903\n",
      "Epoch 199/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2831 - accuracy: 0.1995 - val_loss: 3.4861 - val_accuracy: 0.1007\n",
      "Epoch 200/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2817 - accuracy: 0.1832 - val_loss: 3.4888 - val_accuracy: 0.0972\n"
     ]
    }
   ],
   "source": [
    "train_summary = classify_gabor.fit(train_gen, epochs = 200, verbose = 1, callbacks = None, validation_data = val_gen, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen)//batch_size, \n",
    "                              validation_steps = len(val_gen)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b72ab9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'model_classify_gabor_norm_dense_4096-2048-1024_AWA2_drop0.1_200eph_16bch_tanh_0.0001lr'\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush/Results/'\n",
    "\n",
    "classify_gabor.save_weights(save_path + 'Wt_' + name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e36237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7db054a7",
   "metadata": {},
   "source": [
    "# Define classifier 2 (Conv 1x1 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba62c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 2.51186728e-01 -9.88067091e-02 -2.52698511e-01 ...  9.77901891e-02\n",
      "    -5.02952114e-02 -3.63892764e-02]]]\n",
      "\n",
      "\n",
      " [[[-4.33573425e-01 -1.64868236e-01 -8.72340277e-02 ...  8.97908807e-02\n",
      "    -3.47408950e-02 -7.32663646e-02]]]\n",
      "\n",
      "\n",
      " [[[ 3.29547226e-02  1.67756379e-01 -2.64626563e-01 ...  1.84952453e-01\n",
      "     8.73122662e-02 -1.13576725e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-3.00757438e-01 -3.35388482e-01 -3.16070139e-01 ...  3.49272229e-03\n",
      "    -3.33282277e-02  1.97447091e-03]]]\n",
      "\n",
      "\n",
      " [[[-7.75490224e-01  9.45429951e-02 -4.35241669e-01 ... -1.05705112e-06\n",
      "     6.83911145e-04 -1.66948624e-02]]]\n",
      "\n",
      "\n",
      " [[[-5.36274433e-01 -4.59827274e-01 -2.81575382e-01 ...  2.01415159e-02\n",
      "    -4.24032025e-02 -1.43554091e-01]]]] (23527, 1, 1, 4096)\n"
     ]
    }
   ],
   "source": [
    "transformed_handcrafted_features = np.expand_dims(train_gabor_feature_transformed, axis=1)\n",
    "transformed_handcrafted_features = np.expand_dims(transformed_handcrafted_features, axis=1)\n",
    "print(transformed_handcrafted_features, transformed_handcrafted_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3c01853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 1, 1, 4096)]      0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 1, 1, 4096)        16781312  \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 1, 1, 4096)        0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 1, 1, 2048)        8390656   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 1, 1, 2048)        0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 50)                51250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,321,394\n",
      "Trainable params: 27,321,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_features, feat_height, feat_width, feat_length = transformed_handcrafted_features.shape\n",
    "\n",
    "feat = Input(shape = (feat_height, feat_width, feat_length))\n",
    "\n",
    "\n",
    "conv1 = Conv2D(4096, 1, activation='tanh')(feat)\n",
    "drop1 = Dropout(0.5)(conv1)\n",
    "conv2 = Conv2D(2048, 1,  activation='tanh')(drop1)\n",
    "drop2 = Dropout(0.5)(conv2)\n",
    "conv3 = Conv2D(1024, 1,  activation='tanh')(drop2)\n",
    "drop3 = Dropout(0.5)(conv3)\n",
    "flat = Flatten()(drop3)\n",
    "output = Dense(50, activation='softmax')(flat)\n",
    "\n",
    "\n",
    "classify_gabor = Model(inputs = feat, outputs = output)\n",
    "classify_gabor.summary()\n",
    "\n",
    "opt = SGD(learning_rate = 0.001, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "classify_gabor.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "365e6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(transformed_handcrafted_features, train_labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "batch_size = 16\n",
    "train_gen = DataGenerator(X_train, y_train, batch_size)   \n",
    "val_gen = DataGenerator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d32227f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 3.9772 - accuracy: 0.0351 - val_loss: 3.7576 - val_accuracy: 0.0486\n",
      "Epoch 2/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9224 - accuracy: 0.0265 - val_loss: 3.7675 - val_accuracy: 0.0208\n",
      "Epoch 3/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8863 - accuracy: 0.0402 - val_loss: 3.7095 - val_accuracy: 0.0312\n",
      "Epoch 4/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9133 - accuracy: 0.0377 - val_loss: 3.6651 - val_accuracy: 0.0556\n",
      "Epoch 5/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8651 - accuracy: 0.0377 - val_loss: 3.7372 - val_accuracy: 0.0312\n",
      "Epoch 6/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8574 - accuracy: 0.0377 - val_loss: 3.6819 - val_accuracy: 0.0243\n",
      "Epoch 7/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8111 - accuracy: 0.0428 - val_loss: 3.7099 - val_accuracy: 0.0382\n",
      "Epoch 8/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9035 - accuracy: 0.0325 - val_loss: 3.6837 - val_accuracy: 0.0278\n",
      "Epoch 9/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8242 - accuracy: 0.0377 - val_loss: 3.6508 - val_accuracy: 0.0451\n",
      "Epoch 10/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7919 - accuracy: 0.0591 - val_loss: 3.6881 - val_accuracy: 0.0486\n",
      "Epoch 11/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8072 - accuracy: 0.0471 - val_loss: 3.6943 - val_accuracy: 0.0382\n",
      "Epoch 12/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7870 - accuracy: 0.0497 - val_loss: 3.6489 - val_accuracy: 0.0417\n",
      "Epoch 13/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7233 - accuracy: 0.0510 - val_loss: 3.6770 - val_accuracy: 0.0556\n",
      "Epoch 14/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7735 - accuracy: 0.0531 - val_loss: 3.6592 - val_accuracy: 0.0312\n",
      "Epoch 15/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7157 - accuracy: 0.0488 - val_loss: 3.6688 - val_accuracy: 0.0278\n",
      "Epoch 16/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7492 - accuracy: 0.0522 - val_loss: 3.6427 - val_accuracy: 0.0521\n",
      "Epoch 17/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7403 - accuracy: 0.0411 - val_loss: 3.6153 - val_accuracy: 0.0208\n",
      "Epoch 18/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7261 - accuracy: 0.0582 - val_loss: 3.6119 - val_accuracy: 0.0625\n",
      "Epoch 19/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6683 - accuracy: 0.0599 - val_loss: 3.6517 - val_accuracy: 0.0347\n",
      "Epoch 20/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6987 - accuracy: 0.0657 - val_loss: 3.6160 - val_accuracy: 0.0556\n",
      "Epoch 21/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6264 - accuracy: 0.0745 - val_loss: 3.6225 - val_accuracy: 0.0625\n",
      "Epoch 22/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6340 - accuracy: 0.0625 - val_loss: 3.5909 - val_accuracy: 0.0799\n",
      "Epoch 23/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6222 - accuracy: 0.0616 - val_loss: 3.6074 - val_accuracy: 0.0486\n",
      "Epoch 24/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6105 - accuracy: 0.0728 - val_loss: 3.6257 - val_accuracy: 0.0625\n",
      "Epoch 25/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5802 - accuracy: 0.0719 - val_loss: 3.5968 - val_accuracy: 0.0556\n",
      "Epoch 26/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5641 - accuracy: 0.0959 - val_loss: 3.5737 - val_accuracy: 0.0590\n",
      "Epoch 27/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5637 - accuracy: 0.0813 - val_loss: 3.5755 - val_accuracy: 0.0590\n",
      "Epoch 28/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5319 - accuracy: 0.0856 - val_loss: 3.5769 - val_accuracy: 0.0764\n",
      "Epoch 29/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5178 - accuracy: 0.0865 - val_loss: 3.5605 - val_accuracy: 0.1007\n",
      "Epoch 30/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5568 - accuracy: 0.0753 - val_loss: 3.5597 - val_accuracy: 0.0694\n",
      "Epoch 31/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5211 - accuracy: 0.0967 - val_loss: 3.5791 - val_accuracy: 0.0799\n",
      "Epoch 32/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5049 - accuracy: 0.0830 - val_loss: 3.5565 - val_accuracy: 0.0694\n",
      "Epoch 33/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5312 - accuracy: 0.0668 - val_loss: 3.5263 - val_accuracy: 0.0903\n",
      "Epoch 34/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4750 - accuracy: 0.0967 - val_loss: 3.5749 - val_accuracy: 0.1007\n",
      "Epoch 35/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4357 - accuracy: 0.1062 - val_loss: 3.5623 - val_accuracy: 0.0521\n",
      "Epoch 36/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4704 - accuracy: 0.0993 - val_loss: 3.5320 - val_accuracy: 0.0694\n",
      "Epoch 37/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4443 - accuracy: 0.1070 - val_loss: 3.4771 - val_accuracy: 0.0729\n",
      "Epoch 38/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4380 - accuracy: 0.1036 - val_loss: 3.5021 - val_accuracy: 0.0556\n",
      "Epoch 39/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4240 - accuracy: 0.1190 - val_loss: 3.5098 - val_accuracy: 0.0903\n",
      "Epoch 40/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3832 - accuracy: 0.1175 - val_loss: 3.5424 - val_accuracy: 0.0625\n",
      "Epoch 41/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3711 - accuracy: 0.1241 - val_loss: 3.4801 - val_accuracy: 0.0938\n",
      "Epoch 42/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3771 - accuracy: 0.1036 - val_loss: 3.4895 - val_accuracy: 0.0764\n",
      "Epoch 43/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3780 - accuracy: 0.1147 - val_loss: 3.4412 - val_accuracy: 0.0938\n",
      "Epoch 44/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.3612 - accuracy: 0.1340 - val_loss: 3.4906 - val_accuracy: 0.0694\n",
      "Epoch 45/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2718 - accuracy: 0.1392 - val_loss: 3.4513 - val_accuracy: 0.0833\n",
      "Epoch 46/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.4005 - accuracy: 0.1284 - val_loss: 3.4368 - val_accuracy: 0.1007\n",
      "Epoch 47/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2948 - accuracy: 0.1318 - val_loss: 3.4095 - val_accuracy: 0.1215\n",
      "Epoch 48/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2832 - accuracy: 0.1387 - val_loss: 3.4362 - val_accuracy: 0.1250\n",
      "Epoch 49/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2121 - accuracy: 0.1627 - val_loss: 3.4273 - val_accuracy: 0.1111\n",
      "Epoch 50/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2389 - accuracy: 0.1473 - val_loss: 3.4069 - val_accuracy: 0.1250\n",
      "Epoch 51/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2512 - accuracy: 0.1464 - val_loss: 3.4123 - val_accuracy: 0.1146\n",
      "Epoch 52/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1652 - accuracy: 0.1695 - val_loss: 3.4174 - val_accuracy: 0.1007\n",
      "Epoch 53/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1779 - accuracy: 0.1575 - val_loss: 3.4462 - val_accuracy: 0.0625\n",
      "Epoch 54/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.2202 - accuracy: 0.1592 - val_loss: 3.4068 - val_accuracy: 0.1076\n",
      "Epoch 55/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1608 - accuracy: 0.1627 - val_loss: 3.4345 - val_accuracy: 0.0938\n",
      "Epoch 56/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1762 - accuracy: 0.1498 - val_loss: 3.3978 - val_accuracy: 0.1250\n",
      "Epoch 57/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1194 - accuracy: 0.1737 - val_loss: 3.4813 - val_accuracy: 0.1007\n",
      "Epoch 58/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1064 - accuracy: 0.1747 - val_loss: 3.4083 - val_accuracy: 0.1042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1315 - accuracy: 0.1935 - val_loss: 3.3924 - val_accuracy: 0.0972\n",
      "Epoch 60/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1248 - accuracy: 0.1892 - val_loss: 3.3433 - val_accuracy: 0.1389\n",
      "Epoch 61/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1874 - accuracy: 0.1627 - val_loss: 3.3484 - val_accuracy: 0.1354\n",
      "Epoch 62/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.0761 - accuracy: 0.1841 - val_loss: 3.3722 - val_accuracy: 0.1285\n",
      "Epoch 63/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.0941 - accuracy: 0.1858 - val_loss: 3.3780 - val_accuracy: 0.1007\n",
      "Epoch 64/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.0216 - accuracy: 0.1926 - val_loss: 3.3515 - val_accuracy: 0.1285\n",
      "Epoch 65/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.0763 - accuracy: 0.1763 - val_loss: 3.3665 - val_accuracy: 0.1285\n",
      "Epoch 66/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9950 - accuracy: 0.2046 - val_loss: 3.3645 - val_accuracy: 0.1458\n",
      "Epoch 67/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.0564 - accuracy: 0.1892 - val_loss: 3.3342 - val_accuracy: 0.1389\n",
      "Epoch 68/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.0443 - accuracy: 0.1943 - val_loss: 3.3543 - val_accuracy: 0.1389\n",
      "Epoch 69/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9618 - accuracy: 0.2183 - val_loss: 3.3168 - val_accuracy: 0.1250\n",
      "Epoch 70/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.1087 - accuracy: 0.1866 - val_loss: 3.3140 - val_accuracy: 0.1528\n",
      "Epoch 71/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9899 - accuracy: 0.2192 - val_loss: 3.3201 - val_accuracy: 0.1528\n",
      "Epoch 72/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9133 - accuracy: 0.2286 - val_loss: 3.3563 - val_accuracy: 0.1181\n",
      "Epoch 73/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9999 - accuracy: 0.2089 - val_loss: 3.2946 - val_accuracy: 0.1354\n",
      "Epoch 74/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9201 - accuracy: 0.2354 - val_loss: 3.3399 - val_accuracy: 0.1319\n",
      "Epoch 75/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.8555 - accuracy: 0.2432 - val_loss: 3.3775 - val_accuracy: 0.1250\n",
      "Epoch 76/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9020 - accuracy: 0.2295 - val_loss: 3.3582 - val_accuracy: 0.1111\n",
      "Epoch 77/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.8881 - accuracy: 0.2414 - val_loss: 3.3539 - val_accuracy: 0.1181\n",
      "Epoch 78/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9440 - accuracy: 0.2260 - val_loss: 3.3059 - val_accuracy: 0.1493\n",
      "Epoch 79/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.8798 - accuracy: 0.2389 - val_loss: 3.2689 - val_accuracy: 0.1354\n",
      "Epoch 80/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.9407 - accuracy: 0.2115 - val_loss: 3.3113 - val_accuracy: 0.1389\n",
      "Epoch 81/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.8938 - accuracy: 0.2286 - val_loss: 3.2994 - val_accuracy: 0.1319\n",
      "Epoch 82/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.8363 - accuracy: 0.2432 - val_loss: 3.3132 - val_accuracy: 0.1250\n",
      "Epoch 83/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.8063 - accuracy: 0.2474 - val_loss: 3.3298 - val_accuracy: 0.1215\n",
      "Epoch 84/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7819 - accuracy: 0.2509 - val_loss: 3.2603 - val_accuracy: 0.1701\n",
      "Epoch 85/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7837 - accuracy: 0.2671 - val_loss: 3.3344 - val_accuracy: 0.1528\n",
      "Epoch 86/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7491 - accuracy: 0.2646 - val_loss: 3.3137 - val_accuracy: 0.1632\n",
      "Epoch 87/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7821 - accuracy: 0.2688 - val_loss: 3.3030 - val_accuracy: 0.1562\n",
      "Epoch 88/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.8348 - accuracy: 0.2286 - val_loss: 3.3577 - val_accuracy: 0.1215\n",
      "Epoch 89/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6228 - accuracy: 0.2962 - val_loss: 3.2588 - val_accuracy: 0.1319\n",
      "Epoch 90/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7217 - accuracy: 0.2904 - val_loss: 3.2281 - val_accuracy: 0.1701\n",
      "Epoch 91/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7529 - accuracy: 0.2551 - val_loss: 3.2704 - val_accuracy: 0.1458\n",
      "Epoch 92/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7422 - accuracy: 0.2628 - val_loss: 3.2858 - val_accuracy: 0.1458\n",
      "Epoch 93/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6691 - accuracy: 0.2885 - val_loss: 3.3159 - val_accuracy: 0.1424\n",
      "Epoch 94/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6719 - accuracy: 0.2842 - val_loss: 3.2672 - val_accuracy: 0.1562\n",
      "Epoch 95/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.8010 - accuracy: 0.2526 - val_loss: 3.3793 - val_accuracy: 0.1250\n",
      "Epoch 96/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7138 - accuracy: 0.2928 - val_loss: 3.3167 - val_accuracy: 0.1632\n",
      "Epoch 97/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6763 - accuracy: 0.2825 - val_loss: 3.3497 - val_accuracy: 0.1528\n",
      "Epoch 98/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7224 - accuracy: 0.2697 - val_loss: 3.2807 - val_accuracy: 0.1424\n",
      "Epoch 99/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6953 - accuracy: 0.2783 - val_loss: 3.3102 - val_accuracy: 0.1528\n",
      "Epoch 100/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.7342 - accuracy: 0.2731 - val_loss: 3.2220 - val_accuracy: 0.1701\n",
      "Epoch 101/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6682 - accuracy: 0.2620 - val_loss: 3.3251 - val_accuracy: 0.1493\n",
      "Epoch 102/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6287 - accuracy: 0.2800 - val_loss: 3.2489 - val_accuracy: 0.1285\n",
      "Epoch 103/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6170 - accuracy: 0.3022 - val_loss: 3.3062 - val_accuracy: 0.1181\n",
      "Epoch 104/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6659 - accuracy: 0.2885 - val_loss: 3.2934 - val_accuracy: 0.1528\n",
      "Epoch 105/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6244 - accuracy: 0.2791 - val_loss: 3.3076 - val_accuracy: 0.1424\n",
      "Epoch 106/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5036 - accuracy: 0.3390 - val_loss: 3.2512 - val_accuracy: 0.1701\n",
      "Epoch 107/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.6345 - accuracy: 0.3039 - val_loss: 3.3776 - val_accuracy: 0.1771\n",
      "Epoch 108/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5830 - accuracy: 0.3016 - val_loss: 3.2889 - val_accuracy: 0.1562\n",
      "Epoch 109/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5270 - accuracy: 0.3048 - val_loss: 3.3158 - val_accuracy: 0.1319\n",
      "Epoch 110/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5603 - accuracy: 0.3048 - val_loss: 3.3745 - val_accuracy: 0.1528\n",
      "Epoch 111/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5546 - accuracy: 0.3168 - val_loss: 3.2397 - val_accuracy: 0.1701\n",
      "Epoch 112/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5120 - accuracy: 0.3193 - val_loss: 3.2992 - val_accuracy: 0.1771\n",
      "Epoch 113/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5368 - accuracy: 0.3125 - val_loss: 3.3550 - val_accuracy: 0.1667\n",
      "Epoch 114/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5383 - accuracy: 0.3296 - val_loss: 3.3205 - val_accuracy: 0.1771\n",
      "Epoch 115/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5027 - accuracy: 0.3348 - val_loss: 3.2605 - val_accuracy: 0.1736\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5242 - accuracy: 0.3202 - val_loss: 3.2484 - val_accuracy: 0.1458\n",
      "Epoch 117/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4815 - accuracy: 0.3399 - val_loss: 3.2191 - val_accuracy: 0.1910\n",
      "Epoch 118/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3992 - accuracy: 0.3630 - val_loss: 3.3153 - val_accuracy: 0.1771\n",
      "Epoch 119/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.5658 - accuracy: 0.3228 - val_loss: 3.3920 - val_accuracy: 0.1562\n",
      "Epoch 120/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4820 - accuracy: 0.3313 - val_loss: 3.3999 - val_accuracy: 0.1285\n",
      "Epoch 121/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4592 - accuracy: 0.3245 - val_loss: 3.4001 - val_accuracy: 0.1493\n",
      "Epoch 122/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4453 - accuracy: 0.3373 - val_loss: 3.4023 - val_accuracy: 0.1424\n",
      "Epoch 123/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3244 - accuracy: 0.3803 - val_loss: 3.3861 - val_accuracy: 0.1528\n",
      "Epoch 124/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4864 - accuracy: 0.3527 - val_loss: 3.3560 - val_accuracy: 0.1632\n",
      "Epoch 125/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4118 - accuracy: 0.3365 - val_loss: 3.4247 - val_accuracy: 0.1319\n",
      "Epoch 126/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3776 - accuracy: 0.3502 - val_loss: 3.3235 - val_accuracy: 0.1562\n",
      "Epoch 127/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3524 - accuracy: 0.3536 - val_loss: 3.3642 - val_accuracy: 0.1632\n",
      "Epoch 128/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4213 - accuracy: 0.3408 - val_loss: 3.4153 - val_accuracy: 0.1458\n",
      "Epoch 129/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2721 - accuracy: 0.3750 - val_loss: 3.4116 - val_accuracy: 0.1840\n",
      "Epoch 130/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4106 - accuracy: 0.3509 - val_loss: 3.3105 - val_accuracy: 0.1840\n",
      "Epoch 131/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4422 - accuracy: 0.3322 - val_loss: 3.4145 - val_accuracy: 0.1840\n",
      "Epoch 132/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2753 - accuracy: 0.3750 - val_loss: 3.3386 - val_accuracy: 0.1528\n",
      "Epoch 133/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2792 - accuracy: 0.3896 - val_loss: 3.3156 - val_accuracy: 0.1458\n",
      "Epoch 134/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3979 - accuracy: 0.3425 - val_loss: 3.3416 - val_accuracy: 0.1562\n",
      "Epoch 135/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.4380 - accuracy: 0.3416 - val_loss: 3.3312 - val_accuracy: 0.1771\n",
      "Epoch 136/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2814 - accuracy: 0.3887 - val_loss: 3.3591 - val_accuracy: 0.1597\n",
      "Epoch 137/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3570 - accuracy: 0.3510 - val_loss: 3.2758 - val_accuracy: 0.1771\n",
      "Epoch 138/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2941 - accuracy: 0.3716 - val_loss: 3.3897 - val_accuracy: 0.1424\n",
      "Epoch 139/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3637 - accuracy: 0.3579 - val_loss: 3.3483 - val_accuracy: 0.1806\n",
      "Epoch 140/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3129 - accuracy: 0.3699 - val_loss: 3.3176 - val_accuracy: 0.1806\n",
      "Epoch 141/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.3458 - accuracy: 0.3818 - val_loss: 3.3672 - val_accuracy: 0.1667\n",
      "Epoch 142/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2984 - accuracy: 0.3827 - val_loss: 3.3619 - val_accuracy: 0.1701\n",
      "Epoch 143/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2013 - accuracy: 0.3803 - val_loss: 3.3504 - val_accuracy: 0.1632\n",
      "Epoch 144/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2602 - accuracy: 0.4058 - val_loss: 3.3453 - val_accuracy: 0.1736\n",
      "Epoch 145/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2581 - accuracy: 0.3947 - val_loss: 3.3645 - val_accuracy: 0.1771\n",
      "Epoch 146/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1871 - accuracy: 0.3913 - val_loss: 3.3814 - val_accuracy: 0.1701\n",
      "Epoch 147/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1689 - accuracy: 0.4152 - val_loss: 3.4282 - val_accuracy: 0.1632\n",
      "Epoch 148/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1562 - accuracy: 0.4229 - val_loss: 3.4938 - val_accuracy: 0.1632\n",
      "Epoch 149/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1778 - accuracy: 0.4015 - val_loss: 3.4192 - val_accuracy: 0.1701\n",
      "Epoch 150/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1856 - accuracy: 0.4067 - val_loss: 3.4402 - val_accuracy: 0.1701\n",
      "Epoch 151/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2868 - accuracy: 0.3861 - val_loss: 3.4404 - val_accuracy: 0.1562\n",
      "Epoch 152/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2014 - accuracy: 0.3921 - val_loss: 3.4053 - val_accuracy: 0.1806\n",
      "Epoch 153/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2545 - accuracy: 0.3878 - val_loss: 3.4865 - val_accuracy: 0.1806\n",
      "Epoch 154/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2365 - accuracy: 0.3955 - val_loss: 3.4324 - val_accuracy: 0.1562\n",
      "Epoch 155/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.2015 - accuracy: 0.3973 - val_loss: 3.4016 - val_accuracy: 0.1771\n",
      "Epoch 156/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1485 - accuracy: 0.4281 - val_loss: 3.4404 - val_accuracy: 0.1806\n",
      "Epoch 157/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1131 - accuracy: 0.4212 - val_loss: 3.4694 - val_accuracy: 0.1806\n",
      "Epoch 158/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1864 - accuracy: 0.4007 - val_loss: 3.4382 - val_accuracy: 0.1597\n",
      "Epoch 159/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0322 - accuracy: 0.4229 - val_loss: 3.4562 - val_accuracy: 0.1354\n",
      "Epoch 160/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1899 - accuracy: 0.3973 - val_loss: 3.3841 - val_accuracy: 0.1944\n",
      "Epoch 161/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1334 - accuracy: 0.4152 - val_loss: 3.6165 - val_accuracy: 0.1493\n",
      "Epoch 162/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1039 - accuracy: 0.4247 - val_loss: 3.4853 - val_accuracy: 0.1597\n",
      "Epoch 163/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1192 - accuracy: 0.4384 - val_loss: 3.5915 - val_accuracy: 0.1667\n",
      "Epoch 164/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0381 - accuracy: 0.4512 - val_loss: 3.5568 - val_accuracy: 0.1806\n",
      "Epoch 165/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1393 - accuracy: 0.4212 - val_loss: 3.4742 - val_accuracy: 0.1597\n",
      "Epoch 166/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1525 - accuracy: 0.4084 - val_loss: 3.4628 - val_accuracy: 0.1667\n",
      "Epoch 167/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1722 - accuracy: 0.4110 - val_loss: 3.4129 - val_accuracy: 0.1458\n",
      "Epoch 168/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0780 - accuracy: 0.4152 - val_loss: 3.3816 - val_accuracy: 0.1736\n",
      "Epoch 169/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1580 - accuracy: 0.3898 - val_loss: 3.4768 - val_accuracy: 0.1632\n",
      "Epoch 170/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9942 - accuracy: 0.4478 - val_loss: 3.5903 - val_accuracy: 0.1736\n",
      "Epoch 171/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9524 - accuracy: 0.4555 - val_loss: 3.5152 - val_accuracy: 0.1528\n",
      "Epoch 172/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0365 - accuracy: 0.4272 - val_loss: 3.5168 - val_accuracy: 0.1528\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9921 - accuracy: 0.4324 - val_loss: 3.5696 - val_accuracy: 0.1667\n",
      "Epoch 174/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0759 - accuracy: 0.4118 - val_loss: 3.6023 - val_accuracy: 0.1632\n",
      "Epoch 175/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0288 - accuracy: 0.4495 - val_loss: 3.4751 - val_accuracy: 0.1701\n",
      "Epoch 176/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.1890 - accuracy: 0.4033 - val_loss: 3.4818 - val_accuracy: 0.1354\n",
      "Epoch 177/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0064 - accuracy: 0.4512 - val_loss: 3.4918 - val_accuracy: 0.1875\n",
      "Epoch 178/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9532 - accuracy: 0.4589 - val_loss: 3.6062 - val_accuracy: 0.1736\n",
      "Epoch 179/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9736 - accuracy: 0.4461 - val_loss: 3.5036 - val_accuracy: 0.1840\n",
      "Epoch 180/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9572 - accuracy: 0.4606 - val_loss: 3.4737 - val_accuracy: 0.1667\n",
      "Epoch 181/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.8752 - accuracy: 0.4957 - val_loss: 3.5539 - val_accuracy: 0.1632\n",
      "Epoch 182/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0104 - accuracy: 0.4426 - val_loss: 3.5009 - val_accuracy: 0.1667\n",
      "Epoch 183/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0070 - accuracy: 0.4512 - val_loss: 3.5208 - val_accuracy: 0.1528\n",
      "Epoch 184/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 2.0026 - accuracy: 0.4632 - val_loss: 3.4752 - val_accuracy: 0.1667\n",
      "Epoch 185/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.8723 - accuracy: 0.4649 - val_loss: 3.6398 - val_accuracy: 0.1562\n",
      "Epoch 186/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.7561 - accuracy: 0.5094 - val_loss: 3.6974 - val_accuracy: 0.1840\n",
      "Epoch 187/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9044 - accuracy: 0.4717 - val_loss: 3.5590 - val_accuracy: 0.1667\n",
      "Epoch 188/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9617 - accuracy: 0.4666 - val_loss: 3.6620 - val_accuracy: 0.1632\n",
      "Epoch 189/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.8440 - accuracy: 0.4897 - val_loss: 3.6533 - val_accuracy: 0.1632\n",
      "Epoch 190/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9721 - accuracy: 0.4709 - val_loss: 3.5941 - val_accuracy: 0.1597\n",
      "Epoch 191/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9215 - accuracy: 0.4649 - val_loss: 3.7015 - val_accuracy: 0.1667\n",
      "Epoch 192/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.8261 - accuracy: 0.4883 - val_loss: 3.7301 - val_accuracy: 0.1389\n",
      "Epoch 193/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9940 - accuracy: 0.4726 - val_loss: 3.5758 - val_accuracy: 0.1806\n",
      "Epoch 194/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.8949 - accuracy: 0.4692 - val_loss: 3.7326 - val_accuracy: 0.1528\n",
      "Epoch 195/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9531 - accuracy: 0.4717 - val_loss: 3.5765 - val_accuracy: 0.1528\n",
      "Epoch 196/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9425 - accuracy: 0.4649 - val_loss: 3.5909 - val_accuracy: 0.1562\n",
      "Epoch 197/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.8983 - accuracy: 0.4854 - val_loss: 3.6729 - val_accuracy: 0.1736\n",
      "Epoch 198/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.8357 - accuracy: 0.4906 - val_loss: 3.6850 - val_accuracy: 0.1632\n",
      "Epoch 199/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.8840 - accuracy: 0.4623 - val_loss: 3.6938 - val_accuracy: 0.1771\n",
      "Epoch 200/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 1.9019 - accuracy: 0.4863 - val_loss: 3.6221 - val_accuracy: 0.1667\n"
     ]
    }
   ],
   "source": [
    "train_summary = classify_gabor.fit(train_gen, epochs = 200, verbose = 1, callbacks = None, validation_data = val_gen, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen)//batch_size, \n",
    "                              validation_steps = len(val_gen)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba2f5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'model_classify_gabor_conv1_4096-2048-1024_AWA2_drop0.5_200eph_16bch_tanh_0.001lr'\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush/Results/'\n",
    "\n",
    "classify_gabor.save_weights(save_path + 'Wt_' + name + '.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904c7c2",
   "metadata": {},
   "source": [
    "# Define classifier 3 (Conv 1x1 layers and dense layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbcc709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 2.51186728e-01 -9.88067091e-02 -2.52698511e-01 ...  9.77901891e-02\n",
      "    -5.02952114e-02 -3.63892764e-02]]]\n",
      "\n",
      "\n",
      " [[[-4.33573425e-01 -1.64868236e-01 -8.72340277e-02 ...  8.97908807e-02\n",
      "    -3.47408950e-02 -7.32663646e-02]]]\n",
      "\n",
      "\n",
      " [[[ 3.29547226e-02  1.67756379e-01 -2.64626563e-01 ...  1.84952453e-01\n",
      "     8.73122662e-02 -1.13576725e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-3.00757438e-01 -3.35388482e-01 -3.16070139e-01 ...  3.49272229e-03\n",
      "    -3.33282277e-02  1.97447091e-03]]]\n",
      "\n",
      "\n",
      " [[[-7.75490224e-01  9.45429951e-02 -4.35241669e-01 ... -1.05705112e-06\n",
      "     6.83911145e-04 -1.66948624e-02]]]\n",
      "\n",
      "\n",
      " [[[-5.36274433e-01 -4.59827274e-01 -2.81575382e-01 ...  2.01415159e-02\n",
      "    -4.24032025e-02 -1.43554091e-01]]]] (23527, 1, 1, 4096)\n"
     ]
    }
   ],
   "source": [
    "transformed_handcrafted_features = np.expand_dims(train_gabor_feature_transformed, axis=1)\n",
    "transformed_handcrafted_features = np.expand_dims(transformed_handcrafted_features, axis=1)\n",
    "print(transformed_handcrafted_features, transformed_handcrafted_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "158ac200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 1, 1, 4096)]      0         \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 1, 1, 4096)        16781312  \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 1, 1, 4096)        0         \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 1, 1, 2048)        8390656   \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 1, 1, 2048)        0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 50)                51250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,370,994\n",
      "Trainable params: 28,370,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_features, feat_height, feat_width, feat_length = transformed_handcrafted_features.shape\n",
    "\n",
    "feat = Input(shape = (feat_height, feat_width, feat_length))\n",
    "\n",
    "\n",
    "conv1 = Conv2D(4096, 1, activation='tanh')(feat)\n",
    "drop1 = Dropout(0.5)(conv1)\n",
    "conv2 = Conv2D(2048, 1,  activation='tanh')(drop1)\n",
    "drop2 = Dropout(0.5)(conv2)\n",
    "conv3 = Conv2D(1024, 1,  activation='tanh')(drop2)\n",
    "drop3 = Dropout(0.5)(conv3)\n",
    "flat = Flatten()(drop3)\n",
    "dense1 = Dense(1024, activation='tanh')(flat)\n",
    "drop3 = Dropout(0.5)(dense1)\n",
    "output = Dense(50, activation='softmax')(drop3)\n",
    "\n",
    "\n",
    "classify_gabor = Model(inputs = feat, outputs = output)\n",
    "classify_gabor.summary()\n",
    "\n",
    "opt = SGD(learning_rate = 0.0001, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "classify_gabor.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "953acc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(transformed_handcrafted_features, train_labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "batch_size = 16\n",
    "train_gen = DataGenerator(X_train, y_train, batch_size)   \n",
    "val_gen = DataGenerator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5a3cb74d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 4.1396 - accuracy: 0.0199 - val_loss: 3.8247 - val_accuracy: 0.0208\n",
      "Epoch 2/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 4.0611 - accuracy: 0.0394 - val_loss: 3.7571 - val_accuracy: 0.0382\n",
      "Epoch 3/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 4.0119 - accuracy: 0.0240 - val_loss: 3.7127 - val_accuracy: 0.0451\n",
      "Epoch 4/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9803 - accuracy: 0.0214 - val_loss: 3.7056 - val_accuracy: 0.0451\n",
      "Epoch 5/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9493 - accuracy: 0.0351 - val_loss: 3.6911 - val_accuracy: 0.0347\n",
      "Epoch 6/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9584 - accuracy: 0.0377 - val_loss: 3.6889 - val_accuracy: 0.0417\n",
      "Epoch 7/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9452 - accuracy: 0.0308 - val_loss: 3.6890 - val_accuracy: 0.0417\n",
      "Epoch 8/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9410 - accuracy: 0.0368 - val_loss: 3.6812 - val_accuracy: 0.0451\n",
      "Epoch 9/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9214 - accuracy: 0.0406 - val_loss: 3.6789 - val_accuracy: 0.0451\n",
      "Epoch 10/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9233 - accuracy: 0.0462 - val_loss: 3.6737 - val_accuracy: 0.0451\n",
      "Epoch 11/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9136 - accuracy: 0.0394 - val_loss: 3.6766 - val_accuracy: 0.0312\n",
      "Epoch 12/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9141 - accuracy: 0.0265 - val_loss: 3.6795 - val_accuracy: 0.0417\n",
      "Epoch 13/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9076 - accuracy: 0.0257 - val_loss: 3.6660 - val_accuracy: 0.0451\n",
      "Epoch 14/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8985 - accuracy: 0.0342 - val_loss: 3.6665 - val_accuracy: 0.0451\n",
      "Epoch 15/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9000 - accuracy: 0.0377 - val_loss: 3.6627 - val_accuracy: 0.0382\n",
      "Epoch 16/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8697 - accuracy: 0.0394 - val_loss: 3.6576 - val_accuracy: 0.0243\n",
      "Epoch 17/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8908 - accuracy: 0.0351 - val_loss: 3.6547 - val_accuracy: 0.0451\n",
      "Epoch 18/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.9075 - accuracy: 0.0248 - val_loss: 3.6582 - val_accuracy: 0.0451\n",
      "Epoch 19/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8741 - accuracy: 0.0291 - val_loss: 3.6526 - val_accuracy: 0.0382\n",
      "Epoch 20/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8559 - accuracy: 0.0445 - val_loss: 3.6493 - val_accuracy: 0.0417\n",
      "Epoch 21/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8905 - accuracy: 0.0368 - val_loss: 3.6610 - val_accuracy: 0.0417\n",
      "Epoch 22/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8829 - accuracy: 0.0411 - val_loss: 3.6548 - val_accuracy: 0.0451\n",
      "Epoch 23/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8673 - accuracy: 0.0283 - val_loss: 3.6464 - val_accuracy: 0.0417\n",
      "Epoch 24/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8822 - accuracy: 0.0394 - val_loss: 3.6468 - val_accuracy: 0.0451\n",
      "Epoch 25/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8464 - accuracy: 0.0351 - val_loss: 3.6440 - val_accuracy: 0.0451\n",
      "Epoch 26/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8573 - accuracy: 0.0325 - val_loss: 3.6352 - val_accuracy: 0.0451\n",
      "Epoch 27/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8891 - accuracy: 0.0300 - val_loss: 3.6402 - val_accuracy: 0.0382\n",
      "Epoch 28/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8853 - accuracy: 0.0334 - val_loss: 3.6386 - val_accuracy: 0.0347\n",
      "Epoch 29/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8550 - accuracy: 0.0385 - val_loss: 3.6369 - val_accuracy: 0.0451\n",
      "Epoch 30/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8596 - accuracy: 0.0479 - val_loss: 3.6331 - val_accuracy: 0.0486\n",
      "Epoch 31/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8533 - accuracy: 0.0428 - val_loss: 3.6394 - val_accuracy: 0.0486\n",
      "Epoch 32/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8068 - accuracy: 0.0505 - val_loss: 3.6477 - val_accuracy: 0.0417\n",
      "Epoch 33/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8618 - accuracy: 0.0342 - val_loss: 3.6449 - val_accuracy: 0.0278\n",
      "Epoch 34/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8235 - accuracy: 0.0351 - val_loss: 3.6444 - val_accuracy: 0.0347\n",
      "Epoch 35/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8488 - accuracy: 0.0402 - val_loss: 3.6422 - val_accuracy: 0.0347\n",
      "Epoch 36/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8071 - accuracy: 0.0437 - val_loss: 3.6412 - val_accuracy: 0.0486\n",
      "Epoch 37/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8371 - accuracy: 0.0342 - val_loss: 3.6404 - val_accuracy: 0.0521\n",
      "Epoch 38/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8400 - accuracy: 0.0437 - val_loss: 3.6338 - val_accuracy: 0.0347\n",
      "Epoch 39/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8530 - accuracy: 0.0445 - val_loss: 3.6434 - val_accuracy: 0.0312\n",
      "Epoch 40/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7971 - accuracy: 0.0317 - val_loss: 3.6419 - val_accuracy: 0.0382\n",
      "Epoch 41/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8309 - accuracy: 0.0420 - val_loss: 3.6466 - val_accuracy: 0.0417\n",
      "Epoch 42/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8104 - accuracy: 0.0445 - val_loss: 3.6363 - val_accuracy: 0.0417\n",
      "Epoch 43/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8161 - accuracy: 0.0342 - val_loss: 3.6427 - val_accuracy: 0.0486\n",
      "Epoch 44/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8102 - accuracy: 0.0479 - val_loss: 3.6412 - val_accuracy: 0.0556\n",
      "Epoch 45/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8266 - accuracy: 0.0385 - val_loss: 3.6338 - val_accuracy: 0.0556\n",
      "Epoch 46/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8300 - accuracy: 0.0394 - val_loss: 3.6333 - val_accuracy: 0.0451\n",
      "Epoch 47/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7933 - accuracy: 0.0377 - val_loss: 3.6206 - val_accuracy: 0.0382\n",
      "Epoch 48/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8240 - accuracy: 0.0325 - val_loss: 3.6251 - val_accuracy: 0.0451\n",
      "Epoch 49/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8317 - accuracy: 0.0342 - val_loss: 3.6314 - val_accuracy: 0.0382\n",
      "Epoch 50/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7958 - accuracy: 0.0488 - val_loss: 3.6340 - val_accuracy: 0.0486\n",
      "Epoch 51/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7659 - accuracy: 0.0497 - val_loss: 3.6343 - val_accuracy: 0.0590\n",
      "Epoch 52/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7930 - accuracy: 0.0342 - val_loss: 3.6360 - val_accuracy: 0.0451\n",
      "Epoch 53/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8086 - accuracy: 0.0360 - val_loss: 3.6311 - val_accuracy: 0.0556\n",
      "Epoch 54/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7677 - accuracy: 0.0377 - val_loss: 3.6324 - val_accuracy: 0.0451\n",
      "Epoch 55/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8011 - accuracy: 0.0377 - val_loss: 3.6304 - val_accuracy: 0.0521\n",
      "Epoch 56/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7998 - accuracy: 0.0488 - val_loss: 3.6320 - val_accuracy: 0.0521\n",
      "Epoch 57/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7797 - accuracy: 0.0394 - val_loss: 3.6334 - val_accuracy: 0.0521\n",
      "Epoch 58/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8318 - accuracy: 0.0317 - val_loss: 3.6375 - val_accuracy: 0.0486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7568 - accuracy: 0.0445 - val_loss: 3.6390 - val_accuracy: 0.0451\n",
      "Epoch 60/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8158 - accuracy: 0.0497 - val_loss: 3.6310 - val_accuracy: 0.0451\n",
      "Epoch 61/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7682 - accuracy: 0.0437 - val_loss: 3.6305 - val_accuracy: 0.0312\n",
      "Epoch 62/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7332 - accuracy: 0.0445 - val_loss: 3.6280 - val_accuracy: 0.0521\n",
      "Epoch 63/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8023 - accuracy: 0.0411 - val_loss: 3.6275 - val_accuracy: 0.0590\n",
      "Epoch 64/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7221 - accuracy: 0.0441 - val_loss: 3.6309 - val_accuracy: 0.0590\n",
      "Epoch 65/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7808 - accuracy: 0.0411 - val_loss: 3.6272 - val_accuracy: 0.0556\n",
      "Epoch 66/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7569 - accuracy: 0.0385 - val_loss: 3.6268 - val_accuracy: 0.0590\n",
      "Epoch 67/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7632 - accuracy: 0.0402 - val_loss: 3.6214 - val_accuracy: 0.0451\n",
      "Epoch 68/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7814 - accuracy: 0.0394 - val_loss: 3.6186 - val_accuracy: 0.0521\n",
      "Epoch 69/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.8049 - accuracy: 0.0437 - val_loss: 3.6160 - val_accuracy: 0.0486\n",
      "Epoch 70/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7294 - accuracy: 0.0454 - val_loss: 3.6200 - val_accuracy: 0.0486\n",
      "Epoch 71/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7908 - accuracy: 0.0471 - val_loss: 3.6281 - val_accuracy: 0.0417\n",
      "Epoch 72/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7526 - accuracy: 0.0411 - val_loss: 3.6284 - val_accuracy: 0.0382\n",
      "Epoch 73/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7796 - accuracy: 0.0411 - val_loss: 3.6258 - val_accuracy: 0.0451\n",
      "Epoch 74/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7226 - accuracy: 0.0445 - val_loss: 3.6290 - val_accuracy: 0.0417\n",
      "Epoch 75/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7541 - accuracy: 0.0445 - val_loss: 3.6290 - val_accuracy: 0.0521\n",
      "Epoch 76/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7793 - accuracy: 0.0428 - val_loss: 3.6230 - val_accuracy: 0.0486\n",
      "Epoch 77/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7625 - accuracy: 0.0437 - val_loss: 3.6156 - val_accuracy: 0.0625\n",
      "Epoch 78/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7865 - accuracy: 0.0471 - val_loss: 3.6150 - val_accuracy: 0.0556\n",
      "Epoch 79/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7322 - accuracy: 0.0479 - val_loss: 3.6161 - val_accuracy: 0.0590\n",
      "Epoch 80/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7960 - accuracy: 0.0334 - val_loss: 3.6123 - val_accuracy: 0.0590\n",
      "Epoch 81/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7444 - accuracy: 0.0377 - val_loss: 3.6177 - val_accuracy: 0.0590\n",
      "Epoch 82/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7069 - accuracy: 0.0548 - val_loss: 3.6135 - val_accuracy: 0.0521\n",
      "Epoch 83/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7313 - accuracy: 0.0488 - val_loss: 3.6161 - val_accuracy: 0.0521\n",
      "Epoch 84/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7272 - accuracy: 0.0527 - val_loss: 3.6163 - val_accuracy: 0.0486\n",
      "Epoch 85/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7347 - accuracy: 0.0406 - val_loss: 3.6154 - val_accuracy: 0.0486\n",
      "Epoch 86/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7555 - accuracy: 0.0445 - val_loss: 3.6183 - val_accuracy: 0.0417\n",
      "Epoch 87/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7317 - accuracy: 0.0497 - val_loss: 3.6160 - val_accuracy: 0.0521\n",
      "Epoch 88/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7691 - accuracy: 0.0488 - val_loss: 3.6155 - val_accuracy: 0.0521\n",
      "Epoch 89/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7556 - accuracy: 0.0368 - val_loss: 3.6097 - val_accuracy: 0.0521\n",
      "Epoch 90/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7229 - accuracy: 0.0539 - val_loss: 3.6120 - val_accuracy: 0.0625\n",
      "Epoch 91/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7330 - accuracy: 0.0531 - val_loss: 3.6125 - val_accuracy: 0.0729\n",
      "Epoch 92/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7614 - accuracy: 0.0334 - val_loss: 3.6102 - val_accuracy: 0.0660\n",
      "Epoch 93/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7205 - accuracy: 0.0420 - val_loss: 3.6004 - val_accuracy: 0.0729\n",
      "Epoch 94/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6876 - accuracy: 0.0454 - val_loss: 3.6060 - val_accuracy: 0.0625\n",
      "Epoch 95/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7273 - accuracy: 0.0411 - val_loss: 3.6126 - val_accuracy: 0.0625\n",
      "Epoch 96/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7217 - accuracy: 0.0402 - val_loss: 3.6153 - val_accuracy: 0.0590\n",
      "Epoch 97/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7284 - accuracy: 0.0291 - val_loss: 3.6083 - val_accuracy: 0.0694\n",
      "Epoch 98/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7137 - accuracy: 0.0437 - val_loss: 3.6081 - val_accuracy: 0.0486\n",
      "Epoch 99/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7386 - accuracy: 0.0462 - val_loss: 3.6119 - val_accuracy: 0.0521\n",
      "Epoch 100/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7227 - accuracy: 0.0488 - val_loss: 3.6121 - val_accuracy: 0.0521\n",
      "Epoch 101/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6793 - accuracy: 0.0454 - val_loss: 3.6174 - val_accuracy: 0.0521\n",
      "Epoch 102/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7161 - accuracy: 0.0531 - val_loss: 3.6131 - val_accuracy: 0.0521\n",
      "Epoch 103/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6994 - accuracy: 0.0420 - val_loss: 3.6125 - val_accuracy: 0.0556\n",
      "Epoch 104/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7141 - accuracy: 0.0394 - val_loss: 3.6128 - val_accuracy: 0.0625\n",
      "Epoch 105/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6999 - accuracy: 0.0471 - val_loss: 3.6075 - val_accuracy: 0.0625\n",
      "Epoch 106/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7360 - accuracy: 0.0462 - val_loss: 3.6068 - val_accuracy: 0.0556\n",
      "Epoch 107/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7266 - accuracy: 0.0531 - val_loss: 3.6019 - val_accuracy: 0.0417\n",
      "Epoch 108/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7270 - accuracy: 0.0505 - val_loss: 3.6010 - val_accuracy: 0.0590\n",
      "Epoch 109/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6906 - accuracy: 0.0522 - val_loss: 3.6025 - val_accuracy: 0.0556\n",
      "Epoch 110/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7479 - accuracy: 0.0454 - val_loss: 3.6049 - val_accuracy: 0.0556\n",
      "Epoch 111/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6861 - accuracy: 0.0479 - val_loss: 3.6061 - val_accuracy: 0.0556\n",
      "Epoch 112/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6724 - accuracy: 0.0467 - val_loss: 3.6027 - val_accuracy: 0.0729\n",
      "Epoch 113/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7008 - accuracy: 0.0402 - val_loss: 3.5967 - val_accuracy: 0.0625\n",
      "Epoch 114/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6658 - accuracy: 0.0437 - val_loss: 3.5996 - val_accuracy: 0.0590\n",
      "Epoch 115/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7014 - accuracy: 0.0565 - val_loss: 3.6000 - val_accuracy: 0.0729\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6582 - accuracy: 0.0614 - val_loss: 3.5997 - val_accuracy: 0.0694\n",
      "Epoch 117/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7187 - accuracy: 0.0548 - val_loss: 3.5932 - val_accuracy: 0.0729\n",
      "Epoch 118/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6737 - accuracy: 0.0505 - val_loss: 3.5954 - val_accuracy: 0.0694\n",
      "Epoch 119/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6563 - accuracy: 0.0497 - val_loss: 3.5988 - val_accuracy: 0.0660\n",
      "Epoch 120/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6932 - accuracy: 0.0522 - val_loss: 3.5979 - val_accuracy: 0.0590\n",
      "Epoch 121/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6850 - accuracy: 0.0471 - val_loss: 3.6001 - val_accuracy: 0.0660\n",
      "Epoch 122/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7032 - accuracy: 0.0368 - val_loss: 3.5946 - val_accuracy: 0.0694\n",
      "Epoch 123/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6818 - accuracy: 0.0574 - val_loss: 3.5941 - val_accuracy: 0.0694\n",
      "Epoch 124/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7035 - accuracy: 0.0389 - val_loss: 3.5950 - val_accuracy: 0.0694\n",
      "Epoch 125/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.7270 - accuracy: 0.0454 - val_loss: 3.5905 - val_accuracy: 0.0625\n",
      "Epoch 126/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6366 - accuracy: 0.0651 - val_loss: 3.5862 - val_accuracy: 0.0625\n",
      "Epoch 127/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6984 - accuracy: 0.0454 - val_loss: 3.5870 - val_accuracy: 0.0694\n",
      "Epoch 128/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6830 - accuracy: 0.0479 - val_loss: 3.5872 - val_accuracy: 0.0660\n",
      "Epoch 129/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6315 - accuracy: 0.0625 - val_loss: 3.5912 - val_accuracy: 0.0590\n",
      "Epoch 130/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6790 - accuracy: 0.0599 - val_loss: 3.5895 - val_accuracy: 0.0590\n",
      "Epoch 131/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6519 - accuracy: 0.0565 - val_loss: 3.5903 - val_accuracy: 0.0625\n",
      "Epoch 132/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6527 - accuracy: 0.0588 - val_loss: 3.5880 - val_accuracy: 0.0660\n",
      "Epoch 133/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6961 - accuracy: 0.0385 - val_loss: 3.5878 - val_accuracy: 0.0625\n",
      "Epoch 134/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6460 - accuracy: 0.0608 - val_loss: 3.5793 - val_accuracy: 0.0625\n",
      "Epoch 135/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6469 - accuracy: 0.0522 - val_loss: 3.5763 - val_accuracy: 0.0833\n",
      "Epoch 136/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6685 - accuracy: 0.0539 - val_loss: 3.5786 - val_accuracy: 0.0799\n",
      "Epoch 137/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6735 - accuracy: 0.0539 - val_loss: 3.5776 - val_accuracy: 0.0694\n",
      "Epoch 138/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6675 - accuracy: 0.0565 - val_loss: 3.5741 - val_accuracy: 0.0660\n",
      "Epoch 139/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6069 - accuracy: 0.0676 - val_loss: 3.5748 - val_accuracy: 0.0660\n",
      "Epoch 140/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6476 - accuracy: 0.0642 - val_loss: 3.5756 - val_accuracy: 0.0625\n",
      "Epoch 141/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6316 - accuracy: 0.0548 - val_loss: 3.5739 - val_accuracy: 0.0694\n",
      "Epoch 142/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6885 - accuracy: 0.0557 - val_loss: 3.5739 - val_accuracy: 0.0799\n",
      "Epoch 143/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6536 - accuracy: 0.0574 - val_loss: 3.5800 - val_accuracy: 0.0694\n",
      "Epoch 144/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6446 - accuracy: 0.0514 - val_loss: 3.5809 - val_accuracy: 0.0764\n",
      "Epoch 145/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6505 - accuracy: 0.0497 - val_loss: 3.5767 - val_accuracy: 0.0694\n",
      "Epoch 146/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6587 - accuracy: 0.0651 - val_loss: 3.5769 - val_accuracy: 0.0764\n",
      "Epoch 147/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6381 - accuracy: 0.0599 - val_loss: 3.5825 - val_accuracy: 0.0799\n",
      "Epoch 148/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6447 - accuracy: 0.0548 - val_loss: 3.5818 - val_accuracy: 0.0764\n",
      "Epoch 149/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6315 - accuracy: 0.0548 - val_loss: 3.5746 - val_accuracy: 0.0764\n",
      "Epoch 150/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6790 - accuracy: 0.0488 - val_loss: 3.5728 - val_accuracy: 0.0799\n",
      "Epoch 151/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6026 - accuracy: 0.0599 - val_loss: 3.5623 - val_accuracy: 0.0833\n",
      "Epoch 152/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6682 - accuracy: 0.0514 - val_loss: 3.5609 - val_accuracy: 0.0799\n",
      "Epoch 153/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6477 - accuracy: 0.0514 - val_loss: 3.5608 - val_accuracy: 0.0729\n",
      "Epoch 154/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6769 - accuracy: 0.0557 - val_loss: 3.5608 - val_accuracy: 0.0833\n",
      "Epoch 155/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6241 - accuracy: 0.0642 - val_loss: 3.5630 - val_accuracy: 0.0833\n",
      "Epoch 156/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6496 - accuracy: 0.0719 - val_loss: 3.5662 - val_accuracy: 0.0694\n",
      "Epoch 157/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6392 - accuracy: 0.0616 - val_loss: 3.5659 - val_accuracy: 0.0694\n",
      "Epoch 158/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6551 - accuracy: 0.0599 - val_loss: 3.5644 - val_accuracy: 0.0833\n",
      "Epoch 159/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6269 - accuracy: 0.0616 - val_loss: 3.5598 - val_accuracy: 0.0833\n",
      "Epoch 160/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6583 - accuracy: 0.0608 - val_loss: 3.5639 - val_accuracy: 0.0694\n",
      "Epoch 161/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6070 - accuracy: 0.0608 - val_loss: 3.5646 - val_accuracy: 0.0799\n",
      "Epoch 162/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6014 - accuracy: 0.0736 - val_loss: 3.5690 - val_accuracy: 0.0764\n",
      "Epoch 163/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6328 - accuracy: 0.0651 - val_loss: 3.5633 - val_accuracy: 0.0764\n",
      "Epoch 164/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6107 - accuracy: 0.0565 - val_loss: 3.5609 - val_accuracy: 0.0764\n",
      "Epoch 165/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6265 - accuracy: 0.0574 - val_loss: 3.5602 - val_accuracy: 0.0799\n",
      "Epoch 166/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6826 - accuracy: 0.0557 - val_loss: 3.5633 - val_accuracy: 0.0694\n",
      "Epoch 167/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6104 - accuracy: 0.0599 - val_loss: 3.5612 - val_accuracy: 0.0729\n",
      "Epoch 168/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5843 - accuracy: 0.0666 - val_loss: 3.5618 - val_accuracy: 0.0694\n",
      "Epoch 169/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6396 - accuracy: 0.0539 - val_loss: 3.5668 - val_accuracy: 0.0729\n",
      "Epoch 170/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5975 - accuracy: 0.0685 - val_loss: 3.5665 - val_accuracy: 0.0764\n",
      "Epoch 171/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5846 - accuracy: 0.0582 - val_loss: 3.5642 - val_accuracy: 0.0625\n",
      "Epoch 172/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5874 - accuracy: 0.0685 - val_loss: 3.5633 - val_accuracy: 0.0625\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6022 - accuracy: 0.0659 - val_loss: 3.5617 - val_accuracy: 0.0764\n",
      "Epoch 174/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5860 - accuracy: 0.0693 - val_loss: 3.5640 - val_accuracy: 0.0660\n",
      "Epoch 175/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6207 - accuracy: 0.0685 - val_loss: 3.5643 - val_accuracy: 0.0764\n",
      "Epoch 176/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5963 - accuracy: 0.0659 - val_loss: 3.5558 - val_accuracy: 0.0799\n",
      "Epoch 177/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6107 - accuracy: 0.0565 - val_loss: 3.5603 - val_accuracy: 0.0764\n",
      "Epoch 178/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6154 - accuracy: 0.0685 - val_loss: 3.5573 - val_accuracy: 0.0764\n",
      "Epoch 179/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6033 - accuracy: 0.0634 - val_loss: 3.5539 - val_accuracy: 0.0799\n",
      "Epoch 180/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6176 - accuracy: 0.0693 - val_loss: 3.5532 - val_accuracy: 0.0660\n",
      "Epoch 181/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5697 - accuracy: 0.0771 - val_loss: 3.5481 - val_accuracy: 0.0833\n",
      "Epoch 182/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6228 - accuracy: 0.0616 - val_loss: 3.5469 - val_accuracy: 0.0625\n",
      "Epoch 183/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5808 - accuracy: 0.0668 - val_loss: 3.5455 - val_accuracy: 0.0764\n",
      "Epoch 184/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5973 - accuracy: 0.0642 - val_loss: 3.5442 - val_accuracy: 0.0868\n",
      "Epoch 185/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5569 - accuracy: 0.0668 - val_loss: 3.5426 - val_accuracy: 0.0833\n",
      "Epoch 186/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5933 - accuracy: 0.0668 - val_loss: 3.5467 - val_accuracy: 0.0729\n",
      "Epoch 187/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5891 - accuracy: 0.0634 - val_loss: 3.5509 - val_accuracy: 0.0764\n",
      "Epoch 188/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5933 - accuracy: 0.0616 - val_loss: 3.5489 - val_accuracy: 0.0833\n",
      "Epoch 189/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5949 - accuracy: 0.0651 - val_loss: 3.5438 - val_accuracy: 0.0868\n",
      "Epoch 190/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6064 - accuracy: 0.0462 - val_loss: 3.5369 - val_accuracy: 0.0868\n",
      "Epoch 191/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5700 - accuracy: 0.0648 - val_loss: 3.5404 - val_accuracy: 0.0833\n",
      "Epoch 192/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5465 - accuracy: 0.0736 - val_loss: 3.5441 - val_accuracy: 0.0799\n",
      "Epoch 193/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5959 - accuracy: 0.0666 - val_loss: 3.5412 - val_accuracy: 0.0833\n",
      "Epoch 194/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5729 - accuracy: 0.0616 - val_loss: 3.5444 - val_accuracy: 0.0903\n",
      "Epoch 195/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5946 - accuracy: 0.0574 - val_loss: 3.5425 - val_accuracy: 0.0799\n",
      "Epoch 196/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6182 - accuracy: 0.0651 - val_loss: 3.5408 - val_accuracy: 0.0729\n",
      "Epoch 197/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5604 - accuracy: 0.0676 - val_loss: 3.5398 - val_accuracy: 0.0799\n",
      "Epoch 198/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.6149 - accuracy: 0.0625 - val_loss: 3.5377 - val_accuracy: 0.0694\n",
      "Epoch 199/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5689 - accuracy: 0.0616 - val_loss: 3.5386 - val_accuracy: 0.0764\n",
      "Epoch 200/200\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 3.5820 - accuracy: 0.0648 - val_loss: 3.5401 - val_accuracy: 0.0694\n"
     ]
    }
   ],
   "source": [
    "train_summary = classify_gabor.fit(train_gen, epochs = 200, verbose = 1, callbacks = None, validation_data = val_gen, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen)//batch_size, \n",
    "                              validation_steps = len(val_gen)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe5af0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'model_classify_gabor_norm_dense_4096-2048-1024_AWA2_nodrop_200eph_16bch_tanh_0.001lr'\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush/Results/'\n",
    "\n",
    "classify_gabor.save_weights(save_path + 'Wt_' + name + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e7fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75db527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4096)]            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                51250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,321,394\n",
      "Trainable params: 27,321,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush/Results/'\n",
    "\n",
    "classify_gabor.load_weights(save_path + 'Wt_model_classify_gabor_norm_dense_4096-2048-1024_AWA2_nodrop_200eph_16bch_tanh_0.001lr.h5')\n",
    "classify_gabor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c601488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4096)]            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,171,968\n",
      "Trainable params: 25,171,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor_model = Model(inputs = [classify_gabor.input], outputs = [classify_gabor.layers[-3].output])\n",
    "feature_extractor_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ee4400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21482 11452 21448 ...  7992 10388  5160] 23527\n",
      "23527\n",
      "1470/1470 [==============================] - 2s 891us/step\n",
      "[[ 0.12128699  0.1714684   0.09327443 ... -0.22257476 -0.0789879\n",
      "   0.24663197]\n",
      " [ 0.02415382 -0.11808714  0.02814358 ...  0.05856087  0.00449371\n",
      "   0.08509304]\n",
      " [-0.07248039  0.02478457 -0.01801182 ... -0.37423903  0.03677747\n",
      "  -0.22551486]\n",
      " ...\n",
      " [-0.02271677 -0.02799692  0.05151584 ...  0.03339488 -0.11488006\n",
      "  -0.01951144]\n",
      " [-0.00344384  0.20381     0.13336962 ... -0.13013028 -0.07593524\n",
      "   0.04537195]\n",
      " [ 0.19128346 -0.00070702 -0.08745501 ...  0.06849919  0.02539712\n",
      "   0.24146035]] (23520, 2048)\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "split_path = os.path.join(data_path,'data/xlsa17/data/AWA2/att_splits.mat')\n",
    "matcontent = sio.loadmat(split_path)\n",
    "trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "\n",
    "print(trainval_loc, len(trainval_loc))\n",
    "\n",
    "train_labels = labels[trainval_loc]\n",
    "print(len(train_labels))    \n",
    "batch_size=16\n",
    "train_all_gen = DataGenerator(train_gabor_feature_transformed, train_labels, batch_size=16)  \n",
    "\n",
    "transformed_gabor_features = feature_extractor_model.predict(train_all_gen, steps = len(train_gabor_feature_transformed)//batch_size, verbose = 1)\n",
    "print(transformed_gabor_features, transformed_gabor_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_gabor_features = transformed_gabor_features.reshape([23520, 2048])\n",
    "print(transformed_gabor_features, transformed_gabor_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18957fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('C:/Users/Admin/Sushree_Codes/Sush/Results/transformed_gabor_features_dense_2048.npy', transformed_gabor_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a27cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69d9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf1c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6887be4",
   "metadata": {},
   "source": [
    "# Classifier 1\n",
    "\n",
    "model_classify_gabor_norm_dense_4096-2048-1024_AWA2_nodrop_200eph_16bch_tanh_0.01lr\n",
    "loss: 1.0345 - accuracy: 0.7166 - val_loss: 6.7552 - val_accuracy: 0.1146\n",
    "\n",
    "model_classify_gabor_norm_dense_4096-2048-1024_AWA2_nodrop_200eph_16bch_tanh_0.001lr\n",
    "loss: 0.7242 - accuracy: 0.8048 - val_loss: 5.0129 - val_accuracy: 0.1354\n",
    "\n",
    "model_classify_gabor_norm_dense_4096-2048-1024_AWA2_drop0.1_200eph_16bch_tanh_0.001lr\n",
    "loss: 0.9641 - accuracy: 0.7286 - val_loss: 4.8799 - val_accuracy: 0.1354\n",
    "\n",
    "model_classify_gabor_norm_dense_4096-2048-1024_AWA2_drop0.2_200eph_16bch_tanh_0.001lr\n",
    "loss: 1.1647 - accuracy: 0.6889 - val_loss: 4.6598 - val_accuracy: 0.1389\n",
    "\n",
    "model_classify_gabor_norm_dense_4096-2048-1024_AWA2_drop0.1_200eph_16bch_tanh_0.0001lr\n",
    "loss: 3.2817 - accuracy: 0.1832 - val_loss: 3.4888 - val_accuracy: 0.0972\n",
    "\n",
    "\n",
    "# Classifier 2\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_AWA2_nodrop_200eph_16bch_tanh_0.01lr\n",
    "loss: 0.9290 - accuracy: 0.7372 - val_loss: 6.8631 - val_accuracy: 0.1424\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_AWA2_nodrop_200eph_16bch_tanh_0.001lr\n",
    "loss: 0.8655 - accuracy: 0.7791 - val_loss: 4.9546 - val_accuracy: 0.1424\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_AWA2_nodrop_200eph_16bch_tanh_0.0001lr\n",
    "loss: 3.2592 - accuracy: 0.1978 - val_loss: 3.4928 - val_accuracy: 0.0660\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_AWA2_drop0.1_200eph_16bch_tanh_0.001lr\n",
    "loss: 0.9020 - accuracy: 0.7620 - val_loss: 4.9282 - val_accuracy: 0.1458\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_AWA2_drop0.2_200eph_16bch_tanh_0.001lr\n",
    "loss: 1.0742 - accuracy: 0.6969 - val_loss: 4.7865 - val_accuracy: 0.1493\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_AWA2_drop0.5_200eph_16bch_tanh_0.001lr\n",
    "loss: 1.9019 - accuracy: 0.4863 - val_loss: 3.6221 - val_accuracy: 0.1667\n",
    "\n",
    "\n",
    "# Classifier 3\n",
    "model_classify_gabor_conv1_4096-2048-1024_dense_1024_AWA2_nodrop_200eph_16bch_tanh_0.01lr\n",
    "loss: 1.3389 - accuracy: 0.6541 - val_loss: 5.8730 - val_accuracy: 0.1562\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_dense_1024_AWA2_nodrop_200eph_16bch_tanh_0.001lr\n",
    "loss: 0.7646 - accuracy: 0.7971 - val_loss: 5.7121 - val_accuracy: 0.1354\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_dense_1024_AWA2_nodrop_200eph_16bch_tanh_0.0001lr\n",
    "loss: 3.1531 - accuracy: 0.2140 - val_loss: 3.4561 - val_accuracy: 0.1042\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_dense_1024_AWA2_drop0.1_200eph_16bch_tanh_0.001lr\n",
    "loss: 3.1864 - accuracy: 0.1962 - val_loss: 3.4442 - val_accuracy: 0.0903\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_dense_1024_AWA2_drop0.2_200eph_16bch_tanh_0.001lr\n",
    "loss: 3.2640 - accuracy: 0.1738 - val_loss: 3.4726 - val_accuracy: 0.0729\n",
    "\n",
    "model_classify_gabor_conv1_4096-2048-1024_dense_1024_AWA2_drop0.5_200eph_16bch_tanh_0.001lr\n",
    "loss: 3.5820 - accuracy: 0.0648 - val_loss: 3.5401 - val_accuracy: 0.0694\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
