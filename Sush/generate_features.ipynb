{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as nd        \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt    \n",
    "from numpy.linalg import norm\n",
    "from keras.optimizers import SGD, Adam\n",
    "import graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60fa49",
   "metadata": {},
   "source": [
    "# Define the Path to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b23e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:/Sushree/Dataset/'\n",
    "\n",
    "img_path = os.path.join(data_path,'Animals_with_Attributes2/JPEGImages/')\n",
    "print(img_path)\n",
    "\n",
    "print(len(os.listdir(img_path)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9511f1",
   "metadata": {},
   "source": [
    "# Prepare the dataset of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d0118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_imlist(path, option):\n",
    "    if option == 'jpg':\n",
    "        return[os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "    \n",
    "def prepare_dataset(img_path, width, height, option):\n",
    "    folder_list = os.listdir(img_path)\n",
    "    num_classes = len(folder_list)\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(len(folder_list)):\n",
    "        print(i)\n",
    "        img_list = get_imlist(os.path.join(img_path, folder_list[i]), option)\n",
    "        for j in range(len(img_list)):\n",
    "            img = image.load_img(img_list[j], target_size = (width, height))\n",
    "            img = np.array(img)\n",
    "            #img = np.true_divide(img,[255.0], out = None)\n",
    "            img = np.divide(np.subtract(img, np.mean(img)), np.std(img))\n",
    "            x = image.img_to_array(img)\n",
    "            images.append(x)\n",
    "            labels.append(i)\n",
    "    images = np.array(images, dtype = np.float32)                             \n",
    "    labels = to_categorical(labels, num_classes)                             \n",
    "    return images, labels  \n",
    "\n",
    "\n",
    "width, height, ch = 224, 224, 3\n",
    "images, labels = prepare_dataset(img_path, width, height,'jpg')\n",
    "\n",
    "print(images.shape)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abf817",
   "metadata": {},
   "source": [
    "# Prepare data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91a87c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "split_path = os.path.join(data_path,'data/xlsa17/data/AWA2/att_splits.mat')\n",
    "matcontent = sio.loadmat(split_path)\n",
    "trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "\n",
    "#print(matcontent)\n",
    "\n",
    "print(trainval_loc, len(trainval_loc))\n",
    "\n",
    "train_images = images[trainval_loc]\n",
    "train_labels = labels[trainval_loc]\n",
    "print(train_images.shape)\n",
    "print(len(train_labels))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "batch_size = 16\n",
    "train_gen = DataGenerator(X_train, y_train, batch_size)   \n",
    "val_gen = DataGenerator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178db7a3",
   "metadata": {},
   "source": [
    "# Define the CNN and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3d960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.applications.resnet.ResNet101(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "  \n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "pred = Dense(50, activation='softmax')(x)\n",
    "\n",
    "new_model = Model(inputs = model.input, outputs = pred)\n",
    "new_model.summary()\n",
    "\n",
    "\n",
    "for layer in new_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "\n",
    "sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "new_model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#train_summary = new_model.fit(train_gen, epochs = 100, verbose = 1, callbacks = None, validation_data = val_gen, \n",
    "#                              shuffle = True, steps_per_epoch = len(train_gen)//batch_size, \n",
    "#                              validation_steps = len(val_gen)//batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca3315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "new_model.save_weights('C:/Users/Admin/Sushree_Codes/Sush/Results/Wt_ResNet101_AWA2_finetune_100eph_16bch_1e-2lr.h5')\n",
    "\n",
    "print(train_summary.history.keys())\n",
    "\n",
    "plt.plot(train_summary.history['accuracy'])\n",
    "plt.plot(train_summary.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc = 'upper left')\n",
    "plt.show()\n",
    "plt.savefig('C:/Users/Admin/Sushree_Codes/Sush/Results/Acc_ResNet101_AWA2_finetune_100eph_16bch_1e-2lr.png')\n",
    "\n",
    "plt.plot(train_summary.history['loss'])\n",
    "plt.plot(train_summary.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc = 'upper left')\n",
    "plt.show()\n",
    "plt.savefig('C:/Users/Admin/Sushree_Codes/Sush/Results/Loss_ResNet101_AWA2_finetune_100eph_16bch_1e-2lr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c9b9a",
   "metadata": {},
   "source": [
    "# Define test data and evaluate the model on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c24986",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "print(test_seen_loc, len(test_seen_loc))\n",
    "\n",
    "test_seen_images = images[test_seen_loc]\n",
    "test_seen_labels = labels[test_seen_loc]\n",
    "print(test_seen_images.shape)\n",
    "print(len(test_seen_labels))\n",
    "\n",
    "test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "print(test_unseen_loc, len(test_unseen_loc))\n",
    "\n",
    "test_unseen_images = images[test_unseen_loc]\n",
    "test_unseen_labels = labels[test_unseen_loc]\n",
    "print(test_unseen_images.shape)\n",
    "print(len(test_unseen_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ceeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seen_gen = DataGenerator(test_seen_images, test_seen_labels, batch_size)\n",
    "test_unseen_gen = DataGenerator(test_unseen_images, test_unseen_labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c8ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_model.evaluate(test_seen_gen, steps = len(test_seen_gen)//batch_size, verbose = 1)\n",
    "new_model.evaluate(test_unseen_gen, steps = len(test_unseen_gen)//batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9bf59d",
   "metadata": {},
   "source": [
    "# Load weights to the CNN and extract features from intermediate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b74f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_model.load_weights('C:/Users/Admin/Sushree_Codes/Sush/Results/Wt_ResNet101_AWA2_finetune_100eph_16bch_1e-2lr.h5')\n",
    "train_all_gen = DataGenerator(train_images, train_labels, batch_size)  \n",
    "\n",
    "feature_extractor_model1 = Model(inputs = [new_model.input], outputs = [new_model.layers[-3].output])\n",
    "feature_extractor_model1.summary()\n",
    "\n",
    "train_visual_features1 = feature_extractor_model1.predict(train_all_gen, steps = len(train_images)//batch_size, verbose = 1)\n",
    "print(train_visual_features1.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeab722",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_visual_features1, train_visual_features1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = new_model.get_layer('conv5_block2_out').output\n",
    "out = GlobalAveragePooling2D()(x)\n",
    "\n",
    "feature_extractor_model2 = Model(inputs = new_model.input, outputs = out)\n",
    "feature_extractor_model2.summary()\n",
    "\n",
    "train_visual_features2 = feature_extractor_model2.predict(train_all_gen, steps = len(train_images)//batch_size, verbose = 1)\n",
    "print(train_visual_features2, train_visual_features2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc46990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed9fa67",
   "metadata": {},
   "source": [
    "# Compute Cosine Similarities of train_visual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bbd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visual_features_flaten = train_visual_features.flatten()\n",
    "\n",
    "print(train_visual_features_flaten.shape)\n",
    "cosine = np.dot(train_visual_features_flaten, train_visual_features_flaten)/(norm(train_visual_features_flaten)*norm(train_visual_features_flaten))\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd776d18",
   "metadata": {},
   "source": [
    "# Compute transformed visual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadcd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visual_features, feat_length = visual_features.shape\n",
    "\n",
    "feat = Input(shape = (feat_length))\n",
    "dense1 = Dense(2048, activation='relu')(feat)\n",
    "dense2 = Dense(1024, activation='relu')(dense1)\n",
    "#pred = Dense(50, activation='softmax')(dense2)\n",
    "\n",
    "model_transform_visual = Model(inputs = feat, outputs = dense2)\n",
    "model_transform_visual.summary()\n",
    "\n",
    "sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model_transform_visual.compile(optimizer = sgd, loss = 'mse', metrics = ['accuracy'])\n",
    "\n",
    "transformed_visual_features = model_transform_visual.predict(visual_features)\n",
    "\n",
    "print(transformed_visual_features, transformed_visual_features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548d745",
   "metadata": {},
   "source": [
    "# Compute Cosine Similarities of transformed_visual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_visual_features_flaten = transformed_visual_features.flatten()\n",
    "print(transformed_visual_features_flaten.shape)\n",
    "cosine = np.dot(transformed_visual_features_flaten, transformed_visual_features_flaten)/(norm(transformed_visual_features_flaten)*norm(transformed_visual_features_flaten))\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19db4d",
   "metadata": {},
   "source": [
    "# Extract gabor features from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608dc793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "kernels = []\n",
    "for theta in range(2):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = gabor_kernel(frequency, theta = theta, sigma_x = sigma, sigma_y = sigma)\n",
    "            kernels.append(kernel)\n",
    "            \n",
    "def define_power(image, kernel):\n",
    "    # Normalize images for better comparison.\n",
    "    image = (image - image.mean()) / image.std()\n",
    "    #print(np.shape(image))\n",
    "    return np.sqrt(nd.convolve(image, np.real(kernel), mode='wrap')**2 + nd.convolve(image, np.imag(kernel), mode='wrap')**2)\n",
    "            \n",
    "def compute_gabor_feats(image, kernels):\n",
    "    feature = np.zeros((len(kernels), image.shape[0], image.shape[1]))\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        res_pow = define_power(image, kernel)\n",
    "        feature[k] = res_pow\n",
    "        #feature = np.add(feature, res_pow)                \n",
    "    return feature  \n",
    "\n",
    "def get_imlist(path, option):\n",
    "    if option == 'jpg':\n",
    "        return[os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "\n",
    "width, height, ch = 224, 224, 3\n",
    "\n",
    "def compute_feature_dataset(img_path, width, height, option, kernels):\n",
    "    folder_list = os.listdir(img_path)\n",
    "    num_classes = len(folder_list)\n",
    "    gabor_feature = []\n",
    "    for i in range(len(folder_list)):\n",
    "        print(i)\n",
    "        img_list = get_imlist(os.path.join(img_path, folder_list[i]), option)\n",
    "        for j in range(len(img_list)):\n",
    "            img = image.load_img(img_list[j], color_mode=\"grayscale\", target_size = (width, height))\n",
    "            img = asarray(img)\n",
    "            gabor_feat = compute_gabor_feats(img, kernels)\n",
    "            #gabor_feat = np.expand_dims(gabor_feat, axis=0)\n",
    "            gabor_feature.append(gabor_feat)\n",
    "    gabor_feature = np.array(gabor_feature, dtype = np.float32)         \n",
    "    return gabor_feature \n",
    "\n",
    "\n",
    "gabor_feature = compute_feature_dataset(img_path, width, height, 'jpg', kernels)\n",
    "print(np.shape(gabor_feature))    \n",
    "\n",
    "np.save('C:/Users/Admin/Sushree_Codes/Sush/Results/gabor_feature_8_kernels.npy', gabor_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gabor_feature = np.load('C:/Users/Admin/Sushree_Codes/Sush/Results/gabor_feature_8_kernels.npy')\n",
    "print(gabor_feature.shape)\n",
    "\n",
    "print(np.shape(gabor_feature))\n",
    "print(trainval_loc, trainval_loc.shape)\n",
    "\n",
    "train_gabor_feature = []\n",
    "for i in range (len(trainval_loc)):\n",
    "    train_gabor_feat = gabor_feature[trainval_loc[i]]\n",
    "    train_gabor_feat = train_gabor_feat.reshape(np.prod(train_gabor_feat.shape[0:]))\n",
    "    train_gabor_feature.append(train_gabor_feat)\n",
    "\n",
    "train_gabor_feature = np.array(train_gabor_feature, dtype = np.float32) \n",
    "print(np.shape(train_gabor_feature))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# performing preprocessing part\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96beecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_gabor_feature\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = sc.fit_transform(x)\n",
    "#print(X_t, X_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784f781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    " \n",
    "pca = PCA(n_components = 100)\n",
    " \n",
    "y = pca.fit_transform(X_t)\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9bf90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_s = transformer.fit_transform(X_t)\n",
    "print(y_s, y_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209906eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = KernelPCA(n_components=100, kernel='cosine')\n",
    "X_transformed = transformer.fit_transform(X_t)\n",
    "print(X_transformed, X_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059c47d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "transformer = KernelPCA(n_components=100, kernel='poly')\n",
    "X_transformed = transformer.fit_transform(X_t)\n",
    "print(X_transformed, X_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_t = sc.fit_transform(train_gabor_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    " \n",
    "pca = PCA(n_components = 2048)\n",
    " \n",
    "train_gabor_feature_compressed = pca.fit_transform(X_t)\n",
    "#X_t = pca.transform(X_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_gabor_feature_compressed, np.shape(train_gabor_feature_compressed))\n",
    "np.save('C:/Users/Admin/Sushree_Codes/Sush/Results/train_gabor_feature_8_kernels_compressed.npy', train_gabor_feature_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gabor_feature = np.load('C:/Users/Admin/Sushree_Codes/Sush/Results/train_gabor_feature_8_kernels_compressed.npy')\n",
    "\n",
    "train_gabor_feature = np.array(train_gabor_feature, dtype = np.float32) \n",
    "print(train_gabor_feature, np.shape(train_gabor_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d8f87",
   "metadata": {},
   "source": [
    "# Compute Cosine Similarities of train_gabor_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gabor_feature_flaten = train_gabor_feature.flatten()\n",
    "print(train_gabor_feature_flaten.shape)\n",
    "cosine = np.dot(train_gabor_feature_flaten, train_gabor_feature_flaten)/(norm(train_gabor_feature_flaten)*norm(train_gabor_feature_flaten))\n",
    "print(cosine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1dddfa",
   "metadata": {},
   "source": [
    "# Compute transformed handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features, feat_width, feat_height = np.shape(gabor_feature)\n",
    "\n",
    "feat = Input(shape = (feat_width, feat_height))\n",
    "flat = Flatten()(feat)\n",
    "dense1 = Dense(2048, activation='relu')(flat)\n",
    "dense2 = Dense(1024, activation='relu')(dense1)\n",
    "\n",
    "model_transform_handcrafted = Model(inputs = feat, outputs = dense2)\n",
    "model_transform_handcrafted.summary()\n",
    "\n",
    "sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model_transform_handcrafted.compile(optimizer = sgd, loss = 'mse', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ac9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatGenerator(Sequence):\n",
    "    def __init__(self, x_set, batch_size):\n",
    "        self.x = x_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x\n",
    "    \n",
    "train_gabor_feature_gen = FeatGenerator(train_gabor_feature, batch_size)\n",
    "transformed_handcrafted_features = model_transform_handcrafted.predict(train_gabor_feature_gen, steps = len(train_images)//batch_size, verbose = 1)\n",
    "\n",
    "print(transformed_handcrafted_features, transformed_handcrafted_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc77c1e",
   "metadata": {},
   "source": [
    "# Compute Cosine Similarities of transformed_handcrafted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f146fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_handcrafted_features_flaten = transformed_handcrafted_features.flatten()\n",
    "print(transformed_handcrafted_features_flaten.shape)\n",
    "cosine = np.dot(transformed_handcrafted_features_flaten, transformed_handcrafted_features_flaten)/(norm(transformed_handcrafted_features_flaten)*norm(transformed_handcrafted_features_flaten))\n",
    "print(cosine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09642fd",
   "metadata": {},
   "source": [
    "# Compute Cosine Similarities between transformed_visual_features and transformed_handcrafted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = np.dot(transformed_handcrafted_features_flaten, transformed_visual_features_flaten)/(norm(transformed_handcrafted_features_flaten)*norm(transformed_visual_features_flaten))\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f380d",
   "metadata": {},
   "source": [
    "# Perform visual to handcrafted feature embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatGenerator(Sequence):\n",
    "    def __init__(self, x_set, batch_size):\n",
    "        self.x = x_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x\n",
    "\n",
    "num_features, feat_len = np.shape(train_gabor_feature)\n",
    "\n",
    "feat = Input(shape = (feat_len))\n",
    "flat = Flatten()(feat)\n",
    "\n",
    "model_transform_handcrafted = Model(inputs = feat, outputs = flat)\n",
    "model_transform_handcrafted.summary()\n",
    "\n",
    "train_gabor_feature_gen = FeatGenerator(train_gabor_feature, batch_size)\n",
    "transformed_handcrafted_features = model_transform_handcrafted.predict(train_gabor_feature_gen, steps = len(train_images)//batch_size, verbose = 1)\n",
    "\n",
    "print(transformed_handcrafted_features, transformed_handcrafted_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features, feat_length = train_visual_features.shape\n",
    "\n",
    "feat = Input(shape = (feat_length))\n",
    "dense1 = Dense(1024, activation='relu')(feat)\n",
    "drop1 = Dropout(0.1)(dense1)\n",
    "dense2 = Dense(768, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.1)(dense2)\n",
    "dense3 = Dense(512, activation='relu')(drop2)\n",
    "drop3 = Dropout(0.1)(dense3)\n",
    "dense4 = Dense(768, activation='relu')(drop3)\n",
    "drop4 = Dropout(0.1)(dense4)\n",
    "dense5 = Dense(1024, activation='relu')(drop4)\n",
    "drop5 = Dropout(0.1)(dense5)\n",
    "output = Dense(2048, activation='relu')(drop5)\n",
    "model_handcrafted = Model(inputs = feat, outputs = output)\n",
    "model_handcrafted.summary()\n",
    "\n",
    "#opy = tf.keras.optimizers.Adagrad(learning_rate=0.0001, initial_accumulator_value=0.1, epsilon=1e-07)\n",
    "opt = Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n",
    "#opt = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model_handcrafted.compile(optimizer = opt, loss = 'mse', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19606a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feat = train_visual_features\n",
    "print(input_feat.shape)\n",
    "\n",
    "output_feat = transformed_handcrafted_features\n",
    "print(output_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3450c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(input_feat, output_feat, test_size = 0.2, random_state = 42)\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "y_train = y_train.astype('float32') / 255.0\n",
    "y_val = y_val.astype('float32') / 255.0\n",
    "\n",
    "class FeatureGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "batch_size = 16\n",
    "train_gen = FeatureGenerator(X_train, y_train, batch_size)   \n",
    "val_gen = FeatureGenerator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57cb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920ab0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_summary = model_handcrafted.fit(train_gen, epochs = 100, verbose = 1, callbacks = None, \n",
    "                                      validation_data = val_gen, shuffle = True, \n",
    "                                      steps_per_epoch = len(train_gen)//batch_size, \n",
    "                                      validation_steps = len(val_gen)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afb6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d8561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c4141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab650e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cdcbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca963990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac47152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737970b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52f62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac2795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd1b69f0",
   "metadata": {},
   "source": [
    "#Results\n",
    "\n",
    "1. Finetune ResNet101 model (pretrained on ImageNet) using training data and evaluate on test seen and unseen data\n",
    "\n",
    "test seen data: loss: 0.9136 - accuracy: 0.8098\n",
    "\n",
    "test unseen data: loss: 16.7142 - accuracy: 0.0000e+00\n",
    "\n",
    " \n",
    "   \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
