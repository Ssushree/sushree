{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a4b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "import numpy as np\n",
    "from dataset import Dataset\n",
    "from classifier import Classifier\n",
    "from util import map_label\n",
    "import tensorflow.keras.backend as K\n",
    "import logging\n",
    "\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b37b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:This dataset does not support GZSL validation\n",
      "INFO:root:preprocessing: False - validation: False\n",
      "INFO:root:features: (37322, 2048) - attributes: (50, 85)\n",
      "INFO:root:seen classes: 40 - unseen classes: 10\n",
      "INFO:root:training: 23527 - test seen: 5882 - test unseen: 7913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CE_GZSL:\n",
    "\n",
    "    # optimizers\n",
    "\n",
    "    generator_optimizer = None\n",
    "    discriminator_optimizer = None\n",
    "\n",
    "    # nets\n",
    "\n",
    "    embedding_net = None\n",
    "    comparator_net = None\n",
    "    generator_net = None\n",
    "    discriminator_net = None\n",
    "\n",
    "    # params\n",
    "\n",
    "    discriminator_iterations = None\n",
    "    generator_noise = None\n",
    "    gp_weight = None\n",
    "    instance_weight = None\n",
    "    class_weight = None\n",
    "    instance_temperature = None\n",
    "    class_temperature = None\n",
    "    synthetic_number = None\n",
    "    gzsl = True#None\n",
    "    visual_size = None\n",
    "\n",
    "    def __init__(self, generator_optimizer: keras.optimizers.Optimizer,\n",
    "                 discriminator_optimizer: keras.optimizers.Optimizer,\n",
    "                 args: dict, **kwargs):\n",
    "        super(CE_GZSL, self).__init__(**kwargs)\n",
    "\n",
    "        self.embedding(args[\"visual_size\"], args[\"embedding_hidden\"], args[\"embedding_size\"])\n",
    "        self.comparator(args[\"embedding_hidden\"], args[\"attribute_size\"], args[\"comparator_hidden\"])\n",
    "        self.generator(args[\"visual_size\"], args[\"attribute_size\"], args[\"generator_noise\"], args[\"generator_hidden\"])\n",
    "        self.discriminator(args[\"visual_size\"], args[\"attribute_size\"], args[\"discriminator_hidden\"])\n",
    "\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.instance_weight = args[\"instance_weight\"]\n",
    "        self.class_weight = args[\"class_weight\"]\n",
    "        self.instance_temperature = args[\"instance_temperature\"]\n",
    "        self.class_temperature = args[\"class_temperature\"]\n",
    "        self.gp_weight = args[\"gp_weight\"]\n",
    "        self.synthetic_number = args[\"synthetic_number\"]\n",
    "        self.gzsl = args[\"gzsl\"]\n",
    "        self.visual_size = args[\"visual_size\"]\n",
    "\n",
    "        self.discriminator_iterations = args[\"discriminator_iterations\"]\n",
    "        self.generator_noise = args[\"generator_noise\"]\n",
    "\n",
    "    def summary(self):\n",
    "\n",
    "        networks = [self.embedding_net, self.comparator_net, self.generator_net, self.discriminator_net]\n",
    "\n",
    "        for net in networks:\n",
    "            net.summary()\n",
    "\n",
    "    def embedding(self, visual_size, hidden_units, embedded_size):\n",
    "\n",
    "        inputs = keras.Input(shape=visual_size)\n",
    "        x = keras.layers.Dense(hidden_units)(inputs)\n",
    "        embed_h = ReLU(name=\"embed_h\")(x)\n",
    "        x = keras.layers.Dense(embedded_size)(embed_h)\n",
    "\n",
    "        embed_z = keras.layers.Lambda(lambda x: K.l2_normalize(x, axis=1), name=\"embed_z\")(x)\n",
    "\n",
    "        self.embedding_net = keras.Model(inputs, [embed_h, embed_z], name=\"embedding\")\n",
    "\n",
    "    def comparator(self, embedding_size, attribute_size, hidden_units):\n",
    "\n",
    "        inputs = keras.Input(shape=embedding_size + attribute_size)\n",
    "        x = keras.layers.Dense(hidden_units)(inputs)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        output = keras.layers.Dense(1, name=\"comp_out\")(x)\n",
    "\n",
    "        self.comparator_net = keras.Model(inputs, output, name=\"comparator\")\n",
    "\n",
    "    def generator(self, visual_size, attribute_size, noise, hidden_units):\n",
    "\n",
    "        inputs = keras.Input(shape=attribute_size + noise)\n",
    "        x = keras.layers.Dense(hidden_units)(inputs)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = keras.layers.Dense(visual_size)(x)\n",
    "        output = ReLU(name=\"gen_out\")(x)\n",
    "        self.generator_net = keras.Model(inputs, output, name=\"generator\")\n",
    "\n",
    "    def discriminator(self, visual_size, attribute_size, hidden_units):\n",
    "\n",
    "        inputs = keras.Input(shape=visual_size + attribute_size)\n",
    "        x = keras.layers.Dense(hidden_units)(inputs)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        output = keras.layers.Dense(1, name=\"disc_out\")(x)\n",
    "        self.discriminator_net = keras.Model(inputs, output, name=\"discriminator\")\n",
    "\n",
    "    def d_loss_fn(self, real_logits, fake_logits):\n",
    "\n",
    "        real_loss = tf.reduce_mean(real_logits)\n",
    "        fake_loss = tf.reduce_mean(fake_logits)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images, attribute_data):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.uniform(shape=(batch_size, 1))\n",
    "        alpha = tf.tile(alpha, (1, real_images.shape[1]))\n",
    "        interpolated = real_images * alpha + (1-alpha) * fake_images\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator_net(tf.concat([interpolated, attribute_data], axis=1))\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def contrastive_criterion(self, labels, feature_vectors):\n",
    "\n",
    "        # Compute logits\n",
    "        anchor_dot_contrast = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors, tf.transpose(feature_vectors)\n",
    "            ),\n",
    "            self.instance_temperature,\n",
    "        )\n",
    "\n",
    "        logits_max = tf.reduce_max(anchor_dot_contrast, 1, keepdims=True)\n",
    "        logits = anchor_dot_contrast - logits_max\n",
    "\n",
    "        # Expand to [batch_size, 1]\n",
    "        labels = tf.reshape(labels, (-1, 1))\n",
    "        mask = tf.cast(tf.equal(labels, tf.transpose(labels)), dtype=tf.float32)\n",
    "\n",
    "        # rosife: all except anchor\n",
    "        logits_mask = tf.Variable(tf.ones_like(mask))\n",
    "        indices = tf.reshape(tf.range(0, tf.shape(mask)[0]), (-1, 1))\n",
    "        indices = tf.concat([indices, indices], axis=1)\n",
    "        updates = tf.zeros((tf.shape(mask)[0]))\n",
    "        logits_mask.scatter_nd_update(indices, updates)\n",
    "\n",
    "        # rosife: positive except anchor\n",
    "        mask = mask * logits_mask\n",
    "        single_samples = tf.cast(tf.equal(tf.reduce_sum(mask, axis=1), 0), dtype=tf.float32)\n",
    "\n",
    "        # compute log_prob\n",
    "        masked_logits = tf.exp(logits) * logits_mask\n",
    "        log_prob = logits - tf.math.log(tf.reduce_sum(masked_logits, 1, keepdims=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = tf.reduce_sum(mask * log_prob, 1) / (tf.reduce_sum(mask, 1)+single_samples)\n",
    "\n",
    "        # loss\n",
    "        loss = -mean_log_prob_pos * (1 - single_samples)\n",
    "        loss = tf.reduce_sum(loss) / (tf.cast(tf.shape(loss)[0], dtype=tf.float32) - tf.reduce_sum(single_samples))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def class_scores_for_loop(self, embed, input_label, attribute_seen):\n",
    "\n",
    "        n_class_seen = attribute_seen.shape[0]\n",
    "\n",
    "        expand_embed = tf.reshape(tf.tile(tf.expand_dims(embed, 1), [1, n_class_seen, 1]), [embed.shape[0] * n_class_seen, -1])\n",
    "        expand_att = tf.reshape(tf.tile(tf.expand_dims(attribute_seen, 0), [embed.shape[0], 1, 1]), [embed.shape[0] * n_class_seen, -1])\n",
    "        all_scores = tf.reshape(tf.divide(self.comparator_net(tf.concat([expand_embed, expand_att], axis=1)), self.class_temperature), [embed.shape[0], n_class_seen])\n",
    "\n",
    "        score_max = tf.reduce_max(all_scores, axis=1, keepdims=True)\n",
    "        # normalize the scores for stable training\n",
    "        scores_norm = all_scores - score_max\n",
    "\n",
    "        exp_scores = tf.exp(scores_norm)\n",
    "\n",
    "        mask = tf.one_hot(input_label, n_class_seen)\n",
    "\n",
    "        log_scores = scores_norm - tf.math.log(tf.reduce_sum(exp_scores, axis=1, keepdims=True))\n",
    "\n",
    "        cls_loss = -tf.reduce_mean(tf.reduce_sum(mask * log_scores, axis=1) / tf.reduce_sum(mask, axis=1))\n",
    "\n",
    "        return cls_loss\n",
    "\n",
    "    def generate_synthetic_features(self, classes, attribute_data):\n",
    "\n",
    "        nclass = classes.shape[0]\n",
    "        syn_feature = tf.Variable(tf.zeros((self.synthetic_number * nclass, self.visual_size)))\n",
    "        syn_label = tf.Variable(tf.zeros((self.synthetic_number * nclass, 1), dtype=classes.dtype))\n",
    "\n",
    "        for i in range(nclass):\n",
    "\n",
    "            iclass = classes[i]\n",
    "            iclass_att = attribute_data[iclass]\n",
    "\n",
    "            syn_att = tf.repeat(tf.reshape(iclass_att, (1, -1)), self.synthetic_number, axis=0)\n",
    "\n",
    "            syn_noise = tf.random.normal(shape=(self.synthetic_number, self.generator_noise))\n",
    "\n",
    "            output = self.generator_net(tf.concat([syn_noise, syn_att], axis=1))\n",
    "\n",
    "            syn_feature[i * self.synthetic_number:(i+1) * self.synthetic_number, :].assign(output)\n",
    "            syn_label[i * self.synthetic_number:(i+1) * self.synthetic_number, :].assign(tf.fill((self.synthetic_number, 1), iclass))\n",
    "\n",
    "        return syn_feature, syn_label\n",
    "\n",
    "    def train_step(self, real_features, attribute_data, labels, attribute_seen):\n",
    "\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "\n",
    "        d_loss_tracker = keras.metrics.Mean()\n",
    "\n",
    "        for i in range(self.discriminator_iterations):\n",
    "\n",
    "            # Get the latent vector\n",
    "            noise_data = tf.random.normal(shape=(batch_size, self.generator_noise))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                embed_real, z_real = self.embedding_net(real_features)\n",
    "\n",
    "                real_ins_contras_loss = self.contrastive_criterion(labels, z_real)\n",
    "                cls_loss_real = self.class_scores_for_loop(embed_real, labels, attribute_seen)\n",
    "\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_features = self.generator_net(tf.concat([noise_data, attribute_data], axis=1))\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator_net(tf.concat([fake_features, attribute_data], axis=1))\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator_net(tf.concat([real_features, attribute_data], axis=1))\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_features, fake_features, attribute_data)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight + real_ins_contras_loss + cls_loss_real\n",
    "\n",
    "            trainable_variables = self.discriminator_net.trainable_variables + self.embedding_net.trainable_variables + self.comparator_net.trainable_variables\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, trainable_variables)\n",
    "\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.discriminator_optimizer.apply_gradients(zip(d_gradient, trainable_variables))\n",
    "\n",
    "            d_loss_tracker.update_state(d_loss)\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        noise_data = tf.random.normal(shape=(batch_size, self.generator_noise))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            embed_real, z_real = self.embedding_net(real_features)\n",
    "\n",
    "            embed_fake, z_fake = self.embedding_net(fake_features)\n",
    "\n",
    "            fake_ins_contras_loss = self.contrastive_criterion(tf.concat([labels, labels], axis=0), tf.concat([z_fake, z_real], axis=0))\n",
    "            cls_loss_fake = self.class_scores_for_loop(embed_fake, labels, attribute_seen)\n",
    "\n",
    "            # Generate fake images using the generator\n",
    "            fake_features = self.generator_net(tf.concat([noise_data, attribute_data], axis=1))\n",
    "            # Get the discriminator logits for fake images\n",
    "            fake_logits = self.discriminator_net(tf.concat([fake_features, attribute_data], axis=1))\n",
    "            # Calculate the generator loss\n",
    "            G_cost = -tf.reduce_mean(fake_logits)\n",
    "\n",
    "            errG = G_cost + self.instance_weight * fake_ins_contras_loss + self.class_weight * cls_loss_fake\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(errG, self.generator_net.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.generator_optimizer.apply_gradients(zip(gen_gradient, self.generator_net.trainable_variables))\n",
    "\n",
    "        return d_loss_tracker.result(), errG, cls_loss_fake, cls_loss_real, fake_ins_contras_loss, real_ins_contras_loss\n",
    "\n",
    "    def fit(self, dataset):\n",
    "\n",
    "        train_features = dataset.train_features()\n",
    "        train_attributes = dataset.train_attributes()\n",
    "        train_labels = dataset.train_labels()\n",
    "\n",
    "        unseen_classes = tf.constant(dataset.unseen_classes())\n",
    "        seen_classes = tf.constant(dataset.seen_classes())\n",
    "\n",
    "        attributes = tf.constant(dataset.attributes())\n",
    "\n",
    "        train_feat_ds = tf.data.Dataset.from_tensor_slices(train_features)\n",
    "        train_feat_ds = train_feat_ds.shuffle(buffer_size=train_features.shape[0], seed=seed).batch(batch_size)\n",
    "\n",
    "        train_att_ds = tf.data.Dataset.from_tensor_slices(train_attributes)\n",
    "        train_att_ds = train_att_ds.shuffle(buffer_size=train_attributes.shape[0], seed=seed).batch(batch_size)\n",
    "\n",
    "        train_label_ds = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "        train_label_ds = train_label_ds.shuffle(buffer_size=train_labels.shape[0], seed=seed).batch(batch_size)\n",
    "\n",
    "        attribute_seen = tf.constant(ds.attribute_seen())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            epoch_start = time.time()\n",
    "\n",
    "            att_it = train_att_ds.__iter__()\n",
    "            label_it = train_label_ds.__iter__()\n",
    "            \n",
    "            #att_it = tf.compat.v1.data.make_one_shot_iterator(train_att_ds)\n",
    "            #label_it = tf.compat.v1.data.make_one_shot_iterator(train_label_ds)\n",
    "            \n",
    "            d_loss_tracker = keras.metrics.Mean()\n",
    "            g_loss_tracker = keras.metrics.Mean()\n",
    "\n",
    "            for step, train_feat in enumerate(train_feat_ds):\n",
    "\n",
    "                train_att = att_it.next()\n",
    "                train_label = label_it.next()\n",
    "\n",
    "                train_label = map_label(train_label, seen_classes)\n",
    "\n",
    "                d_loss, g_loss, cls_loss_fake, cls_loss_real, fake_ins_contras_loss, real_ins_contras_loss = ce_gzsl.train_step(train_feat, train_att, train_label, attribute_seen)\n",
    "\n",
    "                d_loss_tracker.update_state(d_loss)\n",
    "                g_loss_tracker.update_state(g_loss)\n",
    "\n",
    "            logging.info(\"main epoch {} - d_loss {:.4f} - g_loss {:.4f} - time: {:.4f}\".format(epoch, d_loss_tracker.result(), g_loss_tracker.result(), time.time() - epoch_start))\n",
    "\n",
    "            # classification\n",
    "\n",
    "            cls_start = time.time()\n",
    "\n",
    "            if self.gzsl:\n",
    "\n",
    "                syn_feature, syn_label = self.generate_synthetic_features(unseen_classes, attributes)\n",
    "\n",
    "                train_x = tf.concat([train_features, syn_feature], axis=0)\n",
    "                train_y = tf.concat([train_labels.reshape(-1, 1), syn_label], axis=0)\n",
    "                num_classes = tf.size(unseen_classes) + tf.size(seen_classes)\n",
    "\n",
    "                cls = Classifier(train_x, train_y, self.embedding_net, seed, num_classes, 25,\n",
    "                                 self.synthetic_number, self.visual_size, cls_lr, beta1, beta2, dataset)\n",
    "\n",
    "                acc_seen, acc_unseen, acc_h = cls.fit()\n",
    "\n",
    "                logging.info('best acc: seen {:.4f} - unseen {:.4f} - H {:.4f} - time {:.4f}'.format(acc_seen, acc_unseen, acc_h, time.time() - cls_start))\n",
    "\n",
    "            else:\n",
    "                syn_feature, syn_label = self.generate_synthetic_features(unseen_classes, attributes)\n",
    "                labels = map_label(syn_label, unseen_classes)\n",
    "                num_classes = tf.size(unseen_classes)\n",
    "\n",
    "                cls = Classifier(syn_feature, labels, self.embedding_net, seed, num_classes, 100,\n",
    "                                 self.synthetic_number, self.visual_size, cls_lr, beta1, beta2, dataset)\n",
    "\n",
    "                acc = cls.fit_zsl()\n",
    "\n",
    "                logging.info('best acc: {:.4f} - time {:.4f}'.format(acc, time.time() - cls_start))\n",
    "\n",
    "            if (epoch + 1) % checkpoint_epochs == 0:\n",
    "                logging.info(\"saving checkpoint: {}\".format(exp_path))\n",
    "                np.save(os.path.join(exp_path, \"syn_feature.npy\"), syn_feature.numpy())\n",
    "                np.save(os.path.join(exp_path, \"syn_label.npy\"), syn_label.numpy())\n",
    "                self.generator_net.save(os.path.join(exp_path, \"generator.h5\"))\n",
    "                self.discriminator_net.save(os.path.join(exp_path, \"discriminator.h5\"))\n",
    "                self.comparator_net.save(os.path.join(exp_path, \"comparator.h5\"))\n",
    "                self.embedding_net.save(os.path.join(exp_path, \"embedding.h5\"))\n",
    "\n",
    "\n",
    "validation = False\n",
    "preprocessing = False\n",
    "\n",
    "exp_local_path = \"<local_path>\"\n",
    "exp_remote_path = \"E:/Sushree/Dataset/data/xlsa17/data\"\n",
    "\n",
    "exp_path = os.path.join(exp_remote_path, \"cegzsl_experiments\", str(uuid.uuid4()))\n",
    "os.makedirs(exp_path)\n",
    "\n",
    "data_local_path = \"xlsa17\"\n",
    "data_remote_path = \"E:/Sushree/Dataset/data/xlsa17/data\"\n",
    "\n",
    "ds = Dataset(data_remote_path)\n",
    "ds.read(\"AWA2\", preprocessing=preprocessing, validation=validation)\n",
    "\n",
    "args = {\"visual_size\": ds.feature_size(),\n",
    "        \"attribute_size\": ds.attribute_size(),\n",
    "        \"embedding_size\": 512,\n",
    "        \"embedding_hidden\": 2048,\n",
    "        \"comparator_hidden\": 2048,\n",
    "        \"generator_hidden\": 4096,\n",
    "        \"generator_noise\": 1024,\n",
    "        \"discriminator_hidden\": 4096,\n",
    "        \"discriminator_iterations\": 5,\n",
    "        \"instance_weight\": 0.001,\n",
    "        \"class_weight\": 0.001,\n",
    "        \"instance_temperature\": 0.1,\n",
    "        \"class_temperature\": 0.1,\n",
    "        \"gp_weight\": 10.0,\n",
    "        \"gzsl\": True,\n",
    "        \"synthetic_number\": 100}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c4380b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2048)]            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              4196352   \n",
      "                                                                 \n",
      " embed_h (ReLU)              (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " embed_z (Lambda)            (None, 512)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,245,440\n",
      "Trainable params: 5,245,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"comparator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2133)]            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2048)              4370432   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 2048)              0         \n",
      "                                                                 \n",
      " comp_out (Dense)            (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,372,481\n",
      "Trainable params: 4,372,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1109)]            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              4546560   \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " gen_out (ReLU)              (None, 2048)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,937,216\n",
      "Trainable params: 12,937,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 2133)]            0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4096)              8740864   \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4096)              0         \n",
      "                                                                 \n",
      " disc_out (Dense)            (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,744,961\n",
      "Trainable params: 8,744,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:main epoch 0 - d_loss -2.1080 - g_loss -7.3806 - time: 27.6669\n",
      "INFO:root:best acc: seen 0.9127 - unseen 0.0023 - H 0.0046 - time 67.0521\n",
      "INFO:root:main epoch 1 - d_loss -0.5279 - g_loss -8.2227 - time: 27.9398\n",
      "INFO:root:best acc: seen 0.9180 - unseen 0.0009 - H 0.0019 - time 67.3670\n",
      "INFO:root:main epoch 2 - d_loss -2.9393 - g_loss -2.6960 - time: 27.8612\n",
      "INFO:root:best acc: seen 0.9194 - unseen 0.0007 - H 0.0015 - time 67.6553\n",
      "INFO:root:main epoch 3 - d_loss -8.6487 - g_loss -1.1355 - time: 28.0231\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m ce_gzsl \u001b[38;5;241m=\u001b[39m CE_GZSL(gen_opt, disc_opt, args)\n\u001b[0;32m     35\u001b[0m ce_gzsl\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 36\u001b[0m \u001b[43mce_gzsl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 345\u001b[0m, in \u001b[0;36mCE_GZSL.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    340\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msize(unseen_classes) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39msize(seen_classes)\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m Classifier(train_x, train_y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_net, seed, num_classes, \u001b[38;5;241m25\u001b[39m,\n\u001b[0;32m    343\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynthetic_number, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_size, cls_lr, beta1, beta2, dataset)\n\u001b[1;32m--> 345\u001b[0m     acc_seen, acc_unseen, acc_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest acc: seen \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m - unseen \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m - H \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m - time \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(acc_seen, acc_unseen, acc_h, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m cls_start))\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Sushree_Codes\\ce-gzsl-keras-main\\classifier.py:156\u001b[0m, in \u001b[0;36mClassifier.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m     loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(train_feat, train_label))\n\u001b[0;32m    155\u001b[0m acc_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_gzsl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_seen_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_seen_label, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_classes)\n\u001b[1;32m--> 156\u001b[0m acc_unseen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_gzsl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_unseen_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_unseen_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munseen_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m acc_h \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m acc_seen \u001b[38;5;241m*\u001b[39m acc_unseen) \u001b[38;5;241m/\u001b[39m (acc_seen \u001b[38;5;241m+\u001b[39m acc_unseen)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acc_h \u001b[38;5;241m>\u001b[39m best_acc_h:\n",
      "File \u001b[1;32m~\\Sushree_Codes\\ce-gzsl-keras-main\\classifier.py:178\u001b[0m, in \u001b[0;36mClassifier.val_gzsl\u001b[1;34m(self, features, labels, classes)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_feat_ds):\n\u001b[0;32m    176\u001b[0m     i \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m--> 178\u001b[0m     embed, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_net(embed)\n\u001b[0;32m    181\u001b[0m     predicted_label[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39massign(tf\u001b[38;5;241m.\u001b[39margmax(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_type\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\engine\\training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    555\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[1;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\engine\\functional.py:510\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03m    In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\engine\\functional.py:667\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 667\u001b[0m outputs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mlayer(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    671\u001b[0m     node\u001b[38;5;241m.\u001b[39mflat_output_ids, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)\n\u001b[0;32m    672\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\layers\\core\\dense.py:241\u001b[0m, in \u001b[0;36mDense.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    237\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39membedding_lookup_sparse(\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel, ids, weights, combiner\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Broadcast kernel to inputs.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtensordot(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel, [[rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3714\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[0;32m   3711\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops\u001b[38;5;241m.\u001b[39mbatch_mat_mul_v3(\n\u001b[0;32m   3712\u001b[0m       a, b, adj_x\u001b[38;5;241m=\u001b[39madjoint_a, adj_y\u001b[38;5;241m=\u001b[39madjoint_b, Tout\u001b[38;5;241m=\u001b[39moutput_type, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   3713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3714\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmat_mul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3715\u001b[0m \u001b[43m      \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranspose_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranspose_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6012\u001b[0m, in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   6011\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 6012\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6013\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMatMul\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranspose_a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranspose_b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6014\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtranspose_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   6016\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main training\n",
    "\n",
    "epochs = 2000\n",
    "batch_size = 4096\n",
    "learning_rate = 0.0001\n",
    "lr_decay = 0.99\n",
    "lr_decay_epochs = 100\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "seed = 1985\n",
    "checkpoint_epochs = 100\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "steps_per_epoch = int(np.ceil(ds.train_features().shape[0] / batch_size))\n",
    "lr_decay_steps = lr_decay_epochs * steps_per_epoch\n",
    "\n",
    "# classifier\n",
    "\n",
    "cls_lr = 0.001\n",
    "\n",
    "gen_lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                              decay_steps=lr_decay_steps, decay_rate=lr_decay)\n",
    "\n",
    "disc_lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                               decay_steps=lr_decay_steps, decay_rate=lr_decay)\n",
    "\n",
    "gen_opt = keras.optimizers.Adam(learning_rate=gen_lr_schedule, beta_1=beta1, beta_2=beta2)\n",
    "disc_opt = keras.optimizers.Adam(learning_rate=disc_lr_schedule, beta_1=beta1, beta_2=beta2)\n",
    "\n",
    "ce_gzsl = CE_GZSL(gen_opt, disc_opt, args)\n",
    "\n",
    "ce_gzsl.summary()\n",
    "ce_gzsl.fit(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0b571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23527, 2048)\n",
      "[[0.00575881 0.003829   0.         ... 0.03639079 0.13208508 0.01148699]\n",
      " [0.         0.00507555 0.         ... 0.15675628 0.09070911 0.01425057]\n",
      " [0.00575881 0.003829   0.         ... 0.03639079 0.13208508 0.01148699]\n",
      " ...\n",
      " [0.         0.06587863 0.         ... 0.02108505 0.10218637 0.0332632 ]\n",
      " [0.0084177  0.01262655 0.         ... 0.02104426 0.04138143 0.02316552]\n",
      " [0.03877321 0.15834625 0.         ... 0.1760296  0.07107783 0.30279845]] (23527, 85)\n",
      "[42 21 42 ... 39 18 45] (23527,)\n",
      "[ 6  8 22 23 29 30 33 40 46 49] (10,)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 24 25 26 27\n",
      " 28 31 32 34 35 36 37 38 39 41 42 43 44 45 47 48] (40,)\n"
     ]
    }
   ],
   "source": [
    "train_features = ds.train_features()\n",
    "print(train_features.shape)\n",
    "\n",
    "train_attributes = ds.train_attributes()\n",
    "print(train_attributes, train_attributes.shape)\n",
    "\n",
    "train_labels = ds.train_labels()\n",
    "print(train_labels, train_labels.shape)\n",
    "\n",
    "unseen_classes = ds.unseen_classes()\n",
    "print(unseen_classes, unseen_classes.shape)\n",
    "\n",
    "seen_classes = ds.seen_classes()\n",
    "print(seen_classes, seen_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf53a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.00375358 -0.00375358 -0.00375358 ...  0.00882092  0.03640974\n",
      "   0.03145501]\n",
      " [ 0.12045617  0.00426584  0.         ...  0.17996307  0.0618086\n",
      "   0.03495531]\n",
      " [ 0.26584458  0.20652363  0.         ...  0.05026821  0.04274552\n",
      "   0.04915256]\n",
      " ...\n",
      " [ 0.03877321  0.15834625  0.         ...  0.1760296   0.07107783\n",
      "   0.30279845]\n",
      " [ 0.22516498  0.15266022  0.         ...  0.12733492  0.10009693\n",
      "   0.01771   ]\n",
      " [ 0.19613947  0.1966714   0.         ...  0.01787277  0.06698743\n",
      "   0.258836  ]], shape=(40, 85), dtype=float32) (40, 85)\n"
     ]
    }
   ],
   "source": [
    "seed = 1985\n",
    "batch_size = 4096\n",
    "train_feat_ds = tf.data.Dataset.from_tensor_slices(train_features)\n",
    "train_feat_ds = train_feat_ds.shuffle(buffer_size=train_features.shape[0], seed=seed).batch(batch_size)\n",
    "\n",
    "train_att_ds = tf.data.Dataset.from_tensor_slices(train_attributes)\n",
    "train_att_ds = train_att_ds.shuffle(buffer_size=train_attributes.shape[0], seed=seed).batch(batch_size)\n",
    "\n",
    "train_label_ds = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "train_label_ds = train_label_ds.shuffle(buffer_size=train_labels.shape[0], seed=seed).batch(batch_size)\n",
    "\n",
    "attribute_seen = tf.constant(ds.attribute_seen())\n",
    "print(attribute_seen, attribute_seen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88fc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb234116",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "INFO:root:best acc: seen 0.8974 - unseen 0.1791 - H 0.2986 - time 74.9681\n",
    "INFO:root:main epoch 1782 - d_loss -3.9678 - g_loss -45.5081 - time: 29.2714"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
