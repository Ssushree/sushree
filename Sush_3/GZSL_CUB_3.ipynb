{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7ebbbae",
   "metadata": {},
   "source": [
    "GZSL_CUB_3: Experiments with New attributes (New attribute length = 312*embedding length, then transform it to length 512)\n",
    "\tStep 1 - Class wise continous attributes are extracted (for CUB: 200 categories, and each category has attribute vectors of length 312\n",
    "\tStep 2 - Word vectors are extracted for each semantic attribute using pretrained w2v model \n",
    "                 New attributes are formed by aggregating the continous attribute values and word vectors\n",
    "\tStep 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n",
    "\tStep 4 - New attribute vectors are transformed into a lower dimensional space using 'model0'\n",
    "\tStep 5 - Define 'model2' for attribute to class label mapping\n",
    "\t\t Define 'model1' for visual feature to class label mapping\n",
    "\t\t Train 'model2' and 'model1' through the iterative process\n",
    "\tStep 6 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bb0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'CUB'\n",
    "path = 'E:/Sushree/Dataset/data/xlsa17/data/'\n",
    "\n",
    "res101 = scipy.io.loadmat(path + dataset + '/res101.mat')\n",
    "att_splits = scipy.io.loadmat(path + dataset + '/att_splits.mat')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "905b6a30",
   "metadata": {},
   "source": [
    "Step 1 - Class wise continous attributes are extracted (for CUB: 200 categories, and each category has attribute vectors of length 312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6082508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 200)\n",
      "[[0.0106384  0.0106384  0.00709227 ... 0.00918617 0.02526198 0.02066889]\n",
      " [0.         0.01133243 0.00944369 ... 0.00266542 0.02132333 0.05863916]\n",
      " [0.         0.         0.00742474 ... 0.         0.00885258 0.01770516]\n",
      " ...\n",
      " [0.         0.00334966 0.         ... 0.00556558 0.         0.15027069]\n",
      " [0.         0.11184146 0.         ... 0.08207164 0.05836206 0.01823814]\n",
      " [0.04378019 0.02814441 0.         ... 0.06022509 0.07695428 0.06189801]] (200, 312)\n"
     ]
    }
   ],
   "source": [
    "signature = att_splits['att']\n",
    "#signature = att_splits['original_att']\n",
    "#signature = signature/100\n",
    "print(signature.shape) #(312, 200)\n",
    "\n",
    "attribute = signature.transpose()\n",
    "attribute[attribute<0] = 0\n",
    "print(attribute, attribute.shape)#(200, 312)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e0b398a",
   "metadata": {},
   "source": [
    "Step 2 - Word vectors are extracted for each semantic attribute using pretrained w2v model \n",
    "                 New attributes are formed by aggregating the continous attribute values and word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab0196a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n",
      "Done replacing OOD words\n",
      "Done preprocessing attribute des\n",
      "bill shape curved \n",
      "bill shape dagger\n",
      "bill shape hooked\n",
      "bill shape needle\n",
      "bill shape hooked seabird\n",
      "bill shape broad\n",
      "bill shape all purpose\n",
      "bill shape cone\n",
      "bill shape specialized\n",
      "wing color blue\n",
      "wing color brown\n",
      "wing color iridescent\n",
      "wing color purple\n",
      "wing color rufous\n",
      "wing color gray\n",
      "wing color yellow\n",
      "wing color olive\n",
      "wing color green\n",
      "wing color pink\n",
      "wing color orange\n",
      "wing color black\n",
      "wing color white\n",
      "wing color red\n",
      "wing color buff\n",
      "upper parts color blue\n",
      "upper parts color brown\n",
      "upper parts color iridescent\n",
      "upper parts color purple\n",
      "upper parts color rufous\n",
      "upper parts color gray\n",
      "upper parts color yellow\n",
      "upper parts color olive\n",
      "upper parts color green\n",
      "upper parts color pink\n",
      "upper parts color orange\n",
      "upper parts color black\n",
      "upper parts color white\n",
      "upper parts color red\n",
      "upper parts color buff\n",
      "underparts color blue\n",
      "underparts color brown\n",
      "underparts color iridescent\n",
      "underparts color purple\n",
      "underparts color rufous\n",
      "underparts color gray\n",
      "underparts color yellow\n",
      "underparts color olive\n",
      "underparts color green\n",
      "underparts color pink\n",
      "underparts color orange\n",
      "underparts color black\n",
      "underparts color white\n",
      "underparts color red\n",
      "underparts color buff\n",
      "breast pattern solid\n",
      "breast pattern spotted\n",
      "breast pattern striped\n",
      "breast pattern multi colored\n",
      "back color blue\n",
      "back color brown\n",
      "back color iridescent\n",
      "back color purple\n",
      "back color rufous\n",
      "back color gray\n",
      "back color yellow\n",
      "back color olive\n",
      "back color green\n",
      "back color pink\n",
      "back color orange\n",
      "back color black\n",
      "back color white\n",
      "back color red\n",
      "back color buff\n",
      "tail shape forked tail\n",
      "tail shape rounded tail\n",
      "tail shape notched tail\n",
      "tail shape fan shaped tail\n",
      "tail shape pointed tail\n",
      "tail shape squared tail\n",
      "upper tail color blue\n",
      "upper tail color brown\n",
      "upper tail color iridescent\n",
      "upper tail color purple\n",
      "upper tail color rufous\n",
      "upper tail color gray\n",
      "upper tail color yellow\n",
      "upper tail color olive\n",
      "upper tail color green\n",
      "upper tail color pink\n",
      "upper tail color orange\n",
      "upper tail color black\n",
      "upper tail color white\n",
      "upper tail color red\n",
      "upper tail color buff\n",
      "head pattern spotted\n",
      "head pattern malar\n",
      "head pattern crested\n",
      "head pattern masked\n",
      "head pattern unique pattern\n",
      "head pattern eyebrow\n",
      "head pattern eye ring\n",
      "head pattern plain\n",
      "head pattern eyeline\n",
      "head pattern striped\n",
      "head pattern capped\n",
      "breast color blue\n",
      "breast color brown\n",
      "breast color iridescent\n",
      "breast color purple\n",
      "breast color rufous\n",
      "breast color gray\n",
      "breast color yellow\n",
      "breast color olive\n",
      "breast color green\n",
      "breast color pink\n",
      "breast color orange\n",
      "breast color black\n",
      "breast color white\n",
      "breast color red\n",
      "breast color buff\n",
      "throat color blue\n",
      "throat color brown\n",
      "throat color iridescent\n",
      "throat color purple\n",
      "throat color rufous\n",
      "throat color gray\n",
      "throat color yellow\n",
      "throat color olive\n",
      "throat color green\n",
      "throat color pink\n",
      "throat color orange\n",
      "throat color black\n",
      "throat color white\n",
      "throat color red\n",
      "throat color buff\n",
      "eye color blue\n",
      "eye color brown\n",
      "eye color purple\n",
      "eye color rufous\n",
      "eye color gray\n",
      "eye color yellow\n",
      "eye color olive\n",
      "eye color green\n",
      "eye color pink\n",
      "eye color orange\n",
      "eye color black\n",
      "eye color white\n",
      "eye color red\n",
      "eye color buff\n",
      "bill length about the same as head\n",
      "bill length longer than head\n",
      "bill length shorter than head\n",
      "forehead color blue\n",
      "forehead color brown\n",
      "forehead color iridescent\n",
      "forehead color purple\n",
      "forehead color rufous\n",
      "forehead color gray\n",
      "forehead color yellow\n",
      "forehead color olive\n",
      "forehead color green\n",
      "forehead color pink\n",
      "forehead color orange\n",
      "forehead color black\n",
      "forehead color white\n",
      "forehead color red\n",
      "forehead color buff\n",
      "under tail color blue\n",
      "under tail color brown\n",
      "under tail color iridescent\n",
      "under tail color purple\n",
      "under tail color rufous\n",
      "under tail color gray\n",
      "under tail color yellow\n",
      "under tail color olive\n",
      "under tail color green\n",
      "under tail color pink\n",
      "under tail color orange\n",
      "under tail color black\n",
      "under tail color white\n",
      "under tail color red\n",
      "under tail color buff\n",
      "nape color blue\n",
      "nape color brown\n",
      "nape color iridescent\n",
      "nape color purple\n",
      "nape color rufous\n",
      "nape color gray\n",
      "nape color yellow\n",
      "nape color olive\n",
      "nape color green\n",
      "nape color pink\n",
      "nape color orange\n",
      "nape color black\n",
      "nape color white\n",
      "nape color red\n",
      "nape color buff\n",
      "belly color blue\n",
      "belly color brown\n",
      "belly color iridescent\n",
      "belly color purple\n",
      "belly color rufous\n",
      "belly color gray\n",
      "belly color yellow\n",
      "belly color olive\n",
      "belly color green\n",
      "belly color pink\n",
      "belly color orange\n",
      "belly color black\n",
      "belly color white\n",
      "belly color red\n",
      "belly color buff\n",
      "wing shape rounded wings\n",
      "wing shape pointed wings\n",
      "wing shape broad wings\n",
      "wing shape tapered wings\n",
      "wing shape long wings\n",
      "size large \n",
      "size small \n",
      "size very large \n",
      "size medium \n",
      "size very small \n",
      "shape upright perching water like\n",
      "shape chicken like marsh\n",
      "shape long legged like\n",
      "shape duck like\n",
      "shape owl like\n",
      "shape gull like\n",
      "shape hummingbird like\n",
      "shape pigeon like\n",
      "shape tree clinging like\n",
      "shape hawk like\n",
      "shape sandpiper like\n",
      "shape upland ground like\n",
      "shape swallow like\n",
      "shape perching like\n",
      "back pattern solid\n",
      "back pattern spotted\n",
      "back pattern striped\n",
      "back pattern multi colored\n",
      "tail pattern solid\n",
      "tail pattern spotted\n",
      "tail pattern striped\n",
      "tail pattern multi colored\n",
      "belly pattern solid\n",
      "belly pattern spotted\n",
      "belly pattern striped\n",
      "belly pattern multi colored\n",
      "primary color blue\n",
      "primary color brown\n",
      "primary color iridescent\n",
      "primary color purple\n",
      "primary color rufous\n",
      "primary color gray\n",
      "primary color yellow\n",
      "primary color olive\n",
      "primary color green\n",
      "primary color pink\n",
      "primary color orange\n",
      "primary color black\n",
      "primary color white\n",
      "primary color red\n",
      "primary color buff\n",
      "leg color blue\n",
      "leg color brown\n",
      "leg color iridescent\n",
      "leg color purple\n",
      "leg color rufous\n",
      "leg color gray\n",
      "leg color yellow\n",
      "leg color olive\n",
      "leg color green\n",
      "leg color pink\n",
      "leg color orange\n",
      "leg color black\n",
      "leg color white\n",
      "leg color red\n",
      "leg color buff\n",
      "bill color blue\n",
      "bill color brown\n",
      "bill color iridescent\n",
      "bill color purple\n",
      "bill color rufous\n",
      "bill color gray\n",
      "bill color yellow\n",
      "bill color olive\n",
      "bill color green\n",
      "bill color pink\n",
      "bill color orange\n",
      "bill color black\n",
      "bill color white\n",
      "bill color red\n",
      "bill color buff\n",
      "crown color blue\n",
      "crown color brown\n",
      "crown color iridescent\n",
      "crown color purple\n",
      "crown color rufous\n",
      "crown color gray\n",
      "crown color yellow\n",
      "crown color olive\n",
      "crown color green\n",
      "crown color pink\n",
      "crown color orange\n",
      "crown color black\n",
      "crown color white\n",
      "crown color red\n",
      "crown color buff\n",
      "wing pattern solid\n",
      "wing pattern spotted\n",
      "wing pattern striped\n",
      "wing pattern multi colored\n",
      "counter_err  0\n",
      "[[-0.05729167  0.16105143 -0.05598958 ... -0.09733073  0.05643717\n",
      "  -0.03955078]\n",
      " [ 0.04947917  0.24023438  0.03629557 ... -0.10221354  0.07727051\n",
      "   0.18522135]\n",
      " [ 0.01708984  0.16105143 -0.11083984 ... -0.10449219  0.13700358\n",
      "  -0.07291667]\n",
      " ...\n",
      " [ 0.05723063  0.14941406 -0.14672852 ... -0.13553874 -0.06761678\n",
      "   0.14355469]\n",
      " [-0.00836182  0.08723958 -0.10310872 ... -0.08117676 -0.05916341\n",
      "   0.12679036]\n",
      " [-0.00526428  0.03051758 -0.05059814 ... -0.04852295 -0.02861023\n",
      "  -0.02587891]] (312, 300)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "print('Load pretrain w2v model')\n",
    "\n",
    "model_name = 'word2vec-google-news-300'#best model\n",
    "#model_name = 'fasttext-wiki-news-subwords-300'\n",
    "\n",
    "model = api.load(model_name)\n",
    "\n",
    "dim_w2v = 300\n",
    "\n",
    "#For CUB\n",
    "replace_word = [('spatulate','broad'),('upperparts','upper parts'),('grey','gray'), ('eyering', 'eye ring')] # for CUB\n",
    "\n",
    "\n",
    "path = 'E:/Sushree/Dataset/CUB_200_2011/CUB_200_2011/attributes/attributes.txt'\n",
    "df=pd.read_csv(path,sep=' ',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "#%% filter\n",
    "new_des = [' '.join(i.split('_')) for i in des]\n",
    "new_des = [' '.join(i.split('-')) for i in new_des]\n",
    "new_des = [' '.join(i.split('::')) for i in new_des]\n",
    "new_des = [i.split('(')[0] for i in new_des]\n",
    "new_des = [i[4:] for i in new_des]\n",
    "\n",
    "\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('E:/Sushree/Dataset/CUB_200_2011/CUB_200_2011/attributes/new_des.csv')\n",
    "print('Done preprocessing attribute des')\n",
    "\n",
    "import pickle\n",
    "\n",
    "counter_err = 0\n",
    "\n",
    "all_w2v = []\n",
    "for s in new_des:\n",
    "    print(s)\n",
    "    words = s.split(' ')\n",
    "    if words[-1] == '':     #remove empty element\n",
    "        words = words[:-1]\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    for w in words:\n",
    "        try:\n",
    "            w2v += model[w]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            counter_err += 1\n",
    "    w2v = w2v / len(words)  \n",
    "    all_w2v.append(w2v[np.newaxis,:])\n",
    "    \n",
    "print('counter_err ',counter_err)\n",
    "\n",
    "#%%\n",
    "w2v_att = np.concatenate(all_w2v,axis=0)\n",
    "#pdb.set_trace()\n",
    "#%%\n",
    "print(w2v_att, w2v_att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5facf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00060949  0.00171333 -0.00059564 ... -0.00100292 -0.00059134\n",
      "  -0.00053489]\n",
      " [ 0.          0.          0.         ... -0.00284534 -0.00167768\n",
      "  -0.00151752]\n",
      " [ 0.          0.          0.         ... -0.00085911 -0.00050655\n",
      "  -0.00045819]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.00729158 -0.00429928\n",
      "  -0.00388884]\n",
      " [ 0.          0.          0.         ... -0.00088497 -0.0005218\n",
      "  -0.00047198]\n",
      " [-0.00250824  0.00705086 -0.00245123 ... -0.00300347 -0.00177092\n",
      "  -0.00160185]] (200, 93600)\n"
     ]
    }
   ],
   "source": [
    "attribute_new = np.einsum('ij,jl->ijl', attribute, w2v_att)\n",
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], attribute_new.shape[1]* attribute_new.shape[2]])\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "#attribute_new[attribute_new<0] = 0\n",
    "#print(attribute_new, attribute_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237b819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1     2     4 ... 11724 11725 11726] 11726\n",
      "[    0     3     6 ... 11716 11717 11727] 11727\n",
      "[  178   179   180 ... 11785 11786 11787] 11787\n"
     ]
    }
   ],
   "source": [
    "# total number of instances or images = 11788: ranges from 0 to 11787\n",
    "\n",
    "trainval_loc = np.squeeze(att_splits['trainval_loc']-1) # -1: to consider the overflow problem\n",
    "print(np.unique(trainval_loc), np.max(np.unique(trainval_loc))) # smallest location: 1, largest location 11726\n",
    "\n",
    "test_seen_loc = np.squeeze(att_splits['test_seen_loc']-1)\n",
    "print(np.unique(test_seen_loc), np.max(np.unique(test_seen_loc))) # smallest location: 0, largest location 11727\n",
    "\n",
    "test_unseen_loc = np.squeeze(att_splits['test_unseen_loc']-1)\n",
    "print(np.unique(test_unseen_loc), np.max(np.unique(test_unseen_loc))) # smallest location: 178, largest location 11727\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c148c5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [[151]\n",
      " [151]\n",
      " [151]\n",
      " ...\n",
      " [150]\n",
      " [150]\n",
      " [150]] (11788, 1)\n",
      "unique_labels [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200] (200,)\n",
      "labels_trainval [[197]\n",
      " [198]\n",
      " [ 31]\n",
      " ...\n",
      " [ 65]\n",
      " [147]\n",
      " [ 22]] (7057, 1)\n",
      "unique_labels_trainval [  1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  20\n",
      "  22  23  24  25  26  27  28  30  31  32  33  35  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  51  52  53  54  55  57  58  59  60  61  63\n",
      "  64  65  66  67  70  71  73  74  75  76  77  78  81  82  83  84  85  86\n",
      "  89  90  92  93  94  96  97  99 101 102 103 105 106 107 109 110 111 112\n",
      " 113 114 115 117 118 119 121 123 126 127 128 130 131 132 133 134 135 136\n",
      " 137 138 140 143 144 145 146 147 148 149 151 153 154 155 156 158 161 162\n",
      " 163 164 165 168 169 170 172 173 175 177 178 180 181 183 184 186 188 190\n",
      " 194 196 197 198 199 200] (150,)\n",
      "labels_test_seen [[113]\n",
      " [107]\n",
      " [ 20]\n",
      " ...\n",
      " [136]\n",
      " [ 94]\n",
      " [180]] (1764, 1)\n",
      "unique_labels_test_seen [  1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  20\n",
      "  22  23  24  25  26  27  28  30  31  32  33  35  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  51  52  53  54  55  57  58  59  60  61  63\n",
      "  64  65  66  67  70  71  73  74  75  76  77  78  81  82  83  84  85  86\n",
      "  89  90  92  93  94  96  97  99 101 102 103 105 106 107 109 110 111 112\n",
      " 113 114 115 117 118 119 121 123 126 127 128 130 131 132 133 134 135 136\n",
      " 137 138 140 143 144 145 146 147 148 149 151 153 154 155 156 158 161 162\n",
      " 163 164 165 168 169 170 172 173 175 177 178 180 181 183 184 186 188 190\n",
      " 194 196 197 198 199 200] (150,)\n",
      "labels_test_unseen [[152]\n",
      " [152]\n",
      " [152]\n",
      " ...\n",
      " [150]\n",
      " [150]\n",
      " [150]] (2967, 1)\n",
      "unique_labels_test_unseen [  7  19  21  29  34  36  50  56  62  68  69  72  79  80  87  88  91  95\n",
      "  98 100 104 108 116 120 122 124 125 129 139 141 142 150 152 157 159 160\n",
      " 166 167 171 174 176 179 182 185 187 189 191 192 193 195] (50,)\n",
      "correct number of instances for training, test seen and test unseen categories\n",
      "Number of overlapping classes between trainval and test seen: 150\n",
      "Number of overlapping classes between trainval and test unseen: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 11788 x 1\n",
    "\n",
    "print('unique_labels', np.unique(labels), np.unique(labels).shape)# class labels range from 1 to 200, 200 classes\n",
    "\n",
    "# get the labels for trainval, test seen and test unseen sets\n",
    "\n",
    "labels_trainval = labels[trainval_loc]\n",
    "print('labels_trainval', labels_trainval, labels_trainval.shape)\n",
    "\n",
    "unique_labels_trainval = np.unique(labels_trainval) # labels min:1 max:200\n",
    "print('unique_labels_trainval', unique_labels_trainval, unique_labels_trainval.shape)# 150 classes\n",
    "\n",
    "\n",
    "labels_test_seen = labels[test_seen_loc]\n",
    "print('labels_test_seen', labels_test_seen, labels_test_seen.shape)\n",
    "\n",
    "unique_labels_test_seen = np.unique(labels_test_seen) # labels min:1 max:200\n",
    "print('unique_labels_test_seen', unique_labels_test_seen, unique_labels_test_seen.shape)# 150 classes\n",
    "\n",
    "\n",
    "labels_test_unseen = labels[test_unseen_loc]\n",
    "print('labels_test_unseen', labels_test_unseen, labels_test_unseen.shape)\n",
    "\n",
    "unique_labels_test_unseen = np.unique(labels_test_unseen) # labels min:7 max:195\n",
    "print('unique_labels_test_unseen', unique_labels_test_unseen, unique_labels_test_unseen.shape)# 50 classes\n",
    "\n",
    "\n",
    "if len(labels) == len(labels_trainval) + len(labels_test_seen) + len(labels_test_unseen):\n",
    "    print('correct number of instances for training, test seen and test unseen categories')\n",
    "    \n",
    "print(\"Number of overlapping classes between trainval and test seen:\",len(set(unique_labels_trainval).intersection(set(unique_labels_test_seen))))\n",
    "\n",
    "print(\"Number of overlapping classes between trainval and test unseen:\",len(set(unique_labels_trainval).intersection(set(unique_labels_test_unseen))))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7ff7443",
   "metadata": {},
   "source": [
    "Step 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88acec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for trainval: (7057, 2048)\n",
      "Features for test seen: (1764, 2048)\n",
      "Features for test unseen: (2967, 2048)\n"
     ]
    }
   ],
   "source": [
    "X_features = res101['features']\n",
    "\n",
    "# locations are already subtracted by 1, so they range from 0 to 37321\n",
    "trainval_vec = X_features[:, trainval_loc].transpose()\n",
    "test_seen_vec = X_features[:, test_seen_loc].transpose()\n",
    "test_unseen_vec = X_features[:, test_unseen_loc].transpose()\n",
    "\n",
    "print(\"Features for trainval:\", trainval_vec.shape) #(23527, 2048)\n",
    "print(\"Features for test seen:\", test_seen_vec.shape)# (5882, 2048)\n",
    "print(\"Features for test unseen:\", test_unseen_vec.shape) #(7913, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e89686f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00019368  0.00054445 -0.00018928 ... -0.00740776 -0.00436778\n",
      "  -0.00395081]\n",
      " [ 0.          0.          0.         ... -0.00729158 -0.00429928\n",
      "  -0.00388884]\n",
      " [ 0.          0.          0.         ... -0.0059929  -0.00353355\n",
      "  -0.00319621]\n",
      " ...\n",
      " [-0.00060491  0.00170045 -0.00059116 ... -0.00724668 -0.00427281\n",
      "  -0.0038649 ]\n",
      " [-0.00036547  0.00102737 -0.00035716 ... -0.00074855 -0.00044136\n",
      "  -0.00039922]\n",
      " [-0.000269    0.00075617 -0.00026288 ... -0.00067509 -0.00039805\n",
      "  -0.00036005]] (7057, 93600)\n",
      "[[ 0.          0.          0.         ... -0.00436294 -0.00257249\n",
      "  -0.0023269 ]\n",
      " [-0.0004094   0.00115086 -0.0004001  ... -0.00216366 -0.00127574\n",
      "  -0.00115395]\n",
      " [ 0.          0.          0.         ... -0.00138165 -0.00081465\n",
      "  -0.00073688]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.00276144 -0.00162821\n",
      "  -0.00147277]\n",
      " [ 0.          0.          0.         ... -0.00277034 -0.00163345\n",
      "  -0.00147751]\n",
      " [-0.00234019  0.00657847 -0.00228701 ... -0.00410407 -0.00241985\n",
      "  -0.00218884]] (1764, 93600)\n",
      "[[ 0.          0.          0.         ... -0.00111293 -0.00065621\n",
      "  -0.00059356]\n",
      " [ 0.          0.          0.         ... -0.00111293 -0.00065621\n",
      "  -0.00059356]\n",
      " [ 0.          0.          0.         ... -0.00111293 -0.00065621\n",
      "  -0.00059356]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.00343186 -0.0020235\n",
      "  -0.00183033]\n",
      " [ 0.          0.          0.         ... -0.00343186 -0.0020235\n",
      "  -0.00183033]\n",
      " [ 0.          0.          0.         ... -0.00343186 -0.0020235\n",
      "  -0.00183033]] (2967, 93600)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# attribute is defined for all 50 classes, so we cant use locations directly, instead we have to use labels \n",
    "# that range from 1 to 50, so we have to subtract 1\n",
    "\n",
    "train_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    train_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "print(train_attributes, train_attributes.shape)# (23527, 85)\n",
    "\n",
    "test_seen_attributes = np.zeros((len(test_seen_loc), attribute_new.shape[1]))\n",
    "for i in range(len(test_seen_loc)):\n",
    "    test_seen_attributes[i] = attribute_new[int(labels_test_seen[i])-1]\n",
    "\n",
    "print(test_seen_attributes, test_seen_attributes.shape)# (5882, 85)\n",
    "\n",
    "test_unseen_attributes = np.zeros((len(test_unseen_loc), attribute_new.shape[1]))\n",
    "for i in range(len(test_unseen_loc)):\n",
    "    test_unseen_attributes[i] = attribute_new[int(labels_test_unseen[i])-1]\n",
    "\n",
    "print(test_unseen_attributes, test_unseen_attributes.shape)# (7913, 85)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "563ecf5e",
   "metadata": {},
   "source": [
    "\tStep 4 - New attribute vectors are transformed into a lower dimensional space using 'model0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85741854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93600\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 93600)]           0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 2048)              191694848 \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 512)               1049088   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 192,743,936\n",
      "Trainable params: 0\n",
      "Non-trainable params: 192,743,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attribute_shape_new = attribute_new.shape[1]\n",
    "print(attribute_shape_new)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "import keras.backend as K\n",
    "\n",
    "# define model for attribute transformation\n",
    "\n",
    "inputt = Input(shape = attribute_shape_new)\n",
    "hidden = Dense(2048, name=\"layer1\", activation='linear')(inputt)\n",
    "#norm_layer = Lambda(lambda x: K.l2_normalize(x,axis=1))\n",
    "#batchnorm = BatchNormalization()(hidden)\n",
    "#batchnorm = norm_layer(hidden)\n",
    "#hidden2 = Dense(1024, name=\"layer2\", activation='linear')(hidden)\n",
    "output = Dense(512, name=\"layer3\", activation='linear')(hidden)\n",
    "\n",
    "model0 = Model(inputs = inputt, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model0.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for layer in model0.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6ebf9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 2s 5ms/step\n",
      "[[ 2.3195506e-03 -5.3370558e-03 -9.6181808e-03 ...  3.5584155e-03\n",
      "  -8.5697556e-03 -4.0199617e-03]\n",
      " [-7.2330935e-04 -2.7464139e-03 -1.6381290e-02 ...  7.1416399e-03\n",
      "  -9.3297521e-03 -7.3106862e-03]\n",
      " [-2.9356044e-03  1.5685475e-03 -7.8670308e-04 ... -2.5534038e-03\n",
      "  -3.3916761e-03  6.7819641e-03]\n",
      " ...\n",
      " [ 1.2522667e-02  8.9214742e-03 -5.6080744e-03 ... -1.6709724e-03\n",
      "  -2.1674134e-02  4.8126569e-03]\n",
      " [-5.1458948e-05 -1.9984022e-03 -4.9966378e-03 ... -6.5460317e-03\n",
      "  -3.0209043e-04 -1.8638221e-03]\n",
      " [-1.4840795e-03 -5.8808769e-03  4.1243201e-03 ... -1.4244585e-02\n",
      "  -6.4662313e-03 -6.4614513e-03]] (7057, 512)\n",
      "56/56 [==============================] - 0s 5ms/step\n",
      "[[-2.93593854e-03  7.27663096e-03 -1.11477450e-02 ...  7.91163370e-03\n",
      "  -1.61553547e-02  5.95002621e-03]\n",
      " [-7.00255623e-05  2.56323675e-03 -8.91199894e-03 ... -1.34321675e-02\n",
      "  -1.29799368e-02 -2.51574768e-03]\n",
      " [-3.80986976e-03 -3.15265614e-03 -1.32568553e-03 ... -1.30187813e-02\n",
      "  -6.97336020e-03 -5.14337420e-03]\n",
      " ...\n",
      " [ 1.95119344e-03 -4.85305255e-03 -6.43005967e-03 ...  4.33317386e-04\n",
      "  -4.52570198e-03 -2.56093685e-03]\n",
      " [-3.47244949e-03  9.29237343e-03 -1.74962543e-02 ... -5.83549123e-03\n",
      "  -7.16142822e-03  3.65869747e-03]\n",
      " [-1.32420035e-02  9.43601504e-03 -1.37618312e-03 ... -1.14961034e-02\n",
      "  -1.21379122e-02 -5.04261628e-03]] (1764, 512)\n",
      "93/93 [==============================] - 0s 5ms/step\n",
      "[[-0.0078503  -0.00672257 -0.00095845 ... -0.01312736 -0.00958559\n",
      "  -0.00784985]\n",
      " [-0.0078503  -0.00672257 -0.00095845 ... -0.01312736 -0.00958559\n",
      "  -0.00784985]\n",
      " [-0.0078503  -0.00672257 -0.00095845 ... -0.01312736 -0.00958559\n",
      "  -0.00784985]\n",
      " ...\n",
      " [-0.00467165 -0.001277    0.0048377  ...  0.00462877 -0.01336748\n",
      "   0.00349149]\n",
      " [-0.00467165 -0.001277    0.0048377  ...  0.00462877 -0.01336748\n",
      "   0.00349149]\n",
      " [-0.00467165 -0.001277    0.0048377  ...  0.00462877 -0.01336748\n",
      "   0.00349149]] (2967, 512)\n"
     ]
    }
   ],
   "source": [
    "train_attributes_2 = model0.predict(train_attributes)\n",
    "print(train_attributes_2, train_attributes_2.shape)\n",
    "\n",
    "test_seen_attributes_2 = model0.predict(test_seen_attributes)\n",
    "print(test_seen_attributes_2, test_seen_attributes_2.shape)\n",
    "\n",
    "test_unseen_attributes_2 = model0.predict(test_unseen_attributes)\n",
    "print(test_unseen_attributes_2, test_unseen_attributes_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1457e426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 5ms/step\n",
      "[[-6.41987287e-03 -1.36634381e-03 -2.59550614e-03 ... -7.90772215e-03\n",
      "  -1.73591711e-02 -7.95497093e-03]\n",
      " [ 5.95690496e-03  1.53175183e-03 -5.61270583e-03 ...  1.27345324e-03\n",
      "  -2.10384466e-02 -4.79713501e-03]\n",
      " [ 8.44085030e-03  7.69310445e-03 -6.47634733e-05 ... -5.96217997e-03\n",
      "  -1.14578605e-02 -4.74875886e-03]\n",
      " ...\n",
      " [-7.27483304e-04 -2.74920557e-03 -1.63862482e-02 ...  7.13950256e-03\n",
      "  -9.32896696e-03 -7.30624888e-03]\n",
      " [-1.22343935e-02  7.45091704e-04 -3.15323542e-03 ... -1.04879132e-02\n",
      "  -1.21410389e-03 -3.14513664e-03]\n",
      " [-5.70588699e-03 -2.94431346e-03 -2.43754173e-03 ... -1.13895815e-02\n",
      "  -1.69059215e-03  1.65885792e-03]] (200, 512)\n",
      "[[-6.41987287e-03  5.95690496e-03  8.44085030e-03 ... -7.27483304e-04\n",
      "  -1.22343935e-02 -5.70588699e-03]\n",
      " [-1.36634381e-03  1.53175183e-03  7.69310445e-03 ... -2.74920557e-03\n",
      "   7.45091704e-04 -2.94431346e-03]\n",
      " [-2.59550614e-03 -5.61270583e-03 -6.47634733e-05 ... -1.63862482e-02\n",
      "  -3.15323542e-03 -2.43754173e-03]\n",
      " ...\n",
      " [-7.90772215e-03  1.27345324e-03 -5.96217997e-03 ...  7.13950256e-03\n",
      "  -1.04879132e-02 -1.13895815e-02]\n",
      " [-1.73591711e-02 -2.10384466e-02 -1.14578605e-02 ... -9.32896696e-03\n",
      "  -1.21410389e-03 -1.69059215e-03]\n",
      " [-7.95497093e-03 -4.79713501e-03 -4.74875886e-03 ... -7.30624888e-03\n",
      "  -3.14513664e-03  1.65885792e-03]] (512, 200)\n",
      "Signature for trainval: (512, 150)\n",
      "Signature for test seen: (512, 150)\n",
      "Signature for test unseen: (512, 50)\n"
     ]
    }
   ],
   "source": [
    "# as labels range from 1 to 50, we have subtract 1\n",
    "attribute_2 = model0.predict(attribute_new)\n",
    "print(attribute_2, attribute_2.shape)\n",
    "\n",
    "signature_2 = attribute_2.transpose()\n",
    "print(signature_2, signature_2.shape)#(50, 300)\n",
    "\n",
    "trainval_sig = signature_2[:, (unique_labels_trainval)-1]\n",
    "test_seen_sig = signature_2[:, (unique_labels_test_seen)-1]\n",
    "test_unseen_sig = signature_2[:, (unique_labels_test_unseen)-1]\n",
    "\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)\n",
    "print(\"Signature for test seen:\", test_seen_sig.shape)\n",
    "print(\"Signature for test unseen:\", test_unseen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "581a4c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[146]\n",
      " [147]\n",
      " [ 26]\n",
      " ...\n",
      " [ 55]\n",
      " [115]\n",
      " [ 18]] (7057, 1)\n",
      "[[ 90]\n",
      " [ 85]\n",
      " [ 17]\n",
      " ...\n",
      " [107]\n",
      " [ 76]\n",
      " [137]] (1764, 1)\n",
      "[[32]\n",
      " [32]\n",
      " [32]\n",
      " ...\n",
      " [31]\n",
      " [31]\n",
      " [31]] (2967, 1)\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149] (150,)\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149] (150,)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49] (50,)\n"
     ]
    }
   ],
   "source": [
    "# by doing this modification, we are changing the range of trainval and test seen labels from 0 to 39 \n",
    "# and test unseen labels from 0 to 9\n",
    "\n",
    "k = 0\n",
    "new_labels_trainval = np.zeros((len(labels_trainval), 1), dtype = 'int')\n",
    "for labels in unique_labels_trainval:\n",
    "    new_labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "    \n",
    "print(new_labels_trainval, new_labels_trainval.shape)#(23527, 1)\n",
    "\n",
    "l = 0\n",
    "new_labels_test_seen = np.zeros((len(labels_test_seen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_seen:\n",
    "    new_labels_test_seen[labels_test_seen == labels] = l\n",
    "    l = l+1\n",
    "    \n",
    "print(new_labels_test_seen, new_labels_test_seen.shape)# (5882, 1)\n",
    "\n",
    "m = 0\n",
    "new_labels_test_unseen = np.zeros((len(labels_test_unseen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_unseen:\n",
    "    new_labels_test_unseen[labels_test_unseen == labels] = m\n",
    "    m = m+1  \n",
    "\n",
    "print(new_labels_test_unseen, new_labels_test_unseen.shape) #  (7913, 1)  \n",
    "\n",
    "\n",
    "print(np.unique(new_labels_trainval), np.unique(new_labels_trainval).shape)\n",
    "\n",
    "print(np.unique(new_labels_test_seen), np.unique(new_labels_test_seen).shape)\n",
    "\n",
    "print(np.unique(new_labels_test_unseen), np.unique(new_labels_test_unseen).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee1341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7057\n",
      "150\n",
      "1764\n",
      "150\n",
      "2967\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#params for trainval and test set\n",
    "m_trainval = new_labels_trainval.shape[0]# number of instances in training set: 23527\n",
    "print(m_trainval)\n",
    "\n",
    "z_trainval = len(unique_labels_trainval)# number of classes in training set: 40\n",
    "print(z_trainval)\n",
    "\n",
    "\n",
    "n_test_seen = new_labels_test_seen.shape[0]# 5882\n",
    "print(n_test_seen)\n",
    "\n",
    "z1_test_seen = len(unique_labels_test_seen)# 40\n",
    "print(z1_test_seen)\n",
    "\n",
    "\n",
    "n_test_unseen = new_labels_test_unseen.shape[0]# 7913\n",
    "print(n_test_unseen)\n",
    "\n",
    "z1_test_unseen = len(unique_labels_test_unseen)# 10\n",
    "print(z1_test_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "845b8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (7057, 150)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "gt_trainval = to_categorical(new_labels_trainval, z_trainval)\n",
    "\n",
    "print(gt_trainval, gt_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08067319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "512\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "input1_shape = trainval_vec.shape[1]\n",
    "print(input1_shape)\n",
    "\n",
    "attribute_shape = trainval_sig.shape[0]\n",
    "print(attribute_shape)\n",
    "\n",
    "output_shape = z_trainval\n",
    "print(output_shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b45d9d5",
   "metadata": {},
   "source": [
    "\tStep 5 - Define 'model2' for attribute to class label mapping\n",
    "\t\t Define 'model1' for visual feature to class label mapping\n",
    "\t\t Train 'model2' and 'model1' through the iterative process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96792076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 1, 512)]       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 150)               76950     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,950\n",
      "Trainable params: 76,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "\n",
    "# define model2 for attribute to class label mapping\n",
    "\n",
    "input2 = Input(shape = (1, 1, attribute_shape))\n",
    "flat = Flatten()(input2)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model2 = Model(inputs = input2, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model2.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b285876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " intermediate (Conv1D)       (None, 1, 1, 512)         1049088   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 150)               76950     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,126,038\n",
      "Trainable params: 1,126,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model1 for resnet feature to class label mapping\n",
    "\n",
    "input1 = Input(shape = (1,1, input1_shape))\n",
    "#inter_pre = Dense(512, name=\"intermediate_previous\", activation='relu')(input1)\n",
    "inter = Conv1D(attribute_shape, kernel_size = 1, name = \"intermediate\", activation = 'linear')(input1)\n",
    "flat = Flatten()(inter)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model1 = Model(inputs = input1, outputs = output)\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model1.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fd4ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7057, 1, 1, 2048)\n",
      "(7057, 1, 1, 512)\n",
      "(7057, 150)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = np.reshape(trainval_vec, [trainval_vec.shape[0], 1, 1, trainval_vec.shape[1]])\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = np.reshape(train_attributes_2, [train_attributes_2.shape[0], 1, 1, train_attributes_2.shape[1]])\n",
    "print(trainval_input2.shape)\n",
    "\n",
    "trainval_output = gt_trainval\n",
    "print(trainval_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cec915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 8\n",
    "from sklearn.model_selection import train_test_split    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7be0da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(trainval_input1, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen1 = DataGenerator(X_train1, y_train1, batch_size)   \n",
    "#val_gen1 = DataGenerator(X_val1, y_val1, batch_size)\n",
    "\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(trainval_input2, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen2 = DataGenerator(X_train2, y_train2, batch_size)   \n",
    "#val_gen2 = DataGenerator(X_val2, y_val2, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f527990a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "model 2 is trained: training acc: 0.7386363744735718 , training loss: 3.6938698291778564 , validation acc: 0.7272727489471436 , validation_loss: 3.6873717308044434\n",
      "model 1 is trained: training acc: 0.9275568127632141 , training loss: 13.537097930908203 , validation acc: 0.6420454382896423 , validation_loss: 204.36444091796875\n",
      "micro average\n",
      "seen accuracy: 47.57965841348194 unseen accuracy: 43.66101521627144 harmonic mean: 45.53618703881652\n",
      "macro average\n",
      "seen accuracy: 47.22222222222222 unseen accuracy: 43.714189416919446 harmonic mean: 45.40054153668756\n",
      "best accuracy micro seen accuracy: 47.57965841348194 unseen accuracy: 43.66101521627144 harmonic mean: 45.53618703881652\n",
      "best accuracy macro seen accuracy: 47.22222222222222 unseen accuracy: 43.714189416919446 harmonic mean: 45.40054153668756\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iteration = 1\n",
    "epochs1 = 100\n",
    "epochs2 = 200\n",
    "\n",
    "best_performance_micro = [0, 0, 0]\n",
    "best_performance_macro = [0, 0, 0]\n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/'\n",
    "name = 'model1_conv_CUB_NewattT_512_lin_it1_100eph_adam_cce_16bch_1e-2lr_model2_adam_lr-2_200'\n",
    "\n",
    "\n",
    "for i in range(iteration):\n",
    "    X_train2_it = X_train2[(len(X_train2)//iteration)*i:(len(X_train2)//iteration)*(i+1)]\n",
    "    X_val2_it = X_val2[(len(X_val2)//iteration)*i:(len(X_val2)//iteration)*(i+1)]\n",
    "    y_train2_it = y_train2[(len(y_train2)//iteration)*i:(len(y_train2)//iteration)*(i+1)]\n",
    "    y_val2_it = y_val2[(len(y_val2)//iteration)*i:(len(y_val2)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen2 = DataGenerator(X_train2_it, y_train2_it, batch_size)   \n",
    "    val_gen2 = DataGenerator(X_val2_it, y_val2_it, batch_size)\n",
    "\n",
    "    train_summary2 = model2.fit(train_gen2, epochs = epochs2, verbose = 0, callbacks = None, validation_data = val_gen2, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen2)//batch_size, \n",
    "                              validation_steps = len(val_gen2)//batch_size)\n",
    "\n",
    "    print(\"iteration:\", i)\n",
    "    print('model 2 is trained:', 'training acc:', train_summary2.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary2.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary2.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary2.history['val_loss'][-1])\n",
    "\n",
    "    weights_list2 = model2.get_weights()\n",
    "    #print(weights_list2)\n",
    "\n",
    "    model1.layers[-1].set_weights(weights_list2)\n",
    "\n",
    "    X_train1_it = X_train1[(len(X_train1)//iteration)*i:(len(X_train1)//iteration)*(i+1)]\n",
    "    X_val1_it = X_val1[(len(X_val1)//iteration)*i:(len(X_val1)//iteration)*(i+1)]\n",
    "    y_train1_it = y_train1[(len(y_train1)//iteration)*i:(len(y_train1)//iteration)*(i+1)]\n",
    "    y_val1_it = y_val1[(len(y_val1)//iteration)*i:(len(y_val1)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen1 = DataGenerator(X_train1_it, y_train1_it, batch_size)   \n",
    "    val_gen1 = DataGenerator(X_val1_it, y_val1_it, batch_size)\n",
    "    \n",
    "    for layer in model1.layers[2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    train_summary1 = model1.fit(train_gen1, epochs = epochs1, verbose = 0, callbacks = None, validation_data = val_gen1, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen1)//batch_size, \n",
    "                              validation_steps = len(val_gen1)//batch_size)\n",
    "    \n",
    "    #print(\"iteration:\", i)\n",
    "    print('model 1 is trained:', 'training acc:', train_summary1.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary1.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary1.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary1.history['val_loss'][-1])\n",
    "    \n",
    "    weights_list1 = Model(inputs = model1.input, outputs = model1.layers[-1].output).get_weights()\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, np.matmul(weights_list1[0], weights_list1[2])), test_seen_sig)\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0]), test_seen_sig)\n",
    "    \n",
    "    outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0][0]), test_seen_sig)\n",
    "    preds_seen = np.array([np.argmax(output) for output in outputs_seen])\n",
    "    \n",
    "    cm_seen = confusion_matrix(new_labels_test_seen, preds_seen)\n",
    "    #print(cm)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_seen_micro = cm_seen.astype('float') / cm_seen.sum(axis=1)[:, np.newaxis]\n",
    "    #print(cm)\n",
    "    avg_seen_micro = (sum(cm_seen_micro.diagonal())/len(unique_labels_test_seen))*100\n",
    "\n",
    "    avg_seen_macro = (sum(cm_seen.diagonal())/len(new_labels_test_seen))*100\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0]), test_unseen_sig)\n",
    "    outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0][0]), test_unseen_sig)\n",
    "    \n",
    "    preds_unseen = np.array([np.argmax(output) for output in outputs_unseen])\n",
    "    \n",
    "    cm_unseen = confusion_matrix(new_labels_test_unseen, preds_unseen)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_unseen_micro = cm_unseen.astype('float') / cm_unseen.sum(axis=1)[:, np.newaxis]\n",
    "    avg_unseen_micro = (sum(cm_unseen_micro.diagonal())/len(unique_labels_test_unseen))*100\n",
    "\n",
    "    avg_unseen_macro = (sum(cm_unseen.diagonal())/len(new_labels_test_unseen))*100\n",
    "    \n",
    "    harmonic_micro = (2*avg_seen_micro*avg_unseen_micro) / (avg_seen_micro + avg_unseen_micro)\n",
    "    harmonic_macro = (2*avg_seen_macro*avg_unseen_macro) / (avg_seen_macro + avg_unseen_macro)\n",
    "    \n",
    "    print('micro average')\n",
    "    print('seen accuracy:', avg_seen_micro, 'unseen accuracy:', avg_unseen_micro, 'harmonic mean:', harmonic_micro)\n",
    "    \n",
    "    print('macro average')\n",
    "    print('seen accuracy:', avg_seen_macro, 'unseen accuracy:', avg_unseen_macro, 'harmonic mean:', harmonic_macro)\n",
    "    \n",
    "    if harmonic_micro > best_performance_micro[2]:\n",
    "        best_performance_micro = [avg_seen_micro, avg_unseen_micro, harmonic_micro]\n",
    "        model1.save_weights(save_path + 'bw_micro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    if harmonic_macro > best_performance_macro[2]:\n",
    "        best_performance_macro = [avg_seen_macro, avg_unseen_macro, harmonic_macro]\n",
    "        model1.save_weights(save_path + 'bw_macro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    print('best accuracy micro','seen accuracy:', best_performance_micro[0], 'unseen accuracy:', best_performance_micro[1], 'harmonic mean:', best_performance_micro[2])\n",
    "    print('best accuracy macro', 'seen accuracy:', best_performance_macro[0], 'unseen accuracy:', best_performance_macro[1], 'harmonic mean:', best_performance_macro[2])\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    weights_list3 = model1.get_weights()\n",
    "    model2.set_weights(weights_list3[2:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c05a0619",
   "metadata": {},
   "source": [
    "\tStep 6 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58bc47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (1764, 150)\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 218.9479 - accuracy: 0.5935\n",
      "cce =  6.4814363\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 3.8750 - accuracy: 0.3475\n",
      "cce =  3.8749657\n"
     ]
    }
   ],
   "source": [
    "gt_test_seen = to_categorical(new_labels_test_seen, z1_test_seen)\n",
    "\n",
    "print(gt_test_seen, gt_test_seen.shape)\n",
    "\n",
    "test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "res1 = model1.evaluate(test_seen_vec, gt_test_seen)\n",
    "\n",
    "p1 = model1.predict(test_seen_vec, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p1).numpy())\n",
    "\n",
    "\n",
    "test_seen_attributes_2 = np.reshape(test_seen_attributes_2, [test_seen_attributes_2.shape[0], 1, 1, test_seen_attributes_2.shape[1]])\n",
    "\n",
    "res2 = model2.evaluate(test_seen_attributes_2, gt_test_seen)\n",
    "\n",
    "p2 = model2.predict(test_seen_attributes_2, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p2).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe3280c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.34461115601937\n",
      "47.05215245485306\n",
      "45.3195418613424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_seen_updated = res1[1]*100\n",
    "unseen_accuracy = 43.71\n",
    "h = (2*accuracy_seen_updated*unseen_accuracy) / (accuracy_seen_updated + unseen_accuracy)\n",
    "print(h)\n",
    "\n",
    "\n",
    "accuracy_seen_updated2 = ((res1[1]*100)+(res2[1]*100))/2\n",
    "print(accuracy_seen_updated2)\n",
    "h = (2*accuracy_seen_updated2*unseen_accuracy) / (accuracy_seen_updated2 + unseen_accuracy)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04a7e6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_seen_macro 0.4926457308363527 recall_seen_macro 0.47959992132050955 f1_seen_macro 0.4488304677122623\n",
      "precision_seen_micro 0.4705215419501134 recall_seen_micro 0.4705215419501134 f1_seen_micro 0.4705215419501134\n",
      "precision_unseen_macro 0.4840612047220758 recall_unseen_macro 0.43661015216271437 f1_unseen_macro 0.40438082827177596\n",
      "precision_unseen_micro 0.43714189416919447 recall_unseen_micro 0.43714189416919447 f1_unseen_micro 0.43714189416919447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pp1 = np.array([np.argmax(output) for output in p1])\n",
    "pp2 = np.array([np.argmax(output) for output in p2])\n",
    "\n",
    "seen_macro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'macro')\n",
    "seen_macro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'macro')\n",
    "print('precision_seen_macro', (seen_macro1[0] + seen_macro2[0])/2, 'recall_seen_macro', (seen_macro1[1] + seen_macro2[1])/2, 'f1_seen_macro', (seen_macro1[2] + seen_macro2[2])/2)\n",
    "\n",
    "\n",
    "seen_micro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'micro')\n",
    "seen_micro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'micro')\n",
    "print('precision_seen_micro', (seen_micro1[0] + seen_micro2[0])/2, 'recall_seen_micro', (seen_micro1[1] + seen_micro2[1])/2, 'f1_seen_micro', (seen_micro1[2] + seen_micro2[2])/2)\n",
    "\n",
    "unseen_macro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'macro')\n",
    "unseen_micro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'micro')\n",
    "\n",
    "print('precision_unseen_macro', unseen_macro[0], 'recall_unseen_macro', unseen_macro[1], 'f1_unseen_macro', unseen_macro[2])\n",
    "print('precision_unseen_micro', unseen_micro[0], 'recall_unseen_micro', unseen_micro[1], 'f1_unseen_micro', unseen_micro[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
