{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c8d9d02",
   "metadata": {},
   "source": [
    "Code Description: \n",
    "\n",
    "GZSL_AWA2_5: Experiments with new attributes (w2v, bert, bart, GPT) and latent embedding\n",
    "\tStep 1 - Class wise continous attributes are extracted (for AWA2: 50 categories, and each category has attribute vectors of length 85) \n",
    "\tStep 2 - Word vectors are extracted for each semantic attribute using pretrained language models (w2v, Bert, Bart, GPT) \n",
    "                 New attributes are formed by aggregating the continous attribute values and word vectors\n",
    "\tStep 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n",
    "\tStep 4 - New attribute vectors are transformed into a lower dimensional space using 'model_transform_attribute'\n",
    "\tStep 5 - 'embedding_model' is trained to perform embedding between visual features and attribute vectors (or semantic features)\n",
    "\t         embedding_model_0 is used to extract embedded visual features\n",
    "\t\t embedding_model_1 is used to extract embedded semantic features\n",
    "\tStep 6 - Define 'model2' for attribute to class label mapping\n",
    "\t\t Define 'model1' for visual feature to class label mapping\n",
    "\t\t Train 'model2' and 'model1' through the iterative process\n",
    "\tStep 7 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bb0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'AWA2'\n",
    "path = 'E:/Sushree/Dataset/data/xlsa17/data/'\n",
    "\n",
    "res101 = scipy.io.loadmat(path + dataset + '/res101.mat')\n",
    "att_splits = scipy.io.loadmat(path + dataset + '/att_splits.mat')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e4521c2",
   "metadata": {},
   "source": [
    "Step 1 - Class wise continous attributes are extracted (for AWA2: 50 categories, and each category has attribute vectors of length 85) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6082508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 50)\n",
      "[[0.     0.     0.     ... 0.0235 0.097  0.0838]\n",
      " [0.3925 0.0139 0.     ... 0.5864 0.2014 0.1139]\n",
      " [0.834  0.6479 0.     ... 0.1577 0.1341 0.1542]\n",
      " ...\n",
      " [0.6357 0.431  0.     ... 0.3595 0.2826 0.05  ]\n",
      " [0.5531 0.5546 0.     ... 0.0504 0.1889 0.7299]\n",
      " [0.1022 0.2153 0.2773 ... 0.0396 0.1405 0.3798]] (50, 85)\n"
     ]
    }
   ],
   "source": [
    "signature = att_splits['att']\n",
    "#signature = att_splits['original_att']\n",
    "#signature = signature/100\n",
    "print(signature.shape) #(85, 50)\n",
    "\n",
    "attribute = signature.transpose()\n",
    "attribute[attribute<0] = 0\n",
    "print(attribute, attribute.shape)#(50, 85)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11f3451a",
   "metadata": {},
   "source": [
    "Step 2 - Word vectors are extracted for each semantic attribute using pretrained language models (w2v, Bert, Bart, GPT) \n",
    "         New attributes are formed by aggregating the continous attribute values and word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2293d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "print('Load pretrain w2v model')\n",
    "\n",
    "#model_name = 'word2vec-google-news-300' # length = 300\n",
    "#model = api.load(model_name)\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel, BartTokenizer, TFBartModel, GPT2Tokenizer, TFGPT2Model\n",
    "from transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\n",
    "\n",
    "# Bert model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # length 768\n",
    "#model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased') # 1024\n",
    "model = TFBertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Bart model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-base') # length 768\n",
    "#model = TFBartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-large') # length 1024\n",
    "#model = TFBartModel.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# GPT2 model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # length 768\n",
    "#model = TFGPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# GPT model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')# length 768\n",
    "#model = TFOpenAIGPTModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d5899c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "#model = TFOpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "#encoded_input = tokenizer('sushree', return_tensors='tf')\n",
    "#print(encoded_input)\n",
    "#length = encoded_input.input_ids.shape[1]\n",
    "#print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25be9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer)\n",
    "#out = model(encoded_input).last_hidden_state[:, 2, :]\n",
    "#print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab0196a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done replacing OOD words\n",
      "Done preprocessing attribute des\n",
      "black\n",
      "white\n",
      "blue\n",
      "brown\n",
      "gray\n",
      "orange\n",
      "red\n",
      "yellow\n",
      "patches\n",
      "spots\n",
      "stripes\n",
      "furry\n",
      "hairless\n",
      "tough skin\n",
      "big\n",
      "small\n",
      "bulbous\n",
      "lean\n",
      "flippers\n",
      "hands\n",
      "hooves\n",
      "pads\n",
      "paws\n",
      "long leg\n",
      "long neck\n",
      "tail\n",
      "chew teeth\n",
      "meat teeth\n",
      "buckteeth\n",
      "strain teeth\n",
      "horns\n",
      "claws\n",
      "tusks\n",
      "smelly\n",
      "flys\n",
      "hops\n",
      "swims\n",
      "tunnels\n",
      "walks\n",
      "fast\n",
      "slow\n",
      "strong\n",
      "weak\n",
      "muscle\n",
      "bipedal\n",
      "quadrupedal\n",
      "active\n",
      "inactive\n",
      "nocturnal\n",
      "hibernate\n",
      "agility\n",
      "fish\n",
      "meat\n",
      "plankton\n",
      "vegetation\n",
      "insects\n",
      "forager\n",
      "grazer\n",
      "hunter\n",
      "scavenger\n",
      "skimmer\n",
      "stalker\n",
      "new world\n",
      "old world\n",
      "arctic\n",
      "coastal\n",
      "desert\n",
      "bush\n",
      "plains\n",
      "forest\n",
      "fields\n",
      "jungle\n",
      "mountains\n",
      "ocean\n",
      "ground\n",
      "water\n",
      "tree\n",
      "cave\n",
      "fierce\n",
      "timid\n",
      "smart\n",
      "group\n",
      "solitary\n",
      "nest spot\n",
      "domestic\n",
      "counter  85\n",
      "[[-0.09144888 -0.13553846 -0.12541848 ... -0.02989655 -0.38680694\n",
      "   0.24406666]\n",
      " [-0.29638147  0.14530022  0.20919298 ... -0.13767773 -0.49952391\n",
      "  -0.02497395]\n",
      " [-0.17159468  0.01720696 -0.01027012 ...  0.11106262  0.08405223\n",
      "   0.08059736]\n",
      " ...\n",
      " [-0.36489192  0.06430094 -0.17526677 ...  0.12331312 -0.36982071\n",
      "   0.07205145]\n",
      " [-0.55117345 -0.27137217 -0.57242227 ...  0.39393783  0.25118184\n",
      "   0.23015559]\n",
      " [-0.34205812 -0.0323039   0.13938682 ... -0.19463865 -0.57925606\n",
      "   0.4089973 ]] (85, 1024)\n",
      "[[ 0.          0.          0.         ... -0.00612236 -0.01822051\n",
      "   0.01286502]\n",
      " [-0.01101558 -0.01632644 -0.01510743 ... -0.00680365 -0.02024807\n",
      "   0.01429663]\n",
      " [-0.02431119 -0.03603217 -0.03334183 ... -0.00956699 -0.02847192\n",
      "   0.02010326]\n",
      " ...\n",
      " [-0.02059108 -0.03051851 -0.02823985 ... -0.00344705 -0.01025863\n",
      "   0.00724334]\n",
      " [-0.01793673 -0.02658444 -0.02459951 ... -0.05037949 -0.14993233\n",
      "   0.10586323]\n",
      " [-0.00349297 -0.00517701 -0.00479047 ... -0.02762801 -0.08222259\n",
      "   0.05805518]] (50, 87040)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim_w2v = 1024\n",
    "\n",
    "#%%\n",
    "replace_word = [('newworld','new world'),('oldworld','old world'),('nestspot','nest spot'),('toughskin','tough skin'),\n",
    "                ('longleg','long leg'), ('longneck', 'long neck'), ('chewteeth','chew teeth'),('meatteeth','meat teeth'),('strainteeth','strain teeth'),\n",
    "                ('quadrapedal','quadrupedal')]  # for AWA2\n",
    "\n",
    "\n",
    "#For AWA2\n",
    "path = 'E:/Sushree/Dataset/Animals_with_Attributes2/attribute/predicates.txt'\n",
    "df=pd.read_csv(path,sep='\\t',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('E:/Sushree/Dataset/Animals_with_Attributes2/attribute/new_des.csv')\n",
    "print('Done preprocessing attribute des')\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for word2vec\n",
    "# -----------------------------------------------------------------------------------\n",
    "#counter_err = 0\n",
    "#\n",
    "#all_w2v = []\n",
    "#for s in des:\n",
    "##    print(s)\n",
    "#    words = s.split(' ')\n",
    "#    if words[-1] == '':     #remove empty element\n",
    "#        words = words[:-1]\n",
    "#    w2v = np.zeros(dim_w2v)\n",
    "#    for w in words:\n",
    "#        try:\n",
    "#            w2v += model[w]\n",
    "#        except Exception as e:\n",
    "#            print(e)\n",
    "#            counter_err += 1\n",
    "#    w2v = w2v / len(words)  \n",
    "#    all_w2v.append(w2v[np.newaxis,:])\n",
    "#    \n",
    "#print('counter_err ',counter_err)\n",
    "\n",
    "#w2v_att = np.concatenate(all_w2v,axis=0)\n",
    "#print(w2v_att, w2v_att.shape)\n",
    "\n",
    "# for Bert attributes\n",
    "# -----------------------------------------------------------------------------------\n",
    "counter = 0\n",
    "w2v_att = np.zeros((signature.shape[0], dim_w2v))\n",
    "for s in des:\n",
    "    print(s)\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    encoded_input = tokenizer(s, return_tensors='tf')\n",
    "    length = encoded_input.input_ids.shape[1]\n",
    "    #print(length)\n",
    "    for i in range(length-2): # for Bert, Bart\n",
    "    #for i in range(length): # for GPT2\n",
    "        w2v = w2v + model(encoded_input).last_hidden_state[:, i+1, :] # for Bert, Bart\n",
    "        #w2v = w2v + model(encoded_input).last_hidden_state[:, i, :] # for GPT2\n",
    "        #print(model(encoded_input).last_hidden_state[:, i+1, :][:,1:2])\n",
    "        #print(w2v[:, 1:2])\n",
    "        #print(w2v.shape)\n",
    "\n",
    "    w2v = w2v / (length - 2)# for Bert, Bart\n",
    "    #w2v = w2v / length # for GPT2\n",
    "    #print(w2v[:, 1:2])\n",
    "    w2v_att[counter] = w2v\n",
    "    counter = counter + 1\n",
    "\n",
    "print('counter ',counter)\n",
    "print(w2v_att, w2v_att.shape)\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "attribute_new = np.einsum('ij,jl->ijl', attribute, w2v_att)\n",
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], attribute_new.shape[1]* attribute_new.shape[2]])\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "#attribute_new[attribute_new<0] = 0\n",
    "#print(attribute_new, attribute_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237b819e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4     5 ... 37318 37320 37321] 37321\n",
      "labels [[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " ...\n",
      " [38]\n",
      " [38]\n",
      " [38]] (37322, 1)\n",
      "unique_labels [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50] (50,)\n",
      "labels_trainval [[43]\n",
      " [22]\n",
      " [43]\n",
      " ...\n",
      " [40]\n",
      " [19]\n",
      " [46]] (23527, 1)\n",
      "unique_labels_trainval [ 1  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28\n",
      " 29 32 33 35 36 37 38 39 40 42 43 44 45 46 48 49] (40,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# total number of instances or images = 37322: ranges from 0 to 37321\n",
    "\n",
    "trainval_loc = np.squeeze(att_splits['trainval_loc']-1) # -1: to consider the overflow problem\n",
    "print(np.unique(trainval_loc), np.max(np.unique(trainval_loc))) # smallest location: 2, largest location 37321\n",
    "\n",
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 37322 x 1\n",
    "\n",
    "print('unique_labels', np.unique(labels), np.unique(labels).shape)# class labels range from 1 to 50, 50 classes\n",
    "\n",
    "# get the labels for trainval, test seen and test unseen sets\n",
    "\n",
    "labels_trainval = labels[trainval_loc]\n",
    "print('labels_trainval', labels_trainval, labels_trainval.shape)\n",
    "\n",
    "unique_labels_trainval = np.unique(labels_trainval) # labels min:1 max:49\n",
    "print('unique_labels_trainval', unique_labels_trainval, unique_labels_trainval.shape)# 40 classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0d0ca4a",
   "metadata": {},
   "source": [
    "Step 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88acec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for trainval: (23527, 2048)\n",
      "[[-0.00052664 -0.00078054 -0.00072226 ... -0.00223581 -0.00665391\n",
      "   0.00469815]\n",
      " [ 0.          0.          0.         ... -0.00277371 -0.00825473\n",
      "   0.00582844]\n",
      " [-0.00052664 -0.00078054 -0.00072226 ... -0.00223581 -0.00665391\n",
      "   0.00469815]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.0064743  -0.01926791\n",
      "   0.01360456]\n",
      " [-0.00076979 -0.00114092 -0.00105574 ... -0.00450891 -0.01341877\n",
      "   0.00947463]\n",
      " [-0.00354577 -0.00525526 -0.00486288 ... -0.05893629 -0.17539784\n",
      "   0.12384375]] (23527, 87040)\n"
     ]
    }
   ],
   "source": [
    "X_features = res101['features']\n",
    "\n",
    "# locations are already subtracted by 1, so they range from 0 to 37321\n",
    "trainval_vec = X_features[:, trainval_loc].transpose()\n",
    "print(\"Features for trainval:\", trainval_vec.shape) #(23527, 2048)\n",
    "\n",
    "# attribute is defined for all 50 classes, so we cant use locations directly, instead we have to use labels \n",
    "# that range from 1 to 50, so we have to subtract 1\n",
    "\n",
    "trainval_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    trainval_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "print(trainval_attributes, trainval_attributes.shape)# (23527, 85)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5392e263",
   "metadata": {},
   "source": [
    "Step 4 - New attribute vectors are transformed into a lower dimensional space using 'model_transform_attribute'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc71b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87040\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 87040)]           0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 4096)              356519936 \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 2048)              8390656   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 364,910,592\n",
      "Trainable params: 0\n",
      "Non-trainable params: 364,910,592\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "[[-0.00382511  0.0777265  -0.02586024 ... -0.03016872  0.0993115\n",
      "   0.00647516]\n",
      " [ 0.08858047  0.02310733  0.11753352 ... -0.07380248  0.07652213\n",
      "  -0.05267316]\n",
      " [ 0.07576825 -0.00099139 -0.00923608 ... -0.15335062  0.11195775\n",
      "  -0.06527486]\n",
      " ...\n",
      " [-0.05235996  0.12140516  0.12348951 ... -0.01514845  0.08340997\n",
      "  -0.03685978]\n",
      " [ 0.06548851  0.03580721  0.02371997 ... -0.04905312  0.18431956\n",
      "   0.08915447]\n",
      " [ 0.0079307  -0.00509144  0.01858312 ... -0.13361055  0.05645684\n",
      "  -0.11200373]] (50, 2048)\n",
      "[[-5.34429215e-02  3.39693204e-02  7.03966096e-02 ... -2.37033628e-02\n",
      "   6.95127323e-02 -1.15822613e-01]\n",
      " [-6.41771033e-02  8.96320194e-02  4.24261279e-02 ... -7.03211278e-02\n",
      "   4.07899916e-02 -8.06409642e-02]\n",
      " [-5.34429215e-02  3.39693204e-02  7.03966096e-02 ... -2.37033628e-02\n",
      "   6.95127323e-02 -1.15822613e-01]\n",
      " ...\n",
      " [-8.81885272e-03  5.45321479e-02 -2.49930713e-02 ... -8.95893760e-03\n",
      "   1.20544948e-01  6.02823757e-02]\n",
      " [ 1.07484274e-02  1.38606299e-02  1.74476416e-04 ... -6.74784258e-02\n",
      "   1.76700488e-01  7.77416900e-02]\n",
      " [-4.76189284e-03  7.94992968e-02  6.26793951e-02 ... -8.49233419e-02\n",
      "   6.55361563e-02 -3.95950750e-02]] (23527, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "import keras.backend as K\n",
    "\n",
    "attribute_shape_new = attribute_new.shape[1]\n",
    "print(attribute_shape_new)\n",
    "\n",
    "# define model for attribute transformation\n",
    "\n",
    "inputt = Input(shape = attribute_shape_new)\n",
    "hidden = Dense(4096, name=\"layer1\", activation='linear')(inputt)\n",
    "output = Dense(2048, name=\"layer3\", activation='linear')(hidden)\n",
    "\n",
    "model_transform_attribute = Model(inputs = inputt, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model_transform_attribute.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for layer in model_transform_attribute.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model_transform_attribute.summary()\n",
    "\n",
    "attribute_new = model_transform_attribute.predict(attribute_new)\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "\n",
    "trainval_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    trainval_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "print(trainval_attributes, trainval_attributes.shape)# (23527, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e99b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23527, 1, 1, 2048)\n",
      "(23527, 1, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = np.reshape(trainval_vec, [trainval_vec.shape[0], 1, 1, trainval_vec.shape[1]])\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = np.reshape(trainval_attributes, [trainval_attributes.shape[0], 1, 1, trainval_attributes.shape[1]])\n",
    "print(trainval_input2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d22fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(trainval_input1, trainval_input2, test_size = 0.2, random_state = 42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 16\n",
    "train_gen = DataGenerator(X_train, y_train, batch_size)   \n",
    "val_gen = DataGenerator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be9bd827",
   "metadata": {},
   "source": [
    "Step 5 - 'embedding_model' is trained to perform embedding between visual features and attribute vectors (or semantic features)\n",
    "         embedding_model_0 is used to extract embedded visual features\n",
    "         embedding_model_1 is used to extract embedded semantic features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74a76d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2D)             (None, 1, 1, 2048)        4196352   \n",
      "                                                                 \n",
      " layer2 (Conv2D)             (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      " layer3 (Conv2D)             (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " layer4 (Conv2D)             (None, 1, 1, 2048)        2099200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,443,328\n",
      "Trainable params: 9,443,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "# -----------------------------------------------------------------------------------\n",
    "inputt = Input(shape = (1, 1, trainval_vec.shape[1]))\n",
    "hidden1 = Conv2D(2048, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "output1 = Conv2D(1024, 1, name=\"layer2\", activation='relu')(hidden1)\n",
    "hidden2 = Conv2D(1024, 1, name=\"layer3\", activation='relu')(output1)\n",
    "output2 = Conv2D(trainval_attributes.shape[1], 1, name=\"layer4\", activation='linear')(hidden2)\n",
    "\n",
    "embedding_model = Model(inputs = inputt, outputs = output2)\n",
    "\n",
    "# embedding model variants\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "#inputt = Input(shape = (1, 1, trainval_vec.shape[1]))\n",
    "#hidden1 = Conv2D(1024, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "#output1 = Conv2D(1024, 1, name=\"layer2\", activation='relu')(hidden1)\n",
    "#hidden2 = Conv2D(1024, 1, name=\"layer3\", activation='relu')(output1)\n",
    "#output2 = Conv2D(trainval_attributes.shape[1], 1, name=\"layer4\", activation='linear')(hidden2)\n",
    "\n",
    "#embedding_model = Model(inputs = inputt, outputs = output2)\n",
    "\n",
    "\n",
    "adam = Adam(learning_rate = 0.0001, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "embedding_model.compile(adam, loss = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.AUTO), metrics = ['accuracy'])\n",
    "#embedding_model.compile(adam, loss = cosine_loss, metrics = ['accuracy'])\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c663ebb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#CosineSimilarity is a number between -1 and 1. When it is a negative number between -1 and 0, 0 indicates orthogonality and \n",
    "#values closer to -1 indicate greater similarity. The values closer to 1 indicate greater dissimilarity. \n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/AWA2/BertAttribute/with_LE/Embedding Model/'\n",
    "name = 'embedding_model_Bert2_2048_1024_1024_AWA2_200eph_adam_cos_16bch_0.0001lr'\n",
    "\n",
    "file_path = save_path + 'bw_' + name + '.h5'\n",
    "\n",
    "#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath = file_path,\n",
    "#    monitor = 'val_accuracy',\n",
    "#    mode = 'max',\n",
    "#    save_best_only=True)\n",
    "\n",
    "#train_summary = embedding_model.fit(train_gen, epochs = 200, verbose = 1, callbacks = [model_checkpoint_callback], validation_data = val_gen, \n",
    "#                              shuffle = True, steps_per_epoch = len(train_gen)//batch_size, \n",
    "#                              validation_steps = len(val_gen)//batch_size)\n",
    "\n",
    "embedding_model.load_weights(save_path + 'bw_' + name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34cf8bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2D)             (None, 1, 1, 2048)        4196352   \n",
      "                                                                 \n",
      " layer2 (Conv2D)             (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,294,528\n",
      "Trainable params: 6,294,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "736/736 [==============================] - 1s 678us/step\n",
      "[[[[0.         0.646958   0.5096992  ... 1.2153103  0.66170496\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.06897884 0.         0.8608626  ... 2.8148031  2.0006714\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         1.8988659  ... 2.3388317  2.5795538\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.2842354  2.585671   ... 2.5207908  1.1933193\n",
      "    0.16591284]]]\n",
      "\n",
      "\n",
      " [[[0.         0.36422437 1.8683327  ... 1.1956234  1.0739142\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.2863497  ... 2.068993   1.3655761\n",
      "    0.        ]]]] (23527, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "embedding_model_0 = Model(inputs = embedding_model.input, outputs = embedding_model.layers[-3].output) # for embedding model\n",
    "#embedding_model_0 = Model(inputs = embedding_model.input, outputs = embedding_model.layers[-2].output) # for embedding model variants\n",
    "embedding_model_0.summary()\n",
    "\n",
    "trainval_vec = embedding_model_0.predict(trainval_input1)\n",
    "print(trainval_vec, trainval_vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ffedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputt = Input(shape = (1, 1, trainval_attributes.shape[1]))\n",
    "hidden1 = Conv2DTranspose(1024, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "\n",
    "embedding_model_1 = Model(inputs = inputt, outputs = hidden1)\n",
    "embedding_model_1.summary()\n",
    "\n",
    "w_list = Model(inputs = embedding_model_0.layers[-2].output, outputs = embedding_model_0.output).get_weights()\n",
    "#print(weights_list2)\n",
    "\n",
    "embedding_model_1.set_weights([np.transpose(w_list[0], (0, 1, 3, 2)), w_list[1]])\n",
    "\n",
    "trainval_attributes = embedding_model_1.predict(trainval_input2)\n",
    "print(trainval_attributes, trainval_attributes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d18a521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2DTranspose)    (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      " layer2 (Conv2DTranspose)    (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,147,776\n",
      "Trainable params: 3,147,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "736/736 [==============================] - 1s 690us/step\n",
      "[[[[0.         0.         0.08818287 ... 0.         0.04905149\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.08435377 ... 0.         0.0668256\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.08818287 ... 0.         0.04905149\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.08196787 ... 0.         0.10544617\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.10573602 ... 0.01957784 0.07044251\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.03713213 0.         0.07227855 ... 0.         0.09928809\n",
      "    0.        ]]]] (23527, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "## for LE new\n",
    "\n",
    "#inputt = Input(shape = (1, 1, trainval_attributes.shape[1]))\n",
    "#hidden1 = Conv2DTranspose(1024, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "#hidden2 = Conv2DTranspose(1024, 1, name=\"layer2\", activation='relu')(hidden1)\n",
    "\n",
    "#embedding_model_1 = Model(inputs = inputt, outputs = hidden1)\n",
    "#embedding_model_1 = Model(inputs = inputt, outputs = hidden2)\n",
    "#embedding_model_1.summary()\n",
    "\n",
    "#w_list = Model(inputs = embedding_model.layers[-3].output, outputs = embedding_model.output).get_weights()\n",
    "\n",
    "#embedding_model_1.layers[-1].set_weights([np.transpose(w_list[0], (0, 1, 3, 2)), w_list[1]])\n",
    "#embedding_model_1.layers[-2].set_weights([w_list[2], w_list[1]])\n",
    "\n",
    "\n",
    "#trainval_attributes = embedding_model_1.predict(trainval_input2)\n",
    "#print(trainval_attributes, trainval_attributes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c3e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "[[[[0.         0.         0.09267049 ... 0.         0.13768362\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.03586135 0.02566641 0.08643061 ... 0.         0.0162082\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.11268637 ... 0.03148045 0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.00930722 0.         0.06534882 ... 0.         0.0441683\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.14253864 ... 0.         0.06400539\n",
      "    0.0064539 ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.13228218 ... 0.         0.\n",
      "    0.        ]]]] (50, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], 1, 1, attribute_new.shape[1]])\n",
    "\n",
    "# as labels range from 1 to 50, we have subtract 1\n",
    "attribute_2 = embedding_model_1.predict(attribute_new)\n",
    "print(attribute_2, attribute_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ecaa989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.03586135 0.         ... 0.00930722 0.         0.        ]\n",
      " [0.         0.02566641 0.         ... 0.         0.         0.        ]\n",
      " [0.09267049 0.08643061 0.11268637 ... 0.06534882 0.14253864 0.13228218]\n",
      " ...\n",
      " [0.         0.         0.03148045 ... 0.         0.         0.        ]\n",
      " [0.13768362 0.0162082  0.         ... 0.0441683  0.06400539 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.0064539  0.        ]] (1024, 50)\n",
      "Signature for trainval: (1024, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "signature_2 = attribute_2.transpose()\n",
    "signature_2 = np.reshape(signature_2, [signature_2.shape[0], signature_2.shape[3]])\n",
    "print(signature_2, signature_2.shape)#(50, 300)\n",
    "\n",
    "trainval_sig = signature_2[:, (unique_labels_trainval)-1]\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "581a4c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34]\n",
      " [19]\n",
      " [34]\n",
      " ...\n",
      " [32]\n",
      " [16]\n",
      " [37]] (23527, 1)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] (40,)\n"
     ]
    }
   ],
   "source": [
    "# by doing this modification, we are changing the range of trainval and test seen labels from 0 to 39 \n",
    "# and test unseen labels from 0 to 9\n",
    "\n",
    "k = 0\n",
    "new_labels_trainval = np.zeros((len(labels_trainval), 1), dtype = 'int')\n",
    "for labels in unique_labels_trainval:\n",
    "    new_labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "    \n",
    "print(new_labels_trainval, new_labels_trainval.shape)#(23527, 1)\n",
    "print(np.unique(new_labels_trainval), np.unique(new_labels_trainval).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee1341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23527\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "#params for trainval and test set\n",
    "m_trainval = new_labels_trainval.shape[0]# number of instances in training set: 23527\n",
    "print(m_trainval)\n",
    "\n",
    "z_trainval = len(unique_labels_trainval)# number of classes in training set: 40\n",
    "print(z_trainval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "845b8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]] (23527, 40)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "gt_trainval = to_categorical(new_labels_trainval, z_trainval)\n",
    "\n",
    "print(gt_trainval, gt_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08067319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "input1_shape = trainval_vec.shape[3]\n",
    "print(input1_shape)\n",
    "\n",
    "attribute_shape = trainval_sig.shape[0]\n",
    "print(attribute_shape)\n",
    "\n",
    "output_shape = z_trainval\n",
    "print(output_shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "858301c3",
   "metadata": {},
   "source": [
    "# Step 6 - Define 'model2' for attribute to class label mapping\n",
    "         Define 'model1' for visual feature to class label mapping\n",
    "         Train 'model2' and 'model1' through the iterative process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96792076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1, 1, 1024)]      0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 40)                41000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,000\n",
      "Trainable params: 41,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "\n",
    "# define model2 for attribute to class label mapping\n",
    "\n",
    "input2 = Input(shape = (1, 1, attribute_shape))\n",
    "flat = Flatten()(input2)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model2 = Model(inputs = input2, outputs = output)\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "#opt = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model2.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b285876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1, 1, 1024)]      0         \n",
      "                                                                 \n",
      " intermediate (Conv1D)       (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 40)                41000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,090,600\n",
      "Trainable params: 1,090,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model1 for resnet feature to class label mapping\n",
    "\n",
    "input1 = Input(shape = (1,1, input1_shape))\n",
    "#inter_pre = Dense(512, name=\"intermediate_previous\", activation='relu')(input1)\n",
    "inter = Conv1D(attribute_shape, kernel_size = 1, name = \"intermediate\", activation = 'linear')(input1)\n",
    "flat = Flatten()(inter)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model1 = Model(inputs = input1, outputs = output)\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "#opt = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model1.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fd4ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23527, 1, 1, 1024)\n",
      "(23527, 1, 1, 1024)\n",
      "(23527, 40)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = trainval_vec\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = trainval_attributes\n",
    "print(trainval_input2.shape)\n",
    "\n",
    "trainval_output = gt_trainval\n",
    "print(trainval_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cec915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 16\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(trainval_input1, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen1 = DataGenerator(X_train1, y_train1, batch_size)   \n",
    "#val_gen1 = DataGenerator(X_val1, y_val1, batch_size)\n",
    "\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(trainval_input2, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen2 = DataGenerator(X_train2, y_train2, batch_size)   \n",
    "#val_gen2 = DataGenerator(X_val2, y_val2, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3fb3ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " ...\n",
      " [38]\n",
      " [38]\n",
      " [38]] (37322, 1)\n",
      "[    0     1     3 ... 37306 37307 37319] 37319\n",
      "labels_test_seen [[22]\n",
      " [49]\n",
      " [14]\n",
      " ...\n",
      " [25]\n",
      " [15]\n",
      " [27]] (5882, 1)\n",
      "unique_labels_test_seen [ 1  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28\n",
      " 29 32 33 35 36 37 38 39 40 42 43 44 45 46 48 49] (40,)\n",
      "Features for test seen: (5882, 2048)\n",
      "184/184 [==============================] - 0s 690us/step\n",
      "(5882, 1, 1, 1024)\n",
      "[[[[0.         0.         0.08435377 ... 0.         0.0668256\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.14253864 ... 0.         0.06400539\n",
      "    0.0064539 ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.02559936 0.12289    ... 0.00933561 0.07491595\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.02211225 ... 0.         0.02683282\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.10873226 ... 0.         0.04536447\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.00945213 0.         0.01766315 ... 0.         0.06192227\n",
      "    0.        ]]]] (5882, 1, 1, 1024)\n",
      "Signature for test seen: (1024, 40)\n"
     ]
    }
   ],
   "source": [
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 37322 x 1\n",
    "\n",
    "test_seen_loc = np.squeeze(att_splits['test_seen_loc']-1)\n",
    "print(np.unique(test_seen_loc), np.max(np.unique(test_seen_loc))) # smallest location: 0, largest location 37319\n",
    "\n",
    "labels_test_seen = labels[test_seen_loc]\n",
    "print('labels_test_seen', labels_test_seen, labels_test_seen.shape)\n",
    "\n",
    "unique_labels_test_seen = np.unique(labels_test_seen) # labels min:1 max:49\n",
    "print('unique_labels_test_seen', unique_labels_test_seen, unique_labels_test_seen.shape)# 40 classes\n",
    "\n",
    "test_seen_vec = X_features[:, test_seen_loc].transpose()\n",
    "print(\"Features for test seen:\", test_seen_vec.shape)# (5882, 2048)\n",
    "\n",
    "test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "test_seen_vec = embedding_model_0.predict(test_seen_vec)\n",
    "print(test_seen_vec.shape)\n",
    "\n",
    "\n",
    "test_seen_attributes = np.zeros((len(test_seen_loc), attribute_2.shape[1], attribute_2.shape[2], attribute_2.shape[3]))\n",
    "for i in range(len(test_seen_loc)):\n",
    "    test_seen_attributes[i] = attribute_2[int(labels_test_seen[i])-1]\n",
    "\n",
    "print(test_seen_attributes, test_seen_attributes.shape)# (5882, 85)\n",
    "\n",
    "\n",
    "\n",
    "test_seen_sig = signature_2[:, (unique_labels_test_seen)-1]\n",
    "print(\"Signature for test seen:\", test_seen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a33b196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1046  1047  1048 ... 35288 35289 35290] 35290\n",
      "labels_test_unseen [[30]\n",
      " [30]\n",
      " [30]\n",
      " ...\n",
      " [47]\n",
      " [47]\n",
      " [47]] (7913, 1)\n",
      "unique_labels_test_unseen [ 7  9 23 24 30 31 34 41 47 50] (10,)\n",
      "Features for test unseen: (7913, 2048)\n",
      "248/248 [==============================] - 0s 689us/step\n",
      "(7913, 1, 1, 1024)\n",
      "Signature for test unseen: (1024, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_unseen_loc = np.squeeze(att_splits['test_unseen_loc']-1)\n",
    "print(np.unique(test_unseen_loc), np.max(np.unique(test_unseen_loc))) # smallest location: 1046, largest location 35290\n",
    "\n",
    "labels_test_unseen = labels[test_unseen_loc]\n",
    "print('labels_test_unseen', labels_test_unseen, labels_test_unseen.shape)\n",
    "\n",
    "unique_labels_test_unseen = np.unique(labels_test_unseen) # labels min:7 max:50\n",
    "print('unique_labels_test_unseen', unique_labels_test_unseen, unique_labels_test_unseen.shape)# 10 classes\n",
    "\n",
    "\n",
    "test_unseen_vec = X_features[:, test_unseen_loc].transpose()\n",
    "print(\"Features for test unseen:\", test_unseen_vec.shape) #(7913, 2048)\n",
    "\n",
    "\n",
    "test_unseen_vec = np.reshape(test_unseen_vec, [test_unseen_vec.shape[0], 1, 1, test_unseen_vec.shape[1]])\n",
    "test_unseen_vec = embedding_model_0.predict(test_unseen_vec)\n",
    "print(test_unseen_vec.shape)\n",
    "\n",
    "#test_unseen_attributes = np.zeros((len(test_unseen_loc), attribute_new.shape[1]))\n",
    "#for i in range(len(test_unseen_loc)):\n",
    "#    test_unseen_attributes[i] = attribute_new[int(labels_test_unseen[i])-1]\n",
    "\n",
    "#print(test_unseen_attributes, test_unseen_attributes.shape)# (7913, 85)\n",
    "\n",
    "\n",
    "#test_unseen_attributes_2 = model0.predict(test_unseen_attributes)\n",
    "#print(test_unseen_attributes_2, test_unseen_attributes_2.shape)\n",
    "\n",
    "test_unseen_sig = signature_2[:, (unique_labels_test_unseen)-1]\n",
    "\n",
    "print(\"Signature for test unseen:\", test_unseen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00ef41ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19]\n",
      " [39]\n",
      " [11]\n",
      " ...\n",
      " [20]\n",
      " [12]\n",
      " [22]] (5882, 1)\n",
      "5882\n",
      "40\n",
      "[[4]\n",
      " [4]\n",
      " [4]\n",
      " ...\n",
      " [8]\n",
      " [8]\n",
      " [8]] (7913, 1)\n",
      "7913\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l = 0\n",
    "new_labels_test_seen = np.zeros((len(labels_test_seen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_seen:\n",
    "    new_labels_test_seen[labels_test_seen == labels] = l\n",
    "    l = l+1\n",
    "    \n",
    "print(new_labels_test_seen, new_labels_test_seen.shape)# (5882, 1)\n",
    "\n",
    "\n",
    "\n",
    "n_test_seen = new_labels_test_seen.shape[0]# 5882\n",
    "print(n_test_seen)\n",
    "\n",
    "z1_test_seen = len(unique_labels_test_seen)# 40\n",
    "print(z1_test_seen)\n",
    "\n",
    "\n",
    "m = 0\n",
    "new_labels_test_unseen = np.zeros((len(labels_test_unseen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_unseen:\n",
    "    new_labels_test_unseen[labels_test_unseen == labels] = m\n",
    "    m = m+1  \n",
    "\n",
    "print(new_labels_test_unseen, new_labels_test_unseen.shape) #  (7913, 1)  \n",
    "\n",
    "n_test_unseen = new_labels_test_unseen.shape[0]# 7913\n",
    "print(n_test_unseen)\n",
    "\n",
    "z1_test_unseen = len(unique_labels_test_unseen)# 10\n",
    "print(z1_test_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f527990a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "model 2 is trained: training acc: 0.859375 , training loss: 1.03725004196167 , validation acc: 0.8125 , validation_loss: 1.0718791484832764\n",
      "model 1 is trained: training acc: 0.890625 , training loss: 1.6108466386795044 , validation acc: 0.75 , validation_loss: 10.454130172729492\n",
      "micro average\n",
      "seen accuracy: 73.12769999829378 unseen accuracy: 51.55851321337048 harmonic mean: 60.47750412028247\n",
      "macro average\n",
      "seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "best accuracy micro seen accuracy: 73.12769999829378 unseen accuracy: 51.55851321337048 harmonic mean: 60.47750412028247\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 1\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.3871808648109436 , validation acc: 1.0 , validation_loss: 0.5512005686759949\n",
      "model 1 is trained: training acc: 0.921875 , training loss: 6.957653999328613 , validation acc: 0.625 , validation_loss: 72.30197143554688\n",
      "micro average\n",
      "seen accuracy: 72.39252575748354 unseen accuracy: 36.78147144711792 harmonic mean: 48.779080867461396\n",
      "macro average\n",
      "seen accuracy: 79.1397483849031 unseen accuracy: 28.573233918867686 harmonic mean: 41.987112314928545\n",
      "best accuracy micro seen accuracy: 73.12769999829378 unseen accuracy: 51.55851321337048 harmonic mean: 60.47750412028247\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 2\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.19259783625602722 , validation acc: 1.0 , validation_loss: 0.2597458064556122\n",
      "model 1 is trained: training acc: 0.859375 , training loss: 18.170040130615234 , validation acc: 0.8125 , validation_loss: 17.587188720703125\n",
      "micro average\n",
      "seen accuracy: 74.42491240101157 unseen accuracy: 44.831119561816806 harmonic mean: 55.956115448608614\n",
      "macro average\n",
      "seen accuracy: 82.48894933696022 unseen accuracy: 46.02552761278908 harmonic mean: 59.084353849764916\n",
      "best accuracy micro seen accuracy: 73.12769999829378 unseen accuracy: 51.55851321337048 harmonic mean: 60.47750412028247\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 3\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.15681737661361694 , validation acc: 1.0 , validation_loss: 0.12100179493427277\n",
      "model 1 is trained: training acc: 0.96875 , training loss: 1.352982997894287 , validation acc: 0.875 , validation_loss: 5.494098663330078\n",
      "micro average\n",
      "seen accuracy: 78.47379030497227 unseen accuracy: 41.65178518152902 harmonic mean: 54.41927654332619\n",
      "macro average\n",
      "seen accuracy: 85.34512070724244 unseen accuracy: 36.26943005181347 harmonic mean: 50.90540344777634\n",
      "best accuracy micro seen accuracy: 73.12769999829378 unseen accuracy: 51.55851321337048 harmonic mean: 60.47750412028247\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 4\n",
      "model 2 is trained: training acc: 0.984375 , training loss: 0.1955723762512207 , validation acc: 1.0 , validation_loss: 0.13000497221946716\n",
      "model 1 is trained: training acc: 0.921875 , training loss: 7.633966445922852 , validation acc: 0.9375 , validation_loss: 15.566497802734375\n",
      "micro average\n",
      "seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "macro average\n",
      "seen accuracy: 86.92621557293437 unseen accuracy: 44.38266144319474 harmonic mean: 58.762467305813836\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 5\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.13951827585697174 , validation acc: 1.0 , validation_loss: 0.12379613518714905\n",
      "model 1 is trained: training acc: 0.953125 , training loss: 1.7269694805145264 , validation acc: 0.9375 , validation_loss: 15.210819244384766\n",
      "micro average\n",
      "seen accuracy: 79.69507459092328 unseen accuracy: 43.54176031575515 harmonic mean: 56.31536770336448\n",
      "macro average\n",
      "seen accuracy: 85.226113566814 unseen accuracy: 36.30734234803488 harmonic mean: 50.92151225306763\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 6\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.1285172998905182 , validation acc: 1.0 , validation_loss: 0.12225997447967529\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 0.2009926289319992 , validation acc: 0.875 , validation_loss: 7.587852478027344\n",
      "micro average\n",
      "seen accuracy: 80.00053506747363 unseen accuracy: 44.32650352313551 harmonic mean: 57.04541891644395\n",
      "macro average\n",
      "seen accuracy: 86.46718803128188 unseen accuracy: 42.63869581700999 harmonic mean: 57.113557007987815\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 7\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.1456764042377472 , validation acc: 1.0 , validation_loss: 0.12233904749155045\n",
      "model 1 is trained: training acc: 0.953125 , training loss: 1.1194943189620972 , validation acc: 0.8125 , validation_loss: 17.693702697753906\n",
      "micro average\n",
      "seen accuracy: 80.07894389215696 unseen accuracy: 45.03668768339731 harmonic mean: 57.65051641703936\n",
      "macro average\n",
      "seen accuracy: 86.33117987079225 unseen accuracy: 43.182105396183495 harmonic mean: 57.56879845141276\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 8\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.12650389969348907 , validation acc: 1.0 , validation_loss: 0.11086955666542053\n",
      "model 1 is trained: training acc: 1.0 , training loss: 1.0840293498404208e-06 , validation acc: 0.9375 , validation_loss: 0.6697553992271423\n",
      "micro average\n",
      "seen accuracy: 82.05931167854563 unseen accuracy: 46.923359768172915 harmonic mean: 59.705673033941345\n",
      "macro average\n",
      "seen accuracy: 88.04828289697382 unseen accuracy: 44.31947428282573 harmonic mean: 58.96078762139686\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 9\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.10465545207262039 , validation acc: 1.0 , validation_loss: 0.10850587487220764\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 2.15753173828125 , validation acc: 0.9375 , validation_loss: 7.917884826660156\n",
      "micro average\n",
      "seen accuracy: 81.6769176307649 unseen accuracy: 43.99141266797717 harmonic mean: 57.18358762946539\n",
      "macro average\n",
      "seen accuracy: 87.60625637538251 unseen accuracy: 39.289776317452294 harmonic mean: 54.24961118098867\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.10259518772363663 , validation acc: 1.0 , validation_loss: 0.12553998827934265\n",
      "model 1 is trained: training acc: 0.953125 , training loss: 4.596445083618164 , validation acc: 0.9375 , validation_loss: 30.583168029785156\n",
      "micro average\n",
      "seen accuracy: 82.60267350078074 unseen accuracy: 43.07422612175757 harmonic mean: 56.622119853699154\n",
      "macro average\n",
      "seen accuracy: 87.87827269636178 unseen accuracy: 38.607354985466955 harmonic mean: 53.64637440124778\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 11\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.10797490179538727 , validation acc: 1.0 , validation_loss: 0.11552275717258453\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 2.610157012939453 , validation acc: 0.75 , validation_loss: 46.638946533203125\n",
      "micro average\n",
      "seen accuracy: 83.85683108820372 unseen accuracy: 42.913597547667095 harmonic mean: 56.77346585737768\n",
      "macro average\n",
      "seen accuracy: 89.44236654199253 unseen accuracy: 37.141412864905845 harmonic mean: 52.487228283363244\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 12\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.10594625771045685 , validation acc: 1.0 , validation_loss: 0.0637601763010025\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 11.485034942626953 , validation acc: 0.9375 , validation_loss: 27.091827392578125\n",
      "micro average\n",
      "seen accuracy: 83.90786859282645 unseen accuracy: 38.89063550892492 harmonic mean: 53.147721263289256\n",
      "macro average\n",
      "seen accuracy: 88.64331859911594 unseen accuracy: 30.696322507266522 harmonic mean: 45.60134202862649\n",
      "best accuracy micro seen accuracy: 81.58759177767548 unseen accuracy: 48.676113581224534 harmonic mean: 60.97426559833962\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 13\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.10414516925811768 , validation acc: 1.0 , validation_loss: 0.08734706789255142\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 0.6479110717773438 , validation acc: 1.0 , validation_loss: 0.0\n",
      "micro average\n",
      "seen accuracy: 84.6074375942711 unseen accuracy: 47.99143202945843 harmonic mean: 61.2438417011281\n",
      "macro average\n",
      "seen accuracy: 89.30635838150289 unseen accuracy: 39.66889927966637 harmonic mean: 54.93588460162817\n",
      "best accuracy micro seen accuracy: 84.6074375942711 unseen accuracy: 47.99143202945843 harmonic mean: 61.2438417011281\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 14\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.0956031009554863 , validation acc: 1.0 , validation_loss: 0.08968507498502731\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.9375 , validation_loss: 30.31365966796875\n",
      "micro average\n",
      "seen accuracy: 83.4121601017076 unseen accuracy: 44.55030541043222 harmonic mean: 58.08011267370277\n",
      "macro average\n",
      "seen accuracy: 88.71132267936076 unseen accuracy: 42.62605838493619 harmonic mean: 57.58321034411571\n",
      "best accuracy micro seen accuracy: 84.6074375942711 unseen accuracy: 47.99143202945843 harmonic mean: 61.2438417011281\n",
      "best accuracy macro seen accuracy: 81.91091465487929 unseen accuracy: 46.36673827878175 harmonic mean: 59.21439713186096\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iteration = 15\n",
    "epochs1 = 200\n",
    "epochs2 = 200\n",
    "\n",
    "best_performance_micro = [0, 0, 0]\n",
    "best_performance_macro = [0, 0, 0]\n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/AWA2/BertAttribute/with_LE/'\n",
    "name = 'model1_conv_AWA2_LEnew_Bert2attT_1024_lin_it15_200eph_adam_cce_16bch_1e-2lr_model2_adam_lr-2_200'\n",
    "\n",
    "for i in range(iteration):\n",
    "    X_train2_it = X_train2[(len(X_train2)//iteration)*i:(len(X_train2)//iteration)*(i+1)]\n",
    "    X_val2_it = X_val2[(len(X_val2)//iteration)*i:(len(X_val2)//iteration)*(i+1)]\n",
    "    y_train2_it = y_train2[(len(y_train2)//iteration)*i:(len(y_train2)//iteration)*(i+1)]\n",
    "    y_val2_it = y_val2[(len(y_val2)//iteration)*i:(len(y_val2)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen2 = DataGenerator(X_train2_it, y_train2_it, batch_size)   \n",
    "    val_gen2 = DataGenerator(X_val2_it, y_val2_it, batch_size)\n",
    "\n",
    "    train_summary2 = model2.fit(train_gen2, epochs = epochs2, verbose = 0, callbacks = None, validation_data = val_gen2, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen2)//batch_size, \n",
    "                              validation_steps = len(val_gen2)//batch_size)\n",
    "\n",
    "    print(\"iteration:\", i)\n",
    "    print('model 2 is trained:', 'training acc:', train_summary2.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary2.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary2.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary2.history['val_loss'][-1])\n",
    "\n",
    "    weights_list2 = model2.get_weights()\n",
    "    #print(weights_list2)\n",
    "\n",
    "    model1.layers[-1].set_weights(weights_list2)\n",
    "\n",
    "    X_train1_it = X_train1[(len(X_train1)//iteration)*i:(len(X_train1)//iteration)*(i+1)]\n",
    "    X_val1_it = X_val1[(len(X_val1)//iteration)*i:(len(X_val1)//iteration)*(i+1)]\n",
    "    y_train1_it = y_train1[(len(y_train1)//iteration)*i:(len(y_train1)//iteration)*(i+1)]\n",
    "    y_val1_it = y_val1[(len(y_val1)//iteration)*i:(len(y_val1)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen1 = DataGenerator(X_train1_it, y_train1_it, batch_size)   \n",
    "    val_gen1 = DataGenerator(X_val1_it, y_val1_it, batch_size)\n",
    "    \n",
    "    for layer in model1.layers[2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    train_summary1 = model1.fit(train_gen1, epochs = epochs1, verbose = 0, callbacks = None, validation_data = val_gen1, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen1)//batch_size, \n",
    "                              validation_steps = len(val_gen1)//batch_size)\n",
    "    \n",
    "    #print(\"iteration:\", i)\n",
    "    print('model 1 is trained:', 'training acc:', train_summary1.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary1.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary1.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary1.history['val_loss'][-1])\n",
    "    \n",
    "    weights_list1 = Model(inputs = model1.input, outputs = model1.layers[-1].output).get_weights()\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, np.matmul(weights_list1[0], weights_list1[2])), test_seen_sig)\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0]), test_seen_sig)\n",
    "    \n",
    "    outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0][0]), test_seen_sig)\n",
    "    preds_seen = np.array([np.argmax(output) for output in outputs_seen])\n",
    "    \n",
    "    cm_seen = confusion_matrix(new_labels_test_seen, preds_seen)\n",
    "    #print(cm)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_seen_micro = cm_seen.astype('float') / cm_seen.sum(axis=1)[:, np.newaxis]\n",
    "    #print(cm)\n",
    "    avg_seen_micro = (sum(cm_seen_micro.diagonal())/len(unique_labels_test_seen))*100\n",
    "\n",
    "    avg_seen_macro = (sum(cm_seen.diagonal())/len(new_labels_test_seen))*100\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0]), test_unseen_sig)\n",
    "    outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0][0]), test_unseen_sig)\n",
    "    \n",
    "    preds_unseen = np.array([np.argmax(output) for output in outputs_unseen])\n",
    "    \n",
    "    cm_unseen = confusion_matrix(new_labels_test_unseen, preds_unseen)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_unseen_micro = cm_unseen.astype('float') / cm_unseen.sum(axis=1)[:, np.newaxis]\n",
    "    avg_unseen_micro = (sum(cm_unseen_micro.diagonal())/len(unique_labels_test_unseen))*100\n",
    "\n",
    "    avg_unseen_macro = (sum(cm_unseen.diagonal())/len(new_labels_test_unseen))*100\n",
    "    \n",
    "    harmonic_micro = (2*avg_seen_micro*avg_unseen_micro) / (avg_seen_micro + avg_unseen_micro)\n",
    "    harmonic_macro = (2*avg_seen_macro*avg_unseen_macro) / (avg_seen_macro + avg_unseen_macro)\n",
    "    \n",
    "    print('micro average')\n",
    "    print('seen accuracy:', avg_seen_micro, 'unseen accuracy:', avg_unseen_micro, 'harmonic mean:', harmonic_micro)\n",
    "    \n",
    "    print('macro average')\n",
    "    print('seen accuracy:', avg_seen_macro, 'unseen accuracy:', avg_unseen_macro, 'harmonic mean:', harmonic_macro)\n",
    "    \n",
    "    if harmonic_micro > best_performance_micro[2]:\n",
    "        best_performance_micro = [avg_seen_micro, avg_unseen_micro, harmonic_micro]\n",
    "        model1.save_weights(save_path + 'bw_micro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    if harmonic_macro > best_performance_macro[2]:\n",
    "        best_performance_macro = [avg_seen_macro, avg_unseen_macro, harmonic_macro]\n",
    "        model1.save_weights(save_path + 'bw_macro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    print('best accuracy micro','seen accuracy:', best_performance_micro[0], 'unseen accuracy:', best_performance_micro[1], 'harmonic mean:', best_performance_micro[2])\n",
    "    print('best accuracy macro', 'seen accuracy:', best_performance_macro[0], 'unseen accuracy:', best_performance_macro[1], 'harmonic mean:', best_performance_macro[2])\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    weights_list3 = model1.get_weights()\n",
    "    model2.set_weights(weights_list3[2:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf41c589",
   "metadata": {},
   "source": [
    "Step 7 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58bc47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (5882, 40)\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 29.5567 - accuracy: 0.9033\n",
      "cce =  1.5199095\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 1.0000\n",
      "cce =  0.13685596\n"
     ]
    }
   ],
   "source": [
    "gt_test_seen = to_categorical(new_labels_test_seen, z1_test_seen)\n",
    "\n",
    "print(gt_test_seen, gt_test_seen.shape)\n",
    "\n",
    "#test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "res1 = model1.evaluate(test_seen_vec, gt_test_seen)\n",
    "\n",
    "p1 = model1.predict(test_seen_vec, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p1).numpy())\n",
    "\n",
    "#test_seen_attributes = np.reshape(train_attributes, [test_seen_attributes.shape[0], 1, train_attributes.shape[1]])\n",
    "#test_seen_attributes = np.reshape(test_seen_attributes, [test_seen_attributes.shape[0], 1, 1, test_seen_attributes.shape[1]])\n",
    "\n",
    "res2 = model2.evaluate(test_seen_attributes, gt_test_seen)\n",
    "\n",
    "p2 = model2.predict(test_seen_attributes, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p2).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c3d7cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.77104924268816\n",
      "95.16321122646332\n",
      "73.25016140911761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_seen_updated = res1[1]*100\n",
    "unseen_accuracy = 59.54\n",
    "h = (2*accuracy_seen_updated*unseen_accuracy) / (accuracy_seen_updated + unseen_accuracy)\n",
    "print(h)\n",
    "\n",
    "\n",
    "accuracy_seen_updated2 = ((res1[1]*100)+(res2[1]*100))/2\n",
    "print(accuracy_seen_updated2)\n",
    "h = (2*accuracy_seen_updated2*unseen_accuracy) / (accuracy_seen_updated2 + unseen_accuracy)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adb1f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_seen_macro 0.9522035396323645 recall_seen_macro 0.9365450051607678 f1_seen_macro 0.9413005107028479\n",
      "precision_seen_micro 0.9516320979258756 recall_seen_micro 0.9516320979258756 f1_seen_micro 0.9516320979258756\n",
      "precision_unseen_macro 0.46990354969739967 recall_unseen_macro 0.4455030541043222 f1_unseen_macro 0.36606799687401165\n",
      "precision_unseen_micro 0.42626058384936183 recall_unseen_micro 0.42626058384936183 f1_unseen_micro 0.42626058384936183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pp1 = np.array([np.argmax(output) for output in p1])\n",
    "pp2 = np.array([np.argmax(output) for output in p2])\n",
    "\n",
    "seen_macro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'macro')\n",
    "seen_macro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'macro')\n",
    "print('precision_seen_macro', (seen_macro1[0] + seen_macro2[0])/2, 'recall_seen_macro', (seen_macro1[1] + seen_macro2[1])/2, 'f1_seen_macro', (seen_macro1[2] + seen_macro2[2])/2)\n",
    "\n",
    "\n",
    "seen_micro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'micro')\n",
    "seen_micro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'micro')\n",
    "print('precision_seen_micro', (seen_micro1[0] + seen_micro2[0])/2, 'recall_seen_micro', (seen_micro1[1] + seen_micro2[1])/2, 'f1_seen_micro', (seen_micro1[2] + seen_micro2[2])/2)\n",
    "\n",
    "unseen_macro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'macro')\n",
    "unseen_micro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'micro')\n",
    "\n",
    "print('precision_unseen_macro', unseen_macro[0], 'recall_unseen_macro', unseen_macro[1], 'f1_unseen_macro', unseen_macro[2])\n",
    "print('precision_unseen_micro', unseen_micro[0], 'recall_unseen_micro', unseen_micro[1], 'f1_unseen_micro', unseen_micro[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc438ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
