{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0af25f3",
   "metadata": {},
   "source": [
    "GZSL_AWA2_2: Experiments with direct matrix multiplcation of 85 length attributes and 300 length word embeddings of the attributes (the length may varry with different language models)\n",
    "    Step 1 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n",
    "\tStep 2 - Class wise continous attributes are extracted (for AWA2: 50 categories, and each category has attribute vectors of length 85) \n",
    "\tStep 3 - Word vectors are extracted for each semantic attribute using pretrained language models (w2v) \n",
    "    Step 4 - New attributes are formed by matrix multiplication of the continous attribute values and word vectors\n",
    "\tStep 5 - Define 'model2' for attribute to class label mapping\n",
    "\t\t Define 'model1' for visual feature to class label mapping\n",
    "\t\t Train 'model2' and 'model1' through the iterative process\n",
    "\tStep 6 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bb0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'AWA2'\n",
    "path = 'E:/Sushree/Dataset/data/xlsa17/data/'\n",
    "\n",
    "res101 = scipy.io.loadmat(path + dataset + '/res101.mat')\n",
    "att_splits = scipy.io.loadmat(path + dataset + '/att_splits.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237b819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4     5 ... 37318 37320 37321] 37321\n",
      "[    0     1     3 ... 37306 37307 37319] 37319\n",
      "[ 1046  1047  1048 ... 35288 35289 35290] 35290\n"
     ]
    }
   ],
   "source": [
    "# total number of instances or images = 37322: ranges from 0 to 37321\n",
    "\n",
    "trainval_loc = np.squeeze(att_splits['trainval_loc']-1) # -1: to consider the overflow problem\n",
    "print(np.unique(trainval_loc), np.max(np.unique(trainval_loc))) # smallest location: 2, largest location 37321\n",
    "\n",
    "test_seen_loc = np.squeeze(att_splits['test_seen_loc']-1)\n",
    "print(np.unique(test_seen_loc), np.max(np.unique(test_seen_loc))) # smallest location: 0, largest location 37319\n",
    "\n",
    "test_unseen_loc = np.squeeze(att_splits['test_unseen_loc']-1)\n",
    "print(np.unique(test_unseen_loc), np.max(np.unique(test_unseen_loc))) # smallest location: 1046, largest location 35290\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c148c5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " ...\n",
      " [38]\n",
      " [38]\n",
      " [38]] (37322, 1)\n",
      "unique_labels [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50] (50,)\n",
      "labels_trainval [[43]\n",
      " [22]\n",
      " [43]\n",
      " ...\n",
      " [40]\n",
      " [19]\n",
      " [46]] (23527, 1)\n",
      "unique_labels_trainval [ 1  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28\n",
      " 29 32 33 35 36 37 38 39 40 42 43 44 45 46 48 49] (40,)\n",
      "labels_test_seen [[22]\n",
      " [49]\n",
      " [14]\n",
      " ...\n",
      " [25]\n",
      " [15]\n",
      " [27]] (5882, 1)\n",
      "unique_labels_test_seen [ 1  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28\n",
      " 29 32 33 35 36 37 38 39 40 42 43 44 45 46 48 49] (40,)\n",
      "labels_test_unseen [[30]\n",
      " [30]\n",
      " [30]\n",
      " ...\n",
      " [47]\n",
      " [47]\n",
      " [47]] (7913, 1)\n",
      "unique_labels_test_unseen [ 7  9 23 24 30 31 34 41 47 50] (10,)\n",
      "correct number of instances for training, test seen and test unseen categories\n",
      "Number of overlapping classes between trainval and test seen: 40\n",
      "Number of overlapping classes between trainval and test unseen: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 37322 x 1\n",
    "\n",
    "print('unique_labels', np.unique(labels), np.unique(labels).shape)# class labels range from 1 to 50, 50 classes\n",
    "\n",
    "# get the labels for trainval, test seen and test unseen sets\n",
    "\n",
    "labels_trainval = labels[trainval_loc]\n",
    "print('labels_trainval', labels_trainval, labels_trainval.shape)\n",
    "\n",
    "unique_labels_trainval = np.unique(labels_trainval) # labels min:1 max:49\n",
    "print('unique_labels_trainval', unique_labels_trainval, unique_labels_trainval.shape)# 40 classes\n",
    "\n",
    "\n",
    "labels_test_seen = labels[test_seen_loc]\n",
    "print('labels_test_seen', labels_test_seen, labels_test_seen.shape)\n",
    "\n",
    "unique_labels_test_seen = np.unique(labels_test_seen) # labels min:1 max:49\n",
    "print('unique_labels_test_seen', unique_labels_test_seen, unique_labels_test_seen.shape)# 40 classes\n",
    "\n",
    "\n",
    "labels_test_unseen = labels[test_unseen_loc]\n",
    "print('labels_test_unseen', labels_test_unseen, labels_test_unseen.shape)\n",
    "\n",
    "unique_labels_test_unseen = np.unique(labels_test_unseen) # labels min:7 max:50\n",
    "print('unique_labels_test_unseen', unique_labels_test_unseen, unique_labels_test_unseen.shape)# 10 classes\n",
    "\n",
    "\n",
    "if len(labels) == len(labels_trainval) + len(labels_test_seen) + len(labels_test_unseen):\n",
    "    print('correct number of instances for training, test seen and test unseen categories')\n",
    "    \n",
    "print(\"Number of overlapping classes between trainval and test seen:\",len(set(unique_labels_trainval).intersection(set(unique_labels_test_seen))))\n",
    "\n",
    "print(\"Number of overlapping classes between trainval and test unseen:\",len(set(unique_labels_trainval).intersection(set(unique_labels_test_unseen))))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "debe6b80",
   "metadata": {},
   "source": [
    "Step 1 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88acec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for trainval: (23527, 2048)\n",
      "Features for test seen: (5882, 2048)\n",
      "Features for test unseen: (7913, 2048)\n"
     ]
    }
   ],
   "source": [
    "X_features = res101['features']\n",
    "\n",
    "# locations are already subtracted by 1, so they range from 0 to 37321\n",
    "trainval_vec = X_features[:, trainval_loc].transpose()\n",
    "test_seen_vec = X_features[:, test_seen_loc].transpose()\n",
    "test_unseen_vec = X_features[:, test_unseen_loc].transpose()\n",
    "\n",
    "print(\"Features for trainval:\", trainval_vec.shape) #(23527, 2048)\n",
    "print(\"Features for test seen:\", test_seen_vec.shape)# (5882, 2048)\n",
    "print(\"Features for test unseen:\", test_unseen_vec.shape) #(7913, 2048)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8170a385",
   "metadata": {},
   "source": [
    "\tStep 2 - Class wise continous attributes are extracted (for AWA2: 50 categories, and each category has attribute vectors of length 85) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e89686f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 50)\n",
      "[[-0.00375358 -0.00375358 -0.00375358 ...  0.00882092  0.03640974\n",
      "   0.03145501]\n",
      " [ 0.12045618  0.00426584  0.         ...  0.17996306  0.0618086\n",
      "   0.03495531]\n",
      " [ 0.26584459  0.20652363  0.         ...  0.05026822  0.04274552\n",
      "   0.04915256]\n",
      " ...\n",
      " [ 0.22516498  0.15266022  0.         ...  0.12733492  0.10009694\n",
      "   0.01771   ]\n",
      " [ 0.19613947  0.1966714   0.         ...  0.01787277  0.06698743\n",
      "   0.25883601]\n",
      " [ 0.03819588  0.08046548  0.10363715 ...  0.01479997  0.05250999\n",
      "   0.14194515]] (50, 85)\n",
      "[[0.00575881 0.003829   0.         ... 0.03639079 0.13208508 0.01148699]\n",
      " [0.         0.00507555 0.         ... 0.15675628 0.09070911 0.01425057]\n",
      " [0.00575881 0.003829   0.         ... 0.03639079 0.13208508 0.01148699]\n",
      " ...\n",
      " [0.         0.06587863 0.         ... 0.02108505 0.10218637 0.0332632 ]\n",
      " [0.0084177  0.01262655 0.         ... 0.02104426 0.04138142 0.02316552]\n",
      " [0.03877321 0.15834626 0.         ... 0.1760296  0.07107783 0.30279846]] (23527, 85)\n",
      "[[0.         0.00507555 0.         ... 0.15675628 0.09070911 0.01425057]\n",
      " [0.19613947 0.1966714  0.         ... 0.01787277 0.06698743 0.25883601]\n",
      " [0.01928112 0.         0.         ... 0.10161998 0.0093374  0.        ]\n",
      " ...\n",
      " [0.13407657 0.02802316 0.         ... 0.02469312 0.11832941 0.10334422]\n",
      " [0.1170179  0.05564648 0.         ... 0.09686609 0.0632034  0.01789046]\n",
      " [0.03588774 0.04482569 0.         ... 0.07673723 0.0732708  0.05016127]] (5882, 85)\n",
      "[[0.27653654 0.00419864 0.         ... 0.06032152 0.10544938 0.01679457]\n",
      " [0.27653654 0.00419864 0.         ... 0.06032152 0.10544938 0.01679457]\n",
      " [0.27653654 0.00419864 0.         ... 0.06032152 0.10544938 0.01679457]\n",
      " ...\n",
      " [0.06218048 0.01590817 0.         ... 0.03693204 0.1114562  0.06188344]\n",
      " [0.06218048 0.01590817 0.         ... 0.03693204 0.1114562  0.06188344]\n",
      " [0.06218048 0.01590817 0.         ... 0.03693204 0.1114562  0.06188344]] (7913, 85)\n"
     ]
    }
   ],
   "source": [
    "signature = att_splits['att']\n",
    "print(signature.shape) #(85, 50)\n",
    "\n",
    "attribute = signature.transpose()\n",
    "print(attribute, attribute.shape)#(50, 85)\n",
    "\n",
    "# attribute is defined for all 50 classes, so we cant use locations directly, instead we have to use labels \n",
    "# that range from 1 to 50, so we have to subtract 1\n",
    "\n",
    "train_attributes = np.zeros((len(trainval_loc), 85))\n",
    "for i in range(len(trainval_loc)):\n",
    "    train_attributes[i] = attribute[int(labels_trainval[i])-1]\n",
    "\n",
    "print(train_attributes, train_attributes.shape)# (23527, 85)\n",
    "\n",
    "test_seen_attributes = np.zeros((len(test_seen_loc), 85))\n",
    "for i in range(len(test_seen_loc)):\n",
    "    test_seen_attributes[i] = attribute[int(labels_test_seen[i])-1]\n",
    "\n",
    "print(test_seen_attributes, test_seen_attributes.shape)# (5882, 85)\n",
    "\n",
    "test_unseen_attributes = np.zeros((len(test_unseen_loc), 85))\n",
    "for i in range(len(test_unseen_loc)):\n",
    "    test_unseen_attributes[i] = attribute[int(labels_test_unseen[i])-1]\n",
    "\n",
    "print(test_unseen_attributes, test_unseen_attributes.shape)# (7913, 85)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6c37c7b",
   "metadata": {},
   "source": [
    "Step 3 - Word vectors are extracted for each semantic attribute using pretrained language models (w2v) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6ebf9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n",
      "[=-------------------------------------------------] 3.1% 23.4/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 7.8% 59.2/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 13.3% 100.8/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 18.9% 143.1/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============-------------------------------------] 27.0% 205.1/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================----------------------------------] 32.7% 248.2/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 38.4% 291.0/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 45.8% 347.0/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 53.9% 408.6/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 60.7% 460.6/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 69.1% 523.8/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================------------] 76.4% 579.3/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================================---------] 83.5% 633.7/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 90.4% 685.9/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 98.9% 750.1/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done replacing OOD words\n",
      "Done preprocessing attribute des\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "print('Load pretrain w2v model')\n",
    "\n",
    "#model_name = 'word2vec-google-news-300'#best model\n",
    "#model_name = 'fasttext-wiki-news-subwords-300'\n",
    "#model_name = 'glove-wiki-gigaword-300'\n",
    "#model_name = 'glove-wiki-gigaword-200'\n",
    "#model_name = 'glove-twitter-100'\n",
    "model_name = 'glove-twitter-200'\n",
    "model = api.load(model_name)\n",
    "\n",
    "dim_w2v = 200\n",
    "\n",
    "#%%\n",
    "replace_word = [('newworld','new world'),('oldworld','old world'),('nestspot','nest spot'),('toughskin','tough skin'),\n",
    "                ('longleg','long leg'),('chewteeth','chew teeth'),('meatteeth','meat teeth'),('strainteeth','strain teeth'),\n",
    "                ('quadrapedal','quadrupedal')]  # for AWA2\n",
    "\n",
    "\n",
    "#For AWA2\n",
    "path = 'E:/Sushree/Dataset/Animals_with_Attributes2/attribute/predicates.txt'\n",
    "df=pd.read_csv(path,sep='\\t',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('E:/Sushree/Dataset/Animals_with_Attributes2/attribute/new_des.csv')\n",
    "print('Done preprocessing attribute des')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdaa34ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n",
      "white\n",
      "blue\n",
      "brown\n",
      "gray\n",
      "orange\n",
      "red\n",
      "yellow\n",
      "patches\n",
      "spots\n",
      "stripes\n",
      "furry\n",
      "hairless\n",
      "tough skin\n",
      "big\n",
      "small\n",
      "bulbous\n",
      "lean\n",
      "flippers\n",
      "hands\n",
      "hooves\n",
      "pads\n",
      "paws\n",
      "long leg\n",
      "longneck\n",
      "tail\n",
      "chew teeth\n",
      "meat teeth\n",
      "buckteeth\n",
      "\"Key 'buckteeth' not present\"\n",
      "strain teeth\n",
      "horns\n",
      "claws\n",
      "tusks\n",
      "smelly\n",
      "flys\n",
      "hops\n",
      "swims\n",
      "tunnels\n",
      "walks\n",
      "fast\n",
      "slow\n",
      "strong\n",
      "weak\n",
      "muscle\n",
      "bipedal\n",
      "\"Key 'bipedal' not present\"\n",
      "quadrupedal\n",
      "\"Key 'quadrupedal' not present\"\n",
      "active\n",
      "inactive\n",
      "nocturnal\n",
      "hibernate\n",
      "agility\n",
      "fish\n",
      "meat\n",
      "plankton\n",
      "vegetation\n",
      "insects\n",
      "forager\n",
      "\"Key 'forager' not present\"\n",
      "grazer\n",
      "hunter\n",
      "scavenger\n",
      "skimmer\n",
      "stalker\n",
      "new world\n",
      "old world\n",
      "arctic\n",
      "coastal\n",
      "desert\n",
      "bush\n",
      "plains\n",
      "forest\n",
      "fields\n",
      "jungle\n",
      "mountains\n",
      "ocean\n",
      "ground\n",
      "water\n",
      "tree\n",
      "cave\n",
      "fierce\n",
      "timid\n",
      "smart\n",
      "group\n",
      "solitary\n",
      "nest spot\n",
      "domestic\n",
      "counter_err  4\n",
      "[[-0.15180001 -0.34540001  0.13564999 ...  0.088699   -0.26209\n",
      "  -0.98259997]\n",
      " [-0.001487   -0.75738001 -0.12503    ... -0.20465    -0.31501999\n",
      "  -0.84643   ]\n",
      " [ 0.33017999 -0.88235998 -0.045686   ... -0.50751001 -0.67422998\n",
      "  -0.30599001]\n",
      " ...\n",
      " [-0.20317     0.23038     0.48669001 ...  0.058899   -0.72808999\n",
      "  -0.89265001]\n",
      " [-0.1242715  -0.54183502 -0.13713499 ...  0.172635    0.49854501\n",
      "  -0.236192  ]\n",
      " [-0.82306999 -0.20922001 -0.37755001 ...  0.44056001  0.24769001\n",
      "  -0.16542999]] (85, 200)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "counter_err = 0\n",
    "\n",
    "all_w2v = []\n",
    "for s in des:\n",
    "    print(s)\n",
    "    words = s.split(' ')\n",
    "    if words[-1] == '':     #remove empty element\n",
    "        words = words[:-1]\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    for w in words:\n",
    "        try:\n",
    "            w2v += model[w]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            counter_err += 1\n",
    "    w2v = w2v / len(words)  \n",
    "    all_w2v.append(w2v[np.newaxis,:])\n",
    "    \n",
    "print('counter_err ',counter_err)\n",
    "\n",
    "#%%\n",
    "w2v_att = np.concatenate(all_w2v,axis=0)\n",
    "#pdb.set_trace()\n",
    "#%%\n",
    "print(w2v_att, w2v_att.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64947c9d",
   "metadata": {},
   "source": [
    "    Step 4 - New attributes are formed by matrix multiplication of the continous attribute values and word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ef62ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.51846261 -1.46128913  0.46780752 ... -0.19895235 -1.11124077\n",
      "  -0.38702676]\n",
      " [-0.71262875 -1.39174934  0.14143536 ... -0.37909013 -1.44642252\n",
      "  -0.29949796]\n",
      " [-0.51846261 -1.46128913  0.46780752 ... -0.19895235 -1.11124077\n",
      "  -0.38702676]\n",
      " ...\n",
      " [-0.38056223 -1.28358856  0.5655959  ... -0.02774314 -0.82397784\n",
      "  -0.11024354]\n",
      " [-0.33602517 -1.27767423  0.55901165 ...  0.15864065 -0.74487098\n",
      "  -0.65227347]\n",
      " [-0.6716412  -1.47075539  0.23837939 ...  0.15955534 -1.05680062\n",
      "  -0.54131677]] (23527, 200)\n",
      "[[-0.71262875 -1.39174934  0.14143536 ... -0.37909013 -1.44642252\n",
      "  -0.29949796]\n",
      " [-0.41481943 -1.56490858  0.38805618 ...  0.26498422 -0.69976478\n",
      "  -0.70761112]\n",
      " [-0.37525223 -1.10834709  0.60077204 ...  0.21162192 -0.50117273\n",
      "  -0.51063679]\n",
      " ...\n",
      " [-0.41315966 -1.19172365  0.87059488 ... -0.50687576 -1.47443621\n",
      "  -0.69418162]\n",
      " [-0.51223378 -1.60515834  0.44764392 ...  0.0124671  -0.87060189\n",
      "  -0.49913848]\n",
      " [-0.30895518 -1.16712091  0.31008908 ... -0.39216176 -1.4220171\n",
      "  -0.71962442]] (5882, 200)\n",
      "[[-0.48969867 -0.88238356  0.35864243 ... -0.30818533 -1.23420019\n",
      "  -1.02830847]\n",
      " [-0.48969867 -0.88238356  0.35864243 ... -0.30818533 -1.23420019\n",
      "  -1.02830847]\n",
      " [-0.48969867 -0.88238356  0.35864243 ... -0.30818533 -1.23420019\n",
      "  -1.02830847]\n",
      " ...\n",
      " [-0.72077047 -1.44539472  0.37935861 ...  0.32740016 -0.5905929\n",
      "  -0.29655395]\n",
      " [-0.72077047 -1.44539472  0.37935861 ...  0.32740016 -0.5905929\n",
      "  -0.29655395]\n",
      " [-0.72077047 -1.44539472  0.37935861 ...  0.32740016 -0.5905929\n",
      "  -0.29655395]] (7913, 200)\n"
     ]
    }
   ],
   "source": [
    "train_attributes_2 = np.matmul(train_attributes, w2v_att)\n",
    "#train_attributes_2[train_attributes_2<0]=0\n",
    "print(train_attributes_2, train_attributes_2.shape)\n",
    "\n",
    "test_seen_attributes_2 = np.matmul(test_seen_attributes, w2v_att)\n",
    "#test_seen_attributes_2[test_seen_attributes_2<0]=0\n",
    "print(test_seen_attributes_2, test_seen_attributes_2.shape)\n",
    "\n",
    "test_unseen_attributes_2 = np.matmul(test_unseen_attributes, w2v_att)\n",
    "#test_unseen_attributes_2[test_unseen_attributes_2<0]=0\n",
    "print(test_unseen_attributes_2, test_unseen_attributes_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1457e426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2706384  -1.16305626  0.75625063 ...  0.18971761 -0.66343482\n",
      "   0.1482837 ]\n",
      " [-0.62173911 -1.0948952   0.35231452 ... -0.32992326 -1.08363979\n",
      "  -0.53303461]\n",
      " [-0.84562576 -1.4363254   0.44401793 ...  0.45779263 -0.36082596\n",
      "  -0.53493833]\n",
      " ...\n",
      " [-0.56132055 -1.24568001  0.11558501 ... -0.36210222 -1.38948177\n",
      "  -0.83668712]\n",
      " [-0.41481943 -1.56490858  0.38805618 ...  0.26498422 -0.69976478\n",
      "  -0.70761112]\n",
      " [-0.51719859 -1.01032215  0.2811525  ...  0.31579891 -0.58841869\n",
      "  -0.44914017]] (50, 200)\n",
      "[[-0.2706384  -0.62173911 -0.84562576 ... -0.56132055 -0.41481943\n",
      "  -0.51719859]\n",
      " [-1.16305626 -1.0948952  -1.4363254  ... -1.24568001 -1.56490858\n",
      "  -1.01032215]\n",
      " [ 0.75625063  0.35231452  0.44401793 ...  0.11558501  0.38805618\n",
      "   0.2811525 ]\n",
      " ...\n",
      " [ 0.18971761 -0.32992326  0.45779263 ... -0.36210222  0.26498422\n",
      "   0.31579891]\n",
      " [-0.66343482 -1.08363979 -0.36082596 ... -1.38948177 -0.69976478\n",
      "  -0.58841869]\n",
      " [ 0.1482837  -0.53303461 -0.53493833 ... -0.83668712 -0.70761112\n",
      "  -0.44914017]] (200, 50)\n",
      "Signature for trainval: (200, 40)\n",
      "Signature for test seen: (200, 40)\n",
      "Signature for test unseen: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "# as labels range from 1 to 50, we have subtract 1\n",
    "attribute_2 = np.matmul(attribute, w2v_att)\n",
    "#attribute_2[attribute_2<0]=0\n",
    "print(attribute_2, attribute_2.shape)\n",
    "\n",
    "signature_2 = attribute_2.transpose()\n",
    "print(signature_2, signature_2.shape)#(50, 300)\n",
    "\n",
    "trainval_sig = signature_2[:, (unique_labels_trainval)-1]\n",
    "test_seen_sig = signature_2[:, (unique_labels_test_seen)-1]\n",
    "test_unseen_sig = signature_2[:, (unique_labels_test_unseen)-1]\n",
    "\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)\n",
    "print(\"Signature for test seen:\", test_seen_sig.shape)\n",
    "print(\"Signature for test unseen:\", test_unseen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "581a4c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34]\n",
      " [19]\n",
      " [34]\n",
      " ...\n",
      " [32]\n",
      " [16]\n",
      " [37]] (23527, 1)\n",
      "[[19]\n",
      " [39]\n",
      " [11]\n",
      " ...\n",
      " [20]\n",
      " [12]\n",
      " [22]] (5882, 1)\n",
      "[[4]\n",
      " [4]\n",
      " [4]\n",
      " ...\n",
      " [8]\n",
      " [8]\n",
      " [8]] (7913, 1)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] (40,)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] (40,)\n",
      "[0 1 2 3 4 5 6 7 8 9] (10,)\n"
     ]
    }
   ],
   "source": [
    "# by doing this modification, we are changing the range of trainval and test seen labels from 0 to 39 \n",
    "# and test unseen labels from 0 to 9\n",
    "\n",
    "k = 0\n",
    "new_labels_trainval = np.zeros((len(labels_trainval), 1), dtype = 'int')\n",
    "for labels in unique_labels_trainval:\n",
    "    new_labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "    \n",
    "print(new_labels_trainval, new_labels_trainval.shape)#(23527, 1)\n",
    "\n",
    "l = 0\n",
    "new_labels_test_seen = np.zeros((len(labels_test_seen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_seen:\n",
    "    new_labels_test_seen[labels_test_seen == labels] = l\n",
    "    l = l+1\n",
    "    \n",
    "print(new_labels_test_seen, new_labels_test_seen.shape)# (5882, 1)\n",
    "\n",
    "m = 0\n",
    "new_labels_test_unseen = np.zeros((len(labels_test_unseen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_unseen:\n",
    "    new_labels_test_unseen[labels_test_unseen == labels] = m\n",
    "    m = m+1  \n",
    "\n",
    "print(new_labels_test_unseen, new_labels_test_unseen.shape) #  (7913, 1)  \n",
    "\n",
    "\n",
    "print(np.unique(new_labels_trainval), np.unique(new_labels_trainval).shape)\n",
    "\n",
    "print(np.unique(new_labels_test_seen), np.unique(new_labels_test_seen).shape)\n",
    "\n",
    "print(np.unique(new_labels_test_unseen), np.unique(new_labels_test_unseen).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee1341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23527\n",
      "40\n",
      "5882\n",
      "40\n",
      "7913\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#params for trainval and test set\n",
    "m_trainval = new_labels_trainval.shape[0]# number of instances in training set: 23527\n",
    "print(m_trainval)\n",
    "\n",
    "z_trainval = len(unique_labels_trainval)# number of classes in training set: 40\n",
    "print(z_trainval)\n",
    "\n",
    "\n",
    "n_test_seen = new_labels_test_seen.shape[0]# 5882\n",
    "print(n_test_seen)\n",
    "\n",
    "z1_test_seen = len(unique_labels_test_seen)# 40\n",
    "print(z1_test_seen)\n",
    "\n",
    "\n",
    "n_test_unseen = new_labels_test_unseen.shape[0]# 7913\n",
    "print(n_test_unseen)\n",
    "\n",
    "z1_test_unseen = len(unique_labels_test_unseen)# 10\n",
    "print(z1_test_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "845b8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]] (23527, 40)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "gt_trainval = to_categorical(new_labels_trainval, z_trainval)\n",
    "\n",
    "print(gt_trainval, gt_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08067319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "200\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "input1_shape = trainval_vec.shape[1]\n",
    "print(input1_shape)\n",
    "\n",
    "attribute_shape = trainval_sig.shape[0]\n",
    "print(attribute_shape)\n",
    "\n",
    "output_shape = z_trainval\n",
    "print(output_shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3660b11b",
   "metadata": {},
   "source": [
    "Step 5 - Define 'model2' for attribute to class label mapping\n",
    "\t\t Define 'model1' for visual feature to class label mapping\n",
    "\t\t Train 'model2' and 'model1' through the iterative process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96792076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 40)                8040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,040\n",
      "Trainable params: 8,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "\n",
    "# define model2 for attribute to class label mapping\n",
    "\n",
    "input2 = Input(shape = attribute_shape)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(input2)\n",
    "\n",
    "model2 = Model(inputs = input2, outputs = output)\n",
    "\n",
    "sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "#opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b285876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2048)]            0         \n",
      "                                                                 \n",
      " intermediate (Dense)        (None, 200)               409800    \n",
      "                                                                 \n",
      " output (Dense)              (None, 40)                8040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 417,840\n",
      "Trainable params: 417,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model1 for resnet feature to class label mapping\n",
    "\n",
    "input1 = Input(shape = input1_shape)\n",
    "#inter_pre = Dense(512, name=\"intermediate_previous\", activation='relu')(input1)\n",
    "inter = Dense(attribute_shape, name = \"intermediate\", activation = 'linear')(input1)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(inter)\n",
    "\n",
    "model1 = Model(inputs = input1, outputs = output)\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model1.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fd4ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23527, 2048)\n",
      "(23527, 200)\n",
      "(23527, 40)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = trainval_vec\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = train_attributes_2\n",
    "print(trainval_input2.shape)\n",
    "\n",
    "trainval_output = gt_trainval\n",
    "print(trainval_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cec915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 16\n",
    "from sklearn.model_selection import train_test_split    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7be0da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(trainval_input1, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen1 = DataGenerator(X_train1, y_train1, batch_size)   \n",
    "#val_gen1 = DataGenerator(X_val1, y_val1, batch_size)\n",
    "\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(trainval_input2, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen2 = DataGenerator(X_train2, y_train2, batch_size)   \n",
    "#val_gen2 = DataGenerator(X_val2, y_val2, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f527990a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.10858611017465591 , validation acc: 1.0 , validation_loss: 0.11289621889591217\n",
      "model 1 is trained: training acc: 0.9642857313156128 , training loss: 0.3377602696418762 , validation acc: 0.8125 , validation_loss: 20.744136810302734\n",
      "micro average\n",
      "seen accuracy: 70.76782874161805 unseen accuracy: 43.64104085690827 harmonic mean: 53.98850135142707\n",
      "macro average\n",
      "seen accuracy: 76.99761985719144 unseen accuracy: 34.133704031340834 harmonic mean: 47.29924696945452\n",
      "best accuracy micro seen accuracy: 70.76782874161805 unseen accuracy: 43.64104085690827 harmonic mean: 53.98850135142707\n",
      "best accuracy macro seen accuracy: 76.99761985719144 unseen accuracy: 34.133704031340834 harmonic mean: 47.29924696945452\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 1\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.05481189489364624 , validation acc: 1.0 , validation_loss: 0.04603008180856705\n",
      "model 1 is trained: training acc: 0.9732142686843872 , training loss: 0.4161294996738434 , validation acc: 0.875 , validation_loss: 26.65810203552246\n",
      "micro average\n",
      "seen accuracy: 69.98158745702796 unseen accuracy: 48.9902385486553 harmonic mean: 57.63405973729627\n",
      "macro average\n",
      "seen accuracy: 75.75654539272357 unseen accuracy: 42.310122583091115 harmonic mean: 54.29591225008946\n",
      "best accuracy micro seen accuracy: 69.98158745702796 unseen accuracy: 48.9902385486553 harmonic mean: 57.63405973729627\n",
      "best accuracy macro seen accuracy: 75.75654539272357 unseen accuracy: 42.310122583091115 harmonic mean: 54.29591225008946\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 2\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.04360330104827881 , validation acc: 1.0 , validation_loss: 0.028311096131801605\n",
      "model 1 is trained: training acc: 0.9821428656578064 , training loss: 0.9683012366294861 , validation acc: 0.9375 , validation_loss: 6.431089401245117\n",
      "micro average\n",
      "seen accuracy: 76.77138020563709 unseen accuracy: 50.7073334960813 harmonic mean: 61.07485502482316\n",
      "macro average\n",
      "seen accuracy: 83.21999319959198 unseen accuracy: 44.43321117148996 harmonic mean: 57.93401818223528\n",
      "best accuracy micro seen accuracy: 76.77138020563709 unseen accuracy: 50.7073334960813 harmonic mean: 61.07485502482316\n",
      "best accuracy macro seen accuracy: 83.21999319959198 unseen accuracy: 44.43321117148996 harmonic mean: 57.93401818223528\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 3\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.03089953400194645 , validation acc: 1.0 , validation_loss: 0.048582859337329865\n",
      "model 1 is trained: training acc: 0.9821428656578064 , training loss: 0.5514907836914062 , validation acc: 0.875 , validation_loss: 37.3511962890625\n",
      "micro average\n",
      "seen accuracy: 77.74675470303791 unseen accuracy: 47.59889980427423 harmonic mean: 59.04728012731398\n",
      "macro average\n",
      "seen accuracy: 83.78102686161169 unseen accuracy: 47.66839378238342 harmonic mean: 60.764162525263906\n",
      "best accuracy micro seen accuracy: 76.77138020563709 unseen accuracy: 50.7073334960813 harmonic mean: 61.07485502482316\n",
      "best accuracy macro seen accuracy: 83.78102686161169 unseen accuracy: 47.66839378238342 harmonic mean: 60.764162525263906\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 4\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.03114433027803898 , validation acc: 1.0 , validation_loss: 0.02790360525250435\n",
      "model 1 is trained: training acc: 0.9642857313156128 , training loss: 0.8941259384155273 , validation acc: 0.875 , validation_loss: 23.120553970336914\n",
      "micro average\n",
      "seen accuracy: 77.96288590362845 unseen accuracy: 49.46068480230302 harmonic mean: 60.52408835499352\n",
      "macro average\n",
      "seen accuracy: 82.60795647738864 unseen accuracy: 48.527739163401996 harmonic mean: 61.139376966242644\n",
      "best accuracy micro seen accuracy: 76.77138020563709 unseen accuracy: 50.7073334960813 harmonic mean: 61.07485502482316\n",
      "best accuracy macro seen accuracy: 82.60795647738864 unseen accuracy: 48.527739163401996 harmonic mean: 61.139376966242644\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 5\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.025701504200696945 , validation acc: 1.0 , validation_loss: 0.024854734539985657\n",
      "model 1 is trained: training acc: 0.9642857313156128 , training loss: 1.0922694206237793 , validation acc: 0.9375 , validation_loss: 68.5641860961914\n",
      "micro average\n",
      "seen accuracy: 77.0709923675584 unseen accuracy: 50.509198207317176 harmonic mean: 61.025054312691175\n",
      "macro average\n",
      "seen accuracy: 82.13192791567494 unseen accuracy: 49.159610767092126 harmonic mean: 61.50546559807221\n",
      "best accuracy micro seen accuracy: 76.77138020563709 unseen accuracy: 50.7073334960813 harmonic mean: 61.07485502482316\n",
      "best accuracy macro seen accuracy: 82.13192791567494 unseen accuracy: 49.159610767092126 harmonic mean: 61.50546559807221\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 6\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.0217590294778347 , validation acc: 1.0 , validation_loss: 0.01717493310570717\n",
      "model 1 is trained: training acc: 0.9642857313156128 , training loss: 1.118431568145752 , validation acc: 1.0 , validation_loss: 0.0\n",
      "micro average\n",
      "seen accuracy: 80.27453666655691 unseen accuracy: 50.46400154538675 harmonic mean: 61.97062316589762\n",
      "macro average\n",
      "seen accuracy: 85.58313498809929 unseen accuracy: 50.259067357512954 harmonic mean: 63.32830993258614\n",
      "best accuracy micro seen accuracy: 80.27453666655691 unseen accuracy: 50.46400154538675 harmonic mean: 61.97062316589762\n",
      "best accuracy macro seen accuracy: 85.58313498809929 unseen accuracy: 50.259067357512954 harmonic mean: 63.32830993258614\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 7\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.022159269079566002 , validation acc: 1.0 , validation_loss: 0.0206008143723011\n",
      "model 1 is trained: training acc: 0.9716981053352356 , training loss: 1.0401729345321655 , validation acc: 0.9375 , validation_loss: 2.8494911193847656\n",
      "micro average\n",
      "seen accuracy: 77.72062424310026 unseen accuracy: 44.03901458901527 harmonic mean: 56.221252588119185\n",
      "macro average\n",
      "seen accuracy: 82.74396463787828 unseen accuracy: 45.98761531656768 harmonic mean: 59.118323831354594\n",
      "best accuracy micro seen accuracy: 80.27453666655691 unseen accuracy: 50.46400154538675 harmonic mean: 61.97062316589762\n",
      "best accuracy macro seen accuracy: 85.58313498809929 unseen accuracy: 50.259067357512954 harmonic mean: 63.32830993258614\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 8\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.023846760392189026 , validation acc: 1.0 , validation_loss: 0.017370380461215973\n",
      "model 1 is trained: training acc: 0.9732142686843872 , training loss: 1.150155782699585 , validation acc: 0.9375 , validation_loss: 4.815464019775391\n",
      "micro average\n",
      "seen accuracy: 81.27819269576109 unseen accuracy: 50.95019850807741 harmonic mean: 62.63617086353207\n",
      "macro average\n",
      "seen accuracy: 86.6371982318939 unseen accuracy: 46.70794894477442 harmonic mean: 60.694309728005635\n",
      "best accuracy micro seen accuracy: 81.27819269576109 unseen accuracy: 50.95019850807741 harmonic mean: 62.63617086353207\n",
      "best accuracy macro seen accuracy: 85.58313498809929 unseen accuracy: 50.259067357512954 harmonic mean: 63.32830993258614\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 9\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.021167965605854988 , validation acc: 1.0 , validation_loss: 0.016164422035217285\n",
      "model 1 is trained: training acc: 1.0 , training loss: 1.0643680603550365e-08 , validation acc: 1.0 , validation_loss: 0.0\n",
      "micro average\n",
      "seen accuracy: 80.80024318846084 unseen accuracy: 48.26813157622208 harmonic mean: 60.43427410815358\n",
      "macro average\n",
      "seen accuracy: 86.77320639238354 unseen accuracy: 45.747504107165426 harmonic mean: 59.909995967626365\n",
      "best accuracy micro seen accuracy: 81.27819269576109 unseen accuracy: 50.95019850807741 harmonic mean: 62.63617086353207\n",
      "best accuracy macro seen accuracy: 85.58313498809929 unseen accuracy: 50.259067357512954 harmonic mean: 63.32830993258614\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iteration = 10\n",
    "epochs1 = 100\n",
    "epochs2 = 200\n",
    "\n",
    "best_performance_micro = [0, 0, 0]\n",
    "best_performance_macro = [0, 0, 0]\n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/'\n",
    "name = 'model1_AWA2_att7_200_it10_100eph_adam_cce_16bch_1e-2lr_model2_lr-2_200'\n",
    "\n",
    "\n",
    "for i in range(iteration):\n",
    "    X_train2_it = X_train2[(len(X_train2)//iteration)*i:(len(X_train2)//iteration)*(i+1)]\n",
    "    X_val2_it = X_val2[(len(X_val2)//iteration)*i:(len(X_val2)//iteration)*(i+1)]\n",
    "    y_train2_it = y_train2[(len(y_train2)//iteration)*i:(len(y_train2)//iteration)*(i+1)]\n",
    "    y_val2_it = y_val2[(len(y_val2)//iteration)*i:(len(y_val2)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen2 = DataGenerator(X_train2_it, y_train2_it, batch_size)   \n",
    "    val_gen2 = DataGenerator(X_val2_it, y_val2_it, batch_size)\n",
    "\n",
    "    train_summary2 = model2.fit(train_gen2, epochs = epochs2, verbose = 0, callbacks = None, validation_data = val_gen2, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen2)//batch_size, \n",
    "                              validation_steps = len(val_gen2)//batch_size)\n",
    "\n",
    "    print(\"iteration:\", i)\n",
    "    print('model 2 is trained:', 'training acc:', train_summary2.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary2.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary2.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary2.history['val_loss'][-1])\n",
    "\n",
    "    weights_list2 = model2.get_weights()\n",
    "    #print(weights_list2)\n",
    "\n",
    "    model1.layers[-1].set_weights(weights_list2)\n",
    "\n",
    "    X_train1_it = X_train1[(len(X_train1)//iteration)*i:(len(X_train1)//iteration)*(i+1)]\n",
    "    X_val1_it = X_val1[(len(X_val1)//iteration)*i:(len(X_val1)//iteration)*(i+1)]\n",
    "    y_train1_it = y_train1[(len(y_train1)//iteration)*i:(len(y_train1)//iteration)*(i+1)]\n",
    "    y_val1_it = y_val1[(len(y_val1)//iteration)*i:(len(y_val1)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen1 = DataGenerator(X_train1_it, y_train1_it, batch_size)   \n",
    "    val_gen1 = DataGenerator(X_val1_it, y_val1_it, batch_size)\n",
    "    \n",
    "    for layer in model1.layers[2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    train_summary1 = model1.fit(train_gen1, epochs = epochs1, verbose = 0, callbacks = None, validation_data = val_gen1, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen1)//batch_size, \n",
    "                              validation_steps = len(val_gen1)//batch_size)\n",
    "    \n",
    "    #print(\"iteration:\", i)\n",
    "    print('model 1 is trained:', 'training acc:', train_summary1.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary1.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary1.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary1.history['val_loss'][-1])\n",
    "    \n",
    "    weights_list1 = Model(inputs = model1.input, outputs = model1.layers[-1].output).get_weights()\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, np.matmul(weights_list1[0], weights_list1[2])), test_seen_sig)\n",
    "    outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0]), test_seen_sig)\n",
    "    \n",
    "    preds_seen = np.array([np.argmax(output) for output in outputs_seen])\n",
    "    \n",
    "    cm_seen = confusion_matrix(new_labels_test_seen, preds_seen)\n",
    "    #print(cm)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_seen_micro = cm_seen.astype('float') / cm_seen.sum(axis=1)[:, np.newaxis]\n",
    "    #print(cm)\n",
    "    avg_seen_micro = (sum(cm_seen_micro.diagonal())/len(unique_labels_test_seen))*100\n",
    "\n",
    "    avg_seen_macro = (sum(cm_seen.diagonal())/len(new_labels_test_seen))*100\n",
    "    \n",
    "    #predictions\n",
    "    outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0]), test_unseen_sig)\n",
    "    \n",
    "    preds_unseen = np.array([np.argmax(output) for output in outputs_unseen])\n",
    "    \n",
    "    cm_unseen = confusion_matrix(new_labels_test_unseen, preds_unseen)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_unseen_micro = cm_unseen.astype('float') / cm_unseen.sum(axis=1)[:, np.newaxis]\n",
    "    avg_unseen_micro = (sum(cm_unseen_micro.diagonal())/len(unique_labels_test_unseen))*100\n",
    "\n",
    "    avg_unseen_macro = (sum(cm_unseen.diagonal())/len(new_labels_test_unseen))*100\n",
    "    \n",
    "    harmonic_micro = (2*avg_seen_micro*avg_unseen_micro) / (avg_seen_micro + avg_unseen_micro)\n",
    "    harmonic_macro = (2*avg_seen_macro*avg_unseen_macro) / (avg_seen_macro + avg_unseen_macro)\n",
    "    \n",
    "    print('micro average')\n",
    "    print('seen accuracy:', avg_seen_micro, 'unseen accuracy:', avg_unseen_micro, 'harmonic mean:', harmonic_micro)\n",
    "    \n",
    "    print('macro average')\n",
    "    print('seen accuracy:', avg_seen_macro, 'unseen accuracy:', avg_unseen_macro, 'harmonic mean:', harmonic_macro)\n",
    "    \n",
    "    if harmonic_micro > best_performance_micro[2]:\n",
    "        best_performance_micro = [avg_seen_micro, avg_unseen_micro, harmonic_micro]\n",
    "        model1.save_weights(save_path + 'bw_micro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    if harmonic_macro > best_performance_macro[2]:\n",
    "        best_performance_macro = [avg_seen_macro, avg_unseen_macro, harmonic_macro]\n",
    "        model1.save_weights(save_path + 'bw_macro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    print('best accuracy micro','seen accuracy:', best_performance_micro[0], 'unseen accuracy:', best_performance_micro[1], 'harmonic mean:', best_performance_micro[2])\n",
    "    print('best accuracy macro', 'seen accuracy:', best_performance_macro[0], 'unseen accuracy:', best_performance_macro[1], 'harmonic mean:', best_performance_macro[2])\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    weights_list3 = model1.get_weights()\n",
    "    model2.set_weights(weights_list3[2:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64ac151f",
   "metadata": {},
   "source": [
    "Step 6 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58bc47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (5882, 40)\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 16.9981 - accuracy: 0.9106\n",
      "184/184 [==============================] - 0s 887us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3823332"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_test_seen = to_categorical(new_labels_test_seen, z1_test_seen)\n",
    "\n",
    "print(gt_test_seen, gt_test_seen.shape)\n",
    "\n",
    "model1.evaluate(test_seen_vec, gt_test_seen)\n",
    "\n",
    "p = model1.predict(test_seen_vec)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "cce(gt_test_seen, p).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04e03392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 0.9898\n",
      "184/184 [==============================] - 0s 627us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07459717"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(test_seen_attributes_2, gt_test_seen)\n",
    "\n",
    "p = model2.predict(test_seen_attributes_2)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "cce(gt_test_seen, p).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a67afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.02000000000001"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(91.06+98.98)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "666f86d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.74484030837003"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_seen_updated = 95.02\n",
    "unseen_accuracy = 50.26\n",
    "(2*accuracy_seen_updated*unseen_accuracy) / (accuracy_seen_updated + unseen_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edbf58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
