{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6778b22",
   "metadata": {},
   "source": [
    "GZSL_CUB_4: New attributes, w2v is replaced by Bert (New attribute length = 312*embedding length, then transform it to length 512)\n",
    "\tStep 1 - Class wise continous attributes are extracted (for CUB: 200 categories, and each category has attribute vectors of length 312) \n",
    "\tStep 2 - Word vectors are extracted for each semantic attribute using pretrained language models (w2v, Bert, Bart, GPT) \n",
    "                 New attributes are formed by aggregating the continous attribute values and word vectors\n",
    "\tStep 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n",
    "\tStep 4 - Visual features are transformed into lower dimensional space using 'model00' (optional)\n",
    "\tStep 5 - New attribute vectors are transformed into a lower dimensional space using 'model0'\n",
    "\tStep 6 - Define 'model2' for attribute to class label mapping\n",
    "\t\t Define 'model1' for visual feature to class label mapping\n",
    "\t\t Train 'model2' and 'model1' through the iterative process\n",
    "\tStep 7 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bb0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'CUB'\n",
    "path = 'E:/Sushree/Dataset/data/xlsa17/data/'\n",
    "\n",
    "res101 = scipy.io.loadmat(path + dataset + '/res101.mat')\n",
    "att_splits = scipy.io.loadmat(path + dataset + '/att_splits.mat')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0422bcf",
   "metadata": {},
   "source": [
    "Step 1 - Class wise continous attributes are extracted (for CUB: 200 categories, and each category has attribute vectors of length 312) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6082508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 200)\n",
      "[[0.0106384  0.0106384  0.00709227 ... 0.00918617 0.02526198 0.02066889]\n",
      " [0.         0.01133243 0.00944369 ... 0.00266542 0.02132333 0.05863916]\n",
      " [0.         0.         0.00742474 ... 0.         0.00885258 0.01770516]\n",
      " ...\n",
      " [0.         0.00334966 0.         ... 0.00556558 0.         0.15027069]\n",
      " [0.         0.11184146 0.         ... 0.08207164 0.05836206 0.01823814]\n",
      " [0.04378019 0.02814441 0.         ... 0.06022509 0.07695428 0.06189801]] (200, 312)\n"
     ]
    }
   ],
   "source": [
    "signature = att_splits['att']\n",
    "#signature = att_splits['original_att']\n",
    "#signature = signature/100\n",
    "print(signature.shape) #(312, 200)\n",
    "\n",
    "attribute = signature.transpose()\n",
    "attribute[attribute<0] = 0\n",
    "print(attribute, attribute.shape)#(200, 312)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec3f2dbd",
   "metadata": {},
   "source": [
    "Step 2 - Word vectors are extracted for each semantic attribute using pretrained language models (w2v, Bert, Bart, GPT) \n",
    "             New attributes are formed by aggregating the continous attribute values and word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab0196a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done replacing OOD words\n",
      "Done preprocessing attribute des\n",
      "has_bill_shape::curved_(up_or_down)\n",
      "has_bill_shape::dagger\n",
      "has_bill_shape::hooked\n",
      "has_bill_shape::needle\n",
      "has_bill_shape::hooked_seabird\n",
      "has_bill_shape::broad\n",
      "has_bill_shape::all-purpose\n",
      "has_bill_shape::cone\n",
      "has_bill_shape::specialized\n",
      "has_wing_color::blue\n",
      "has_wing_color::brown\n",
      "has_wing_color::iridescent\n",
      "has_wing_color::purple\n",
      "has_wing_color::rufous\n",
      "has_wing_color::gray\n",
      "has_wing_color::yellow\n",
      "has_wing_color::olive\n",
      "has_wing_color::green\n",
      "has_wing_color::pink\n",
      "has_wing_color::orange\n",
      "has_wing_color::black\n",
      "has_wing_color::white\n",
      "has_wing_color::red\n",
      "has_wing_color::buff\n",
      "has_upper parts_color::blue\n",
      "has_upper parts_color::brown\n",
      "has_upper parts_color::iridescent\n",
      "has_upper parts_color::purple\n",
      "has_upper parts_color::rufous\n",
      "has_upper parts_color::gray\n",
      "has_upper parts_color::yellow\n",
      "has_upper parts_color::olive\n",
      "has_upper parts_color::green\n",
      "has_upper parts_color::pink\n",
      "has_upper parts_color::orange\n",
      "has_upper parts_color::black\n",
      "has_upper parts_color::white\n",
      "has_upper parts_color::red\n",
      "has_upper parts_color::buff\n",
      "has_underparts_color::blue\n",
      "has_underparts_color::brown\n",
      "has_underparts_color::iridescent\n",
      "has_underparts_color::purple\n",
      "has_underparts_color::rufous\n",
      "has_underparts_color::gray\n",
      "has_underparts_color::yellow\n",
      "has_underparts_color::olive\n",
      "has_underparts_color::green\n",
      "has_underparts_color::pink\n",
      "has_underparts_color::orange\n",
      "has_underparts_color::black\n",
      "has_underparts_color::white\n",
      "has_underparts_color::red\n",
      "has_underparts_color::buff\n",
      "has_breast_pattern::solid\n",
      "has_breast_pattern::spotted\n",
      "has_breast_pattern::striped\n",
      "has_breast_pattern::multi-colored\n",
      "has_back_color::blue\n",
      "has_back_color::brown\n",
      "has_back_color::iridescent\n",
      "has_back_color::purple\n",
      "has_back_color::rufous\n",
      "has_back_color::gray\n",
      "has_back_color::yellow\n",
      "has_back_color::olive\n",
      "has_back_color::green\n",
      "has_back_color::pink\n",
      "has_back_color::orange\n",
      "has_back_color::black\n",
      "has_back_color::white\n",
      "has_back_color::red\n",
      "has_back_color::buff\n",
      "has_tail_shape::forked_tail\n",
      "has_tail_shape::rounded_tail\n",
      "has_tail_shape::notched_tail\n",
      "has_tail_shape::fan-shaped_tail\n",
      "has_tail_shape::pointed_tail\n",
      "has_tail_shape::squared_tail\n",
      "has_upper_tail_color::blue\n",
      "has_upper_tail_color::brown\n",
      "has_upper_tail_color::iridescent\n",
      "has_upper_tail_color::purple\n",
      "has_upper_tail_color::rufous\n",
      "has_upper_tail_color::gray\n",
      "has_upper_tail_color::yellow\n",
      "has_upper_tail_color::olive\n",
      "has_upper_tail_color::green\n",
      "has_upper_tail_color::pink\n",
      "has_upper_tail_color::orange\n",
      "has_upper_tail_color::black\n",
      "has_upper_tail_color::white\n",
      "has_upper_tail_color::red\n",
      "has_upper_tail_color::buff\n",
      "has_head_pattern::spotted\n",
      "has_head_pattern::malar\n",
      "has_head_pattern::crested\n",
      "has_head_pattern::masked\n",
      "has_head_pattern::unique_pattern\n",
      "has_head_pattern::eyebrow\n",
      "has_head_pattern::eyering\n",
      "has_head_pattern::plain\n",
      "has_head_pattern::eyeline\n",
      "has_head_pattern::striped\n",
      "has_head_pattern::capped\n",
      "has_breast_color::blue\n",
      "has_breast_color::brown\n",
      "has_breast_color::iridescent\n",
      "has_breast_color::purple\n",
      "has_breast_color::rufous\n",
      "has_breast_color::gray\n",
      "has_breast_color::yellow\n",
      "has_breast_color::olive\n",
      "has_breast_color::green\n",
      "has_breast_color::pink\n",
      "has_breast_color::orange\n",
      "has_breast_color::black\n",
      "has_breast_color::white\n",
      "has_breast_color::red\n",
      "has_breast_color::buff\n",
      "has_throat_color::blue\n",
      "has_throat_color::brown\n",
      "has_throat_color::iridescent\n",
      "has_throat_color::purple\n",
      "has_throat_color::rufous\n",
      "has_throat_color::gray\n",
      "has_throat_color::yellow\n",
      "has_throat_color::olive\n",
      "has_throat_color::green\n",
      "has_throat_color::pink\n",
      "has_throat_color::orange\n",
      "has_throat_color::black\n",
      "has_throat_color::white\n",
      "has_throat_color::red\n",
      "has_throat_color::buff\n",
      "has_eye_color::blue\n",
      "has_eye_color::brown\n",
      "has_eye_color::purple\n",
      "has_eye_color::rufous\n",
      "has_eye_color::gray\n",
      "has_eye_color::yellow\n",
      "has_eye_color::olive\n",
      "has_eye_color::green\n",
      "has_eye_color::pink\n",
      "has_eye_color::orange\n",
      "has_eye_color::black\n",
      "has_eye_color::white\n",
      "has_eye_color::red\n",
      "has_eye_color::buff\n",
      "has_bill_length::about_the_same_as_head\n",
      "has_bill_length::longer_than_head\n",
      "has_bill_length::shorter_than_head\n",
      "has_forehead_color::blue\n",
      "has_forehead_color::brown\n",
      "has_forehead_color::iridescent\n",
      "has_forehead_color::purple\n",
      "has_forehead_color::rufous\n",
      "has_forehead_color::gray\n",
      "has_forehead_color::yellow\n",
      "has_forehead_color::olive\n",
      "has_forehead_color::green\n",
      "has_forehead_color::pink\n",
      "has_forehead_color::orange\n",
      "has_forehead_color::black\n",
      "has_forehead_color::white\n",
      "has_forehead_color::red\n",
      "has_forehead_color::buff\n",
      "has_under_tail_color::blue\n",
      "has_under_tail_color::brown\n",
      "has_under_tail_color::iridescent\n",
      "has_under_tail_color::purple\n",
      "has_under_tail_color::rufous\n",
      "has_under_tail_color::gray\n",
      "has_under_tail_color::yellow\n",
      "has_under_tail_color::olive\n",
      "has_under_tail_color::green\n",
      "has_under_tail_color::pink\n",
      "has_under_tail_color::orange\n",
      "has_under_tail_color::black\n",
      "has_under_tail_color::white\n",
      "has_under_tail_color::red\n",
      "has_under_tail_color::buff\n",
      "has_nape_color::blue\n",
      "has_nape_color::brown\n",
      "has_nape_color::iridescent\n",
      "has_nape_color::purple\n",
      "has_nape_color::rufous\n",
      "has_nape_color::gray\n",
      "has_nape_color::yellow\n",
      "has_nape_color::olive\n",
      "has_nape_color::green\n",
      "has_nape_color::pink\n",
      "has_nape_color::orange\n",
      "has_nape_color::black\n",
      "has_nape_color::white\n",
      "has_nape_color::red\n",
      "has_nape_color::buff\n",
      "has_belly_color::blue\n",
      "has_belly_color::brown\n",
      "has_belly_color::iridescent\n",
      "has_belly_color::purple\n",
      "has_belly_color::rufous\n",
      "has_belly_color::gray\n",
      "has_belly_color::yellow\n",
      "has_belly_color::olive\n",
      "has_belly_color::green\n",
      "has_belly_color::pink\n",
      "has_belly_color::orange\n",
      "has_belly_color::black\n",
      "has_belly_color::white\n",
      "has_belly_color::red\n",
      "has_belly_color::buff\n",
      "has_wing_shape::rounded-wings\n",
      "has_wing_shape::pointed-wings\n",
      "has_wing_shape::broad-wings\n",
      "has_wing_shape::tapered-wings\n",
      "has_wing_shape::long-wings\n",
      "has_size::large_(16_-_32_in)\n",
      "has_size::small_(5_-_9_in)\n",
      "has_size::very_large_(32_-_72_in)\n",
      "has_size::medium_(9_-_16_in)\n",
      "has_size::very_small_(3_-_5_in)\n",
      "has_shape::upright-perching_water-like\n",
      "has_shape::chicken-like-marsh\n",
      "has_shape::long-legged-like\n",
      "has_shape::duck-like\n",
      "has_shape::owl-like\n",
      "has_shape::gull-like\n",
      "has_shape::hummingbird-like\n",
      "has_shape::pigeon-like\n",
      "has_shape::tree-clinging-like\n",
      "has_shape::hawk-like\n",
      "has_shape::sandpiper-like\n",
      "has_shape::upland-ground-like\n",
      "has_shape::swallow-like\n",
      "has_shape::perching-like\n",
      "has_back_pattern::solid\n",
      "has_back_pattern::spotted\n",
      "has_back_pattern::striped\n",
      "has_back_pattern::multi-colored\n",
      "has_tail_pattern::solid\n",
      "has_tail_pattern::spotted\n",
      "has_tail_pattern::striped\n",
      "has_tail_pattern::multi-colored\n",
      "has_belly_pattern::solid\n",
      "has_belly_pattern::spotted\n",
      "has_belly_pattern::striped\n",
      "has_belly_pattern::multi-colored\n",
      "has_primary_color::blue\n",
      "has_primary_color::brown\n",
      "has_primary_color::iridescent\n",
      "has_primary_color::purple\n",
      "has_primary_color::rufous\n",
      "has_primary_color::gray\n",
      "has_primary_color::yellow\n",
      "has_primary_color::olive\n",
      "has_primary_color::green\n",
      "has_primary_color::pink\n",
      "has_primary_color::orange\n",
      "has_primary_color::black\n",
      "has_primary_color::white\n",
      "has_primary_color::red\n",
      "has_primary_color::buff\n",
      "has_leg_color::blue\n",
      "has_leg_color::brown\n",
      "has_leg_color::iridescent\n",
      "has_leg_color::purple\n",
      "has_leg_color::rufous\n",
      "has_leg_color::gray\n",
      "has_leg_color::yellow\n",
      "has_leg_color::olive\n",
      "has_leg_color::green\n",
      "has_leg_color::pink\n",
      "has_leg_color::orange\n",
      "has_leg_color::black\n",
      "has_leg_color::white\n",
      "has_leg_color::red\n",
      "has_leg_color::buff\n",
      "has_bill_color::blue\n",
      "has_bill_color::brown\n",
      "has_bill_color::iridescent\n",
      "has_bill_color::purple\n",
      "has_bill_color::rufous\n",
      "has_bill_color::gray\n",
      "has_bill_color::yellow\n",
      "has_bill_color::olive\n",
      "has_bill_color::green\n",
      "has_bill_color::pink\n",
      "has_bill_color::orange\n",
      "has_bill_color::black\n",
      "has_bill_color::white\n",
      "has_bill_color::red\n",
      "has_bill_color::buff\n",
      "has_crown_color::blue\n",
      "has_crown_color::brown\n",
      "has_crown_color::iridescent\n",
      "has_crown_color::purple\n",
      "has_crown_color::rufous\n",
      "has_crown_color::gray\n",
      "has_crown_color::yellow\n",
      "has_crown_color::olive\n",
      "has_crown_color::green\n",
      "has_crown_color::pink\n",
      "has_crown_color::orange\n",
      "has_crown_color::black\n",
      "has_crown_color::white\n",
      "has_crown_color::red\n",
      "has_crown_color::buff\n",
      "has_wing_pattern::solid\n",
      "has_wing_pattern::spotted\n",
      "has_wing_pattern::striped\n",
      "has_wing_pattern::multi-colored\n",
      "counter_err  0\n",
      "[[ 0.05212105  0.0075245   0.49733996 ... -0.33823124 -0.2336414\n",
      "  -0.08391333]\n",
      " [ 0.07815347 -0.01515857  0.07717857 ... -0.48531315 -0.01718637\n",
      "  -0.06311301]\n",
      " [ 0.38475081 -0.17769523  0.42568105 ... -0.38018307  0.05972182\n",
      "   0.13356039]\n",
      " ...\n",
      " [-0.13154849 -0.19325779 -0.19377798 ... -0.15703391 -0.24170311\n",
      "   0.11968013]\n",
      " [ 0.0707561  -0.31268418 -0.09783425 ... -0.23413777 -0.16987358\n",
      "   0.22994283]\n",
      " [ 0.27138266 -0.30414492  0.11182283 ... -0.10643208 -0.14129114\n",
      "   0.04606724]] (312, 768)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "print('Load pretrain w2v model')\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "#model = TFBertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "dim_w2v = 768\n",
    "\n",
    "#%%\n",
    "replace_word = [('spatulate','broad'),('upperparts','upper parts'),('grey','gray')]  # for CUB\n",
    "\n",
    "\n",
    "#For CUB\n",
    "path = 'E:/Sushree/Dataset/CUB_200_2011/CUB_200_2011/attributes/attributes.txt'\n",
    "df=pd.read_csv(path,sep=' ',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% filter\n",
    "new_des = [' '.join(i.split('_')) for i in des]\n",
    "new_des = [' '.join(i.split('-')) for i in new_des]\n",
    "new_des = [' '.join(i.split('::')) for i in new_des]\n",
    "new_des = [i.split('(')[0] for i in new_des]\n",
    "new_des = [i[4:] for i in new_des]\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('E:/Sushree/Dataset/CUB_200_2011/CUB_200_2011/attributes/new_des.csv')\n",
    "print('Done preprocessing attribute des')\n",
    "\n",
    "import pickle\n",
    "\n",
    "counter_err = 0\n",
    "counter = 0\n",
    "w2v_att = np.zeros((signature.shape[0], dim_w2v))\n",
    "for s in des:\n",
    "    print(s)\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    encoded_input = tokenizer(s, return_tensors='tf')\n",
    "    length = encoded_input.input_ids.shape[1]\n",
    "    #print(length)\n",
    "    for i in range(length-2):\n",
    "        try:\n",
    "            w2v = w2v + model(encoded_input).last_hidden_state[:, i+1, :]\n",
    "        #print(model(encoded_input).last_hidden_state[:, i+1, :][:,1:2])\n",
    "        #print(w2v[:, 1:2])\n",
    "        #print(w2v.shape)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            counter_err += 1\n",
    "    w2v = w2v / (length - 2)\n",
    "    #print(w2v[:, 1:2])\n",
    "    w2v_att[counter] = w2v\n",
    "    counter = counter + 1\n",
    "\n",
    "print('counter_err ',counter_err)\n",
    "\n",
    "print(w2v_att, w2v_att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5facf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.54484646e-04  8.00486601e-05  5.29090167e-03 ... -2.19983320e-03\n",
      "  -2.92033137e-03  9.52158774e-04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -6.24108737e-03\n",
      "  -8.28519330e-03  2.70134395e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.88439686e-03\n",
      "  -2.50158207e-03  8.15627754e-04]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.59936223e-02\n",
      "  -2.12319176e-02  6.92255570e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.94112346e-03\n",
      "  -2.57688799e-03  8.40180854e-04]\n",
      " [ 2.28186951e-03  3.29424085e-04  2.17736367e-02 ... -6.58793416e-03\n",
      "  -8.74564075e-03  2.85147043e-03]] (200, 239616)\n"
     ]
    }
   ],
   "source": [
    "attribute_new = np.einsum('ij,jl->ijl', attribute, w2v_att)\n",
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], attribute_new.shape[1]* attribute_new.shape[2]])\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "#attribute_new[attribute_new<0] = 0\n",
    "#print(attribute_new, attribute_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237b819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1     2     4 ... 11724 11725 11726] 11726\n",
      "[    0     3     6 ... 11716 11717 11727] 11727\n",
      "[  178   179   180 ... 11785 11786 11787] 11787\n"
     ]
    }
   ],
   "source": [
    "# total number of instances or images = 11788: ranges from 0 to 11787\n",
    "\n",
    "trainval_loc = np.squeeze(att_splits['trainval_loc']-1) # -1: to consider the overflow problem\n",
    "print(np.unique(trainval_loc), np.max(np.unique(trainval_loc))) # smallest location: 1, largest location 11726\n",
    "\n",
    "test_seen_loc = np.squeeze(att_splits['test_seen_loc']-1)\n",
    "print(np.unique(test_seen_loc), np.max(np.unique(test_seen_loc))) # smallest location: 0, largest location 11727\n",
    "\n",
    "test_unseen_loc = np.squeeze(att_splits['test_unseen_loc']-1)\n",
    "print(np.unique(test_unseen_loc), np.max(np.unique(test_unseen_loc))) # smallest location: 178, largest location 11787\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c148c5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [[151]\n",
      " [151]\n",
      " [151]\n",
      " ...\n",
      " [150]\n",
      " [150]\n",
      " [150]] (11788, 1)\n",
      "unique_labels [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200] (200,)\n",
      "labels_trainval [[197]\n",
      " [198]\n",
      " [ 31]\n",
      " ...\n",
      " [ 65]\n",
      " [147]\n",
      " [ 22]] (7057, 1)\n",
      "unique_labels_trainval [  1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  20\n",
      "  22  23  24  25  26  27  28  30  31  32  33  35  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  51  52  53  54  55  57  58  59  60  61  63\n",
      "  64  65  66  67  70  71  73  74  75  76  77  78  81  82  83  84  85  86\n",
      "  89  90  92  93  94  96  97  99 101 102 103 105 106 107 109 110 111 112\n",
      " 113 114 115 117 118 119 121 123 126 127 128 130 131 132 133 134 135 136\n",
      " 137 138 140 143 144 145 146 147 148 149 151 153 154 155 156 158 161 162\n",
      " 163 164 165 168 169 170 172 173 175 177 178 180 181 183 184 186 188 190\n",
      " 194 196 197 198 199 200] (150,)\n",
      "labels_test_seen [[113]\n",
      " [107]\n",
      " [ 20]\n",
      " ...\n",
      " [136]\n",
      " [ 94]\n",
      " [180]] (1764, 1)\n",
      "unique_labels_test_seen [  1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  20\n",
      "  22  23  24  25  26  27  28  30  31  32  33  35  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  51  52  53  54  55  57  58  59  60  61  63\n",
      "  64  65  66  67  70  71  73  74  75  76  77  78  81  82  83  84  85  86\n",
      "  89  90  92  93  94  96  97  99 101 102 103 105 106 107 109 110 111 112\n",
      " 113 114 115 117 118 119 121 123 126 127 128 130 131 132 133 134 135 136\n",
      " 137 138 140 143 144 145 146 147 148 149 151 153 154 155 156 158 161 162\n",
      " 163 164 165 168 169 170 172 173 175 177 178 180 181 183 184 186 188 190\n",
      " 194 196 197 198 199 200] (150,)\n",
      "labels_test_unseen [[152]\n",
      " [152]\n",
      " [152]\n",
      " ...\n",
      " [150]\n",
      " [150]\n",
      " [150]] (2967, 1)\n",
      "unique_labels_test_unseen [  7  19  21  29  34  36  50  56  62  68  69  72  79  80  87  88  91  95\n",
      "  98 100 104 108 116 120 122 124 125 129 139 141 142 150 152 157 159 160\n",
      " 166 167 171 174 176 179 182 185 187 189 191 192 193 195] (50,)\n",
      "correct number of instances for training, test seen and test unseen categories\n",
      "Number of overlapping classes between trainval and test seen: 150\n",
      "Number of overlapping classes between trainval and test unseen: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 11788 x 1\n",
    "\n",
    "print('unique_labels', np.unique(labels), np.unique(labels).shape)# class labels range from 1 to 200, 200 classes\n",
    "\n",
    "# get the labels for trainval, test seen and test unseen sets\n",
    "\n",
    "labels_trainval = labels[trainval_loc]\n",
    "print('labels_trainval', labels_trainval, labels_trainval.shape)\n",
    "\n",
    "unique_labels_trainval = np.unique(labels_trainval) # labels min:1 max:200\n",
    "print('unique_labels_trainval', unique_labels_trainval, unique_labels_trainval.shape)# 150 classes\n",
    "\n",
    "\n",
    "labels_test_seen = labels[test_seen_loc]\n",
    "print('labels_test_seen', labels_test_seen, labels_test_seen.shape)\n",
    "\n",
    "unique_labels_test_seen = np.unique(labels_test_seen) # labels min:1 max:200\n",
    "print('unique_labels_test_seen', unique_labels_test_seen, unique_labels_test_seen.shape)# 200 classes\n",
    "\n",
    "\n",
    "labels_test_unseen = labels[test_unseen_loc]\n",
    "print('labels_test_unseen', labels_test_unseen, labels_test_unseen.shape)\n",
    "\n",
    "unique_labels_test_unseen = np.unique(labels_test_unseen) # labels min:7 max:195\n",
    "print('unique_labels_test_unseen', unique_labels_test_unseen, unique_labels_test_unseen.shape)# 50 classes\n",
    "\n",
    "\n",
    "if len(labels) == len(labels_trainval) + len(labels_test_seen) + len(labels_test_unseen):\n",
    "    print('correct number of instances for training, test seen and test unseen categories')\n",
    "    \n",
    "print(\"Number of overlapping classes between trainval and test seen:\",len(set(unique_labels_trainval).intersection(set(unique_labels_test_seen))))\n",
    "\n",
    "print(\"Number of overlapping classes between trainval and test unseen:\",len(set(unique_labels_trainval).intersection(set(unique_labels_test_unseen))))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca590b3e",
   "metadata": {},
   "source": [
    "Step 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88acec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for trainval: (7057, 2048)\n",
      "Features for test seen: (1764, 2048)\n",
      "Features for test unseen: (2967, 2048)\n"
     ]
    }
   ],
   "source": [
    "X_features = res101['features']\n",
    "\n",
    "# locations are already subtracted by 1, so they range from 0 to 11787\n",
    "trainval_vec = X_features[:, trainval_loc].transpose()\n",
    "test_seen_vec = X_features[:, test_seen_loc].transpose()\n",
    "test_unseen_vec = X_features[:, test_unseen_loc].transpose()\n",
    "\n",
    "print(\"Features for trainval:\", trainval_vec.shape) #(7057, 2048)\n",
    "print(\"Features for test seen:\", test_seen_vec.shape)# (1764, 2048)\n",
    "print(\"Features for test unseen:\", test_unseen_vec.shape) #(2967, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cc7e5",
   "metadata": {},
   "source": [
    "Step 4 - Visual features are transformed into lower dimensional space using 'model00' (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c87a4f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2048)]            0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 2048)              4196352   \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 1024)              2098176   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,294,528\n",
      "Trainable params: 0\n",
      "Non-trainable params: 6,294,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "import keras.backend as K\n",
    "\n",
    "inputt = Input(shape = trainval_vec.shape[1])\n",
    "hidden = Dense(2048, name=\"layer1\", activation='linear')(inputt)\n",
    "output = Dense(1024, name=\"layer3\", activation='linear')(hidden)\n",
    "\n",
    "model00 = Model(inputs = inputt, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model00.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for layer in model00.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model00.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c1b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 0s 2ms/step\n",
      "[[ 0.45416874  0.19463372  0.19167903 ...  0.03908704 -0.01939579\n",
      "  -0.4777062 ]\n",
      " [ 1.39617    -0.2577108   0.36600727 ...  0.2645291   0.00395067\n",
      "  -0.17885016]\n",
      " [ 0.21738106 -0.45402807 -1.224737   ...  0.49598676 -0.9398737\n",
      "  -0.22090249]\n",
      " ...\n",
      " [ 0.10661393 -1.1455524   0.5329402  ...  0.3324868   0.19960032\n",
      "  -0.5832321 ]\n",
      " [ 1.0479207  -0.9007417   0.9421424  ... -0.11639652  0.06683135\n",
      "   0.11531349]\n",
      " [ 0.9121201  -0.5548997   0.5800729  ...  0.5317137   0.6418545\n",
      "   0.4222274 ]] (7057, 1024)\n",
      "56/56 [==============================] - 0s 762us/step\n",
      "[[ 1.0471067  -0.57765985  0.01955435 ...  1.25606    -0.18194157\n",
      "  -0.19010037]\n",
      " [ 0.55126256 -0.3458984   0.73049915 ...  0.34805155  0.11943989\n",
      "  -0.57076   ]\n",
      " [ 0.60622334  0.4024467   0.8423179  ...  0.92681515 -0.16391273\n",
      "  -0.9854436 ]\n",
      " ...\n",
      " [ 1.0556808  -0.2903452   0.4205092  ... -0.1514939   0.0577338\n",
      "  -0.02499562]\n",
      " [ 0.10998748 -1.3449864   0.10097586 ...  1.2354566   0.45526606\n",
      "  -0.4629066 ]\n",
      " [-0.29280552 -0.17523545  0.10572381 ...  0.11439459 -0.28288054\n",
      "  -0.43089503]] (1764, 1024)\n",
      "93/93 [==============================] - 0s 752us/step\n",
      "[[ 0.6336422   0.23264442  0.93002504 ...  0.9158764   0.1688472\n",
      "  -0.22554177]\n",
      " [ 0.65409917  0.87363595  0.74818206 ...  1.5104663  -0.14252597\n",
      "   0.02331859]\n",
      " [ 0.41391635 -0.16520022 -0.7684865  ...  0.7159815   0.90016705\n",
      "  -0.75630224]\n",
      " ...\n",
      " [ 1.2730757  -0.8019265   0.36307618 ... -0.13938445 -0.36653405\n",
      "  -0.04123263]\n",
      " [ 1.0749522   0.00949001  0.8532952  ...  0.43570966  1.5289344\n",
      "   0.22681157]\n",
      " [ 1.3279076  -1.0668056  -0.16092432 ...  0.47701806  0.14141975\n",
      "  -0.04143962]] (2967, 1024)\n"
     ]
    }
   ],
   "source": [
    "trainval_vec = model00.predict(trainval_vec)\n",
    "print(trainval_vec, trainval_vec.shape)\n",
    "\n",
    "test_seen_vec = model00.predict(test_seen_vec)\n",
    "print(test_seen_vec, test_seen_vec.shape)\n",
    "\n",
    "test_unseen_vec = model00.predict(test_unseen_vec)\n",
    "print(test_unseen_vec, test_unseen_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e89686f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# attribute is defined for all 200 classes, so we cant use locations directly, instead we have to use labels \n",
    "# that range from 1 to 200, so we have to subtract 1\n",
    "\n",
    "#train_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "#for i in range(len(trainval_loc)):\n",
    "#    train_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "#print(train_attributes, train_attributes.shape)# (7057, 85)\n",
    "\n",
    "#test_seen_attributes = np.zeros((len(test_seen_loc), attribute_new.shape[1]))\n",
    "#for i in range(len(test_seen_loc)):\n",
    "#    test_seen_attributes[i] = attribute_new[int(labels_test_seen[i])-1]\n",
    "\n",
    "#print(test_seen_attributes, test_seen_attributes.shape)# (1764, 85)\n",
    "\n",
    "#test_unseen_attributes = np.zeros((len(test_unseen_loc), attribute_new.shape[1]))\n",
    "#for i in range(len(test_unseen_loc)):\n",
    "#    test_unseen_attributes[i] = attribute_new[int(labels_test_unseen[i])-1]\n",
    "\n",
    "#print(test_unseen_attributes, test_unseen_attributes.shape)# (2967, 85)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfae56f5",
   "metadata": {},
   "source": [
    "Step 5 - New attribute vectors are transformed into a lower dimensional space using 'model0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85741854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239616\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 239616)]          0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 2048)              490735616 \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 512)               1049088   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 491,784,704\n",
      "Trainable params: 0\n",
      "Non-trainable params: 491,784,704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "import keras.backend as K\n",
    "\n",
    "attribute_shape_new = attribute_new.shape[1]\n",
    "print(attribute_shape_new)\n",
    "\n",
    "# define model for attribute transformation\n",
    "\n",
    "inputt = Input(shape = attribute_shape_new)\n",
    "hidden = Dense(2048, name=\"layer1\", activation='linear')(inputt)\n",
    "#norm_layer = Lambda(lambda x: K.l2_normalize(x,axis=1))\n",
    "#batchnorm = BatchNormalization()(hidden)\n",
    "#batchnorm = norm_layer(hidden)\n",
    "#hidden2 = Dense(1024, name=\"layer2\", activation='linear')(hidden)\n",
    "output = Dense(512, name=\"layer3\", activation='linear')(hidden)\n",
    "\n",
    "model0 = Model(inputs = inputt, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model0.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for layer in model0.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d6ebf9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_attributes_2 = model0.predict(train_attributes)\n",
    "#print(train_attributes_2, train_attributes_2.shape)\n",
    "\n",
    "#test_seen_attributes_2 = model0.predict(test_seen_attributes)\n",
    "#print(test_seen_attributes_2, test_seen_attributes_2.shape)\n",
    "\n",
    "#test_unseen_attributes_2 = model0.predict(test_unseen_attributes)\n",
    "#print(test_unseen_attributes_2, test_unseen_attributes_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1457e426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 13ms/step\n",
      "[[-0.0229471   0.07510577 -0.03159445 ... -0.00481179 -0.02753432\n",
      "  -0.02307973]\n",
      " [-0.04022178  0.03333969 -0.01334414 ...  0.00528593 -0.07030255\n",
      "  -0.02019609]\n",
      " [-0.07379604  0.01190492 -0.00020688 ... -0.04179192 -0.02175125\n",
      "   0.00319578]\n",
      " ...\n",
      " [-0.04659723 -0.02292373  0.01543862 ... -0.02240662  0.00034931\n",
      "  -0.02164221]\n",
      " [-0.02966184  0.01231147 -0.04022513 ... -0.00497477  0.01425731\n",
      "  -0.03207773]\n",
      " [-0.01673997 -0.00583957 -0.03908215 ... -0.02292036  0.00414741\n",
      "  -0.03114864]] (200, 512)\n",
      "[[-0.0229471  -0.04022178 -0.07379604 ... -0.04659723 -0.02966184\n",
      "  -0.01673997]\n",
      " [ 0.07510577  0.03333969  0.01190492 ... -0.02292373  0.01231147\n",
      "  -0.00583957]\n",
      " [-0.03159445 -0.01334414 -0.00020688 ...  0.01543862 -0.04022513\n",
      "  -0.03908215]\n",
      " ...\n",
      " [-0.00481179  0.00528593 -0.04179192 ... -0.02240662 -0.00497477\n",
      "  -0.02292036]\n",
      " [-0.02753432 -0.07030255 -0.02175125 ...  0.00034931  0.01425731\n",
      "   0.00414741]\n",
      " [-0.02307973 -0.02019609  0.00319578 ... -0.02164221 -0.03207773\n",
      "  -0.03114864]] (512, 200)\n",
      "Signature for trainval: (512, 150)\n",
      "Signature for test seen: (512, 150)\n",
      "Signature for test unseen: (512, 50)\n"
     ]
    }
   ],
   "source": [
    "# as labels range from 1 to 200, we have subtract 1\n",
    "attribute_2 = model0.predict(attribute_new)\n",
    "print(attribute_2, attribute_2.shape)\n",
    "\n",
    "signature_2 = attribute_2.transpose()\n",
    "print(signature_2, signature_2.shape)#(200, 512)\n",
    "\n",
    "trainval_sig = signature_2[:, (unique_labels_trainval)-1]\n",
    "test_seen_sig = signature_2[:, (unique_labels_test_seen)-1]\n",
    "test_unseen_sig = signature_2[:, (unique_labels_test_unseen)-1]\n",
    "\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)\n",
    "print(\"Signature for test seen:\", test_seen_sig.shape)\n",
    "print(\"Signature for test unseen:\", test_unseen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6783e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08021165 -0.01808144 -0.00341598 ... -0.06926766 -0.01052626\n",
      "  -0.02498364]\n",
      " [-0.04659723 -0.02292373  0.01543862 ... -0.02240662  0.00034931\n",
      "  -0.02164221]\n",
      " [-0.02985013 -0.03224066 -0.01089176 ... -0.05014036  0.00728656\n",
      "   0.00701989]\n",
      " ...\n",
      " [-0.04889159  0.04968572 -0.01653836 ...  0.0155907  -0.011329\n",
      "   0.0158615 ]\n",
      " [-0.0106124  -0.00122564 -0.02376048 ... -0.0011317  -0.01826474\n",
      "  -0.04771242]\n",
      " [-0.00710925  0.00376841 -0.03390612 ...  0.00313603 -0.01409978\n",
      "  -0.06517885]] (7057, 512)\n",
      "[[-0.03623462 -0.02562167 -0.01932816 ... -0.04845976 -0.03028455\n",
      "   0.02654061]\n",
      " [-0.00575622  0.05044925 -0.00839194 ... -0.04786494 -0.04703646\n",
      "   0.02090461]\n",
      " [-0.02258146 -0.00405991 -0.03689091 ... -0.00748469 -0.019003\n",
      "  -0.0595558 ]\n",
      " ...\n",
      " [-0.02995334 -0.02076211 -0.03915599 ... -0.02585158 -0.0560357\n",
      "   0.01368153]\n",
      " [-0.04574835  0.00040772 -0.03283903 ... -0.05698505 -0.02141205\n",
      "  -0.00142536]\n",
      " [-0.03913843  0.03670869 -0.01809737 ... -0.04114115 -0.02414337\n",
      "   0.00059707]] (1764, 512)\n"
     ]
    }
   ],
   "source": [
    "train_attributes_2 = np.zeros((len(trainval_loc), attribute_2.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    train_attributes_2[i] = attribute_2[int(labels_trainval[i])-1]\n",
    "\n",
    "print(train_attributes_2, train_attributes_2.shape)# (7057, 85)\n",
    "\n",
    "test_seen_attributes_2 = np.zeros((len(test_seen_loc), attribute_2.shape[1]))\n",
    "for i in range(len(test_seen_loc)):\n",
    "    test_seen_attributes_2[i] = attribute_2[int(labels_test_seen[i])-1]\n",
    "\n",
    "print(test_seen_attributes_2, test_seen_attributes_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "581a4c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[146]\n",
      " [147]\n",
      " [ 26]\n",
      " ...\n",
      " [ 55]\n",
      " [115]\n",
      " [ 18]] (7057, 1)\n",
      "[[ 90]\n",
      " [ 85]\n",
      " [ 17]\n",
      " ...\n",
      " [107]\n",
      " [ 76]\n",
      " [137]] (1764, 1)\n",
      "[[32]\n",
      " [32]\n",
      " [32]\n",
      " ...\n",
      " [31]\n",
      " [31]\n",
      " [31]] (2967, 1)\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149] (150,)\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149] (150,)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49] (50,)\n"
     ]
    }
   ],
   "source": [
    "# by doing this modification, we are changing the range of trainval and test seen labels from 0 to 149 \n",
    "# and test unseen labels from 0 to 49\n",
    "\n",
    "k = 0\n",
    "new_labels_trainval = np.zeros((len(labels_trainval), 1), dtype = 'int')\n",
    "for labels in unique_labels_trainval:\n",
    "    new_labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "    \n",
    "print(new_labels_trainval, new_labels_trainval.shape)#(7057, 1)\n",
    "\n",
    "l = 0\n",
    "new_labels_test_seen = np.zeros((len(labels_test_seen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_seen:\n",
    "    new_labels_test_seen[labels_test_seen == labels] = l\n",
    "    l = l+1\n",
    "    \n",
    "print(new_labels_test_seen, new_labels_test_seen.shape)# (1764, 1)\n",
    "\n",
    "m = 0\n",
    "new_labels_test_unseen = np.zeros((len(labels_test_unseen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_unseen:\n",
    "    new_labels_test_unseen[labels_test_unseen == labels] = m\n",
    "    m = m+1  \n",
    "\n",
    "print(new_labels_test_unseen, new_labels_test_unseen.shape) #  (2967, 1)  \n",
    "\n",
    "\n",
    "print(np.unique(new_labels_trainval), np.unique(new_labels_trainval).shape)\n",
    "\n",
    "print(np.unique(new_labels_test_seen), np.unique(new_labels_test_seen).shape)\n",
    "\n",
    "print(np.unique(new_labels_test_unseen), np.unique(new_labels_test_unseen).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fee1341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7057\n",
      "150\n",
      "1764\n",
      "150\n",
      "2967\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#params for trainval and test set\n",
    "m_trainval = new_labels_trainval.shape[0]# number of instances in training set: 7057\n",
    "print(m_trainval)\n",
    "\n",
    "z_trainval = len(unique_labels_trainval)# number of classes in training set: 150\n",
    "print(z_trainval)\n",
    "\n",
    "\n",
    "n_test_seen = new_labels_test_seen.shape[0]# 1764\n",
    "print(n_test_seen)\n",
    "\n",
    "z1_test_seen = len(unique_labels_test_seen)# 150\n",
    "print(z1_test_seen)\n",
    "\n",
    "\n",
    "n_test_unseen = new_labels_test_unseen.shape[0]# 2967\n",
    "print(n_test_unseen)\n",
    "\n",
    "z1_test_unseen = len(unique_labels_test_unseen)# 50\n",
    "print(z1_test_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "845b8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (7057, 150)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "gt_trainval = to_categorical(new_labels_trainval, z_trainval)\n",
    "\n",
    "print(gt_trainval, gt_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08067319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "512\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "input1_shape = trainval_vec.shape[1]\n",
    "print(input1_shape)\n",
    "\n",
    "attribute_shape = trainval_sig.shape[0]\n",
    "print(attribute_shape)\n",
    "\n",
    "output_shape = z_trainval\n",
    "print(output_shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6889738",
   "metadata": {},
   "source": [
    "Step 6 - Define 'model2' for attribute to class label mapping\n",
    "     Define 'model1' for visual feature to class label mapping\n",
    "     Train 'model2' and 'model1' through the iterative process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96792076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 1, 512)]       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 150)               76950     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,950\n",
      "Trainable params: 76,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "\n",
    "# define model2 for attribute to class label mapping\n",
    "\n",
    "input2 = Input(shape = (1, 1, attribute_shape))\n",
    "flat = Flatten()(input2)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model2 = Model(inputs = input2, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model2.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b285876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1, 1, 1024)]      0         \n",
      "                                                                 \n",
      " intermediate (Conv1D)       (None, 1, 1, 512)         524800    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 150)               76950     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 601,750\n",
      "Trainable params: 601,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model1 for resnet feature to class label mapping\n",
    "\n",
    "input1 = Input(shape = (1, 1, input1_shape))\n",
    "#inter_pre = Dense(512, name=\"intermediate_previous\", activation='relu')(input1)\n",
    "inter = Conv1D(attribute_shape, kernel_size = 1, name = \"intermediate\", activation = 'linear')(input1)\n",
    "flat = Flatten()(inter)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model1 = Model(inputs = input1, outputs = output)\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model1.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fd4ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7057, 1, 1, 1024)\n",
      "(7057, 1, 1, 512)\n",
      "(7057, 150)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = np.reshape(trainval_vec, [trainval_vec.shape[0], 1, 1, trainval_vec.shape[1]])\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = np.reshape(train_attributes_2, [train_attributes_2.shape[0], 1, 1, train_attributes_2.shape[1]])\n",
    "print(trainval_input2.shape)\n",
    "\n",
    "trainval_output = gt_trainval\n",
    "print(trainval_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 8\n",
    "from sklearn.model_selection import train_test_split    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7be0da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(trainval_input1, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen1 = DataGenerator(X_train1, y_train1, batch_size)   \n",
    "#val_gen1 = DataGenerator(X_val1, y_val1, batch_size)\n",
    "\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(trainval_input2, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen2 = DataGenerator(X_train2, y_train2, batch_size)   \n",
    "#val_gen2 = DataGenerator(X_val2, y_val2, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f527990a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "model 2 is trained: training acc: 0.59375 , training loss: 3.0177924633026123 , validation acc: 0.375 , validation_loss: 3.5994279384613037\n",
      "model 1 is trained: training acc: 0.949999988079071 , training loss: 3.217773914337158 , validation acc: 0.1875 , validation_loss: 237.46218872070312\n",
      "micro average\n",
      "seen accuracy: 22.439306771659716 unseen accuracy: 25.39327213112344 harmonic mean: 23.82507639592737\n",
      "macro average\n",
      "seen accuracy: 21.71201814058957 unseen accuracy: 25.379170879676437 harmonic mean: 23.402807616325706\n",
      "best accuracy micro seen accuracy: 22.439306771659716 unseen accuracy: 25.39327213112344 harmonic mean: 23.82507639592737\n",
      "best accuracy macro seen accuracy: 21.71201814058957 unseen accuracy: 25.379170879676437 harmonic mean: 23.402807616325706\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 1\n",
      "model 2 is trained: training acc: 0.8500000238418579 , training loss: 1.7834725379943848 , validation acc: 0.75 , validation_loss: 2.035480499267578\n",
      "model 1 is trained: training acc: 0.890625 , training loss: 28.474254608154297 , validation acc: 0.4375 , validation_loss: 251.8747100830078\n",
      "micro average\n",
      "seen accuracy: 28.79481928746635 unseen accuracy: 33.449132299170905 harmonic mean: 30.947961860569116\n",
      "macro average\n",
      "seen accuracy: 28.117913832199548 unseen accuracy: 33.46814964610718 harmonic mean: 30.56063319273139\n",
      "best accuracy micro seen accuracy: 28.79481928746635 unseen accuracy: 33.449132299170905 harmonic mean: 30.947961860569116\n",
      "best accuracy macro seen accuracy: 28.117913832199548 unseen accuracy: 33.46814964610718 harmonic mean: 30.56063319273139\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 2\n",
      "model 2 is trained: training acc: 0.921875 , training loss: 1.5200403928756714 , validation acc: 0.8125 , validation_loss: 1.758758544921875\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.5625 , validation_loss: 150.23245239257812\n",
      "micro average\n",
      "seen accuracy: 31.841024389553823 unseen accuracy: 33.97202719258034 harmonic mean: 32.87202524112039\n",
      "macro average\n",
      "seen accuracy: 31.23582766439909 unseen accuracy: 34.07482305358948 harmonic mean: 32.59362106780105\n",
      "best accuracy micro seen accuracy: 31.841024389553823 unseen accuracy: 33.97202719258034 harmonic mean: 32.87202524112039\n",
      "best accuracy macro seen accuracy: 31.23582766439909 unseen accuracy: 34.07482305358948 harmonic mean: 32.59362106780105\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 3\n",
      "model 2 is trained: training acc: 0.9333333373069763 , training loss: 1.1344496011734009 , validation acc: 0.9375 , validation_loss: 1.1632615327835083\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.4375 , validation_loss: 336.2183837890625\n",
      "micro average\n",
      "seen accuracy: 36.99990477931655 unseen accuracy: 38.47451363541127 harmonic mean: 37.722803854370504\n",
      "macro average\n",
      "seen accuracy: 36.224489795918366 unseen accuracy: 38.52376137512639 harmonic mean: 37.33876255219297\n",
      "best accuracy micro seen accuracy: 36.99990477931655 unseen accuracy: 38.47451363541127 harmonic mean: 37.722803854370504\n",
      "best accuracy macro seen accuracy: 36.224489795918366 unseen accuracy: 38.52376137512639 harmonic mean: 37.33876255219297\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 4\n",
      "model 2 is trained: training acc: 0.90625 , training loss: 1.0884588956832886 , validation acc: 0.8125 , validation_loss: 1.0808579921722412\n",
      "model 1 is trained: training acc: 1.0 , training loss: 1.033733838085027e-06 , validation acc: 0.625 , validation_loss: 239.72055053710938\n",
      "micro average\n",
      "seen accuracy: 38.207674134144725 unseen accuracy: 38.67633793912126 harmonic mean: 38.44057761376475\n",
      "macro average\n",
      "seen accuracy: 37.188208616780045 unseen accuracy: 38.658577687900234 harmonic mean: 37.90914083321997\n",
      "best accuracy micro seen accuracy: 38.207674134144725 unseen accuracy: 38.67633793912126 harmonic mean: 38.44057761376475\n",
      "best accuracy macro seen accuracy: 37.188208616780045 unseen accuracy: 38.658577687900234 harmonic mean: 37.90914083321997\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 5\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.8285604119300842 , validation acc: 1.0 , validation_loss: 1.0285297632217407\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.375 , validation_loss: 304.6251220703125\n",
      "micro average\n",
      "seen accuracy: 37.547453526865304 unseen accuracy: 39.05189682740976 harmonic mean: 38.28490122909434\n",
      "macro average\n",
      "seen accuracy: 36.621315192743765 unseen accuracy: 39.06302662622178 harmonic mean: 37.802783933385136\n",
      "best accuracy micro seen accuracy: 38.207674134144725 unseen accuracy: 38.67633793912126 harmonic mean: 38.44057761376475\n",
      "best accuracy macro seen accuracy: 37.188208616780045 unseen accuracy: 38.658577687900234 harmonic mean: 37.90914083321997\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 6\n",
      "model 2 is trained: training acc: 0.96875 , training loss: 0.7774924039840698 , validation acc: 0.875 , validation_loss: 1.085176706314087\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.6875 , validation_loss: 207.55874633789062\n",
      "micro average\n",
      "seen accuracy: 40.50719857484564 unseen accuracy: 39.57720050432092 harmonic mean: 40.0367996338518\n",
      "macro average\n",
      "seen accuracy: 39.285714285714285 unseen accuracy: 39.5685877991237 harmonic mean: 39.426643667282356\n",
      "best accuracy micro seen accuracy: 40.50719857484564 unseen accuracy: 39.57720050432092 harmonic mean: 40.0367996338518\n",
      "best accuracy macro seen accuracy: 39.285714285714285 unseen accuracy: 39.5685877991237 harmonic mean: 39.426643667282356\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 7\n",
      "model 2 is trained: training acc: 0.96875 , training loss: 0.6150662899017334 , validation acc: 0.9375 , validation_loss: 0.6526581645011902\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.5 , validation_loss: 465.825439453125\n",
      "micro average\n",
      "seen accuracy: 39.42565778448132 unseen accuracy: 40.739661809873546 harmonic mean: 40.07189076022053\n",
      "macro average\n",
      "seen accuracy: 38.83219954648526 unseen accuracy: 40.74823053589484 harmonic mean: 39.76714922746815\n",
      "best accuracy micro seen accuracy: 39.42565778448132 unseen accuracy: 40.739661809873546 harmonic mean: 40.07189076022053\n",
      "best accuracy macro seen accuracy: 38.83219954648526 unseen accuracy: 40.74823053589484 harmonic mean: 39.76714922746815\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 8\n",
      "model 2 is trained: training acc: 0.984375 , training loss: 0.5860307812690735 , validation acc: 1.0 , validation_loss: 0.8347989320755005\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.375 , validation_loss: 587.9459228515625\n",
      "micro average\n",
      "seen accuracy: 41.02459653636123 unseen accuracy: 42.761375172434796 harmonic mean: 41.87498522750784\n",
      "macro average\n",
      "seen accuracy: 39.965986394557824 unseen accuracy: 42.77047522750253 harmonic mean: 41.320699423659285\n",
      "best accuracy micro seen accuracy: 41.02459653636123 unseen accuracy: 42.761375172434796 harmonic mean: 41.87498522750784\n",
      "best accuracy macro seen accuracy: 39.965986394557824 unseen accuracy: 42.77047522750253 harmonic mean: 41.320699423659285\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 9\n",
      "model 2 is trained: training acc: 0.984375 , training loss: 0.5485978126525879 , validation acc: 1.0 , validation_loss: 0.6008827090263367\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.4375 , validation_loss: 493.221435546875\n",
      "micro average\n",
      "seen accuracy: 42.14080255256727 unseen accuracy: 45.07114200540552 harmonic mean: 43.55674227183323\n",
      "macro average\n",
      "seen accuracy: 41.723356009070294 unseen accuracy: 45.16346477923829 harmonic mean: 43.375308303162726\n",
      "best accuracy micro seen accuracy: 42.14080255256727 unseen accuracy: 45.07114200540552 harmonic mean: 43.55674227183323\n",
      "best accuracy macro seen accuracy: 41.723356009070294 unseen accuracy: 45.16346477923829 harmonic mean: 43.375308303162726\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iteration = 10\n",
    "epochs1 = 200\n",
    "epochs2 = 200\n",
    "\n",
    "best_performance_micro = [0, 0, 0]\n",
    "best_performance_macro = [0, 0, 0]\n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/'\n",
    "name = 'model1_conv_CUB_vfT_BertattT_1024_lin_it10_200eph_adam_cce_16bch_1e-2lr_model2_adam_lr-2_200'\n",
    "      \n",
    "for i in range(iteration):\n",
    "    X_train2_it = X_train2[(len(X_train2)//iteration)*i:(len(X_train2)//iteration)*(i+1)]\n",
    "    X_val2_it = X_val2[(len(X_val2)//iteration)*i:(len(X_val2)//iteration)*(i+1)]\n",
    "    y_train2_it = y_train2[(len(y_train2)//iteration)*i:(len(y_train2)//iteration)*(i+1)]\n",
    "    y_val2_it = y_val2[(len(y_val2)//iteration)*i:(len(y_val2)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen2 = DataGenerator(X_train2_it, y_train2_it, batch_size)   \n",
    "    val_gen2 = DataGenerator(X_val2_it, y_val2_it, batch_size)\n",
    "\n",
    "    train_summary2 = model2.fit(train_gen2, epochs = epochs2, verbose = 0, callbacks = None, validation_data = val_gen2, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen2)//batch_size, \n",
    "                              validation_steps = len(val_gen2)//batch_size)\n",
    "\n",
    "    print(\"iteration:\", i)\n",
    "    print('model 2 is trained:', 'training acc:', train_summary2.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary2.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary2.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary2.history['val_loss'][-1])\n",
    "\n",
    "    weights_list2 = model2.get_weights()\n",
    "    #print(weights_list2)\n",
    "\n",
    "    model1.layers[-1].set_weights(weights_list2)\n",
    "\n",
    "    X_train1_it = X_train1[(len(X_train1)//iteration)*i:(len(X_train1)//iteration)*(i+1)]\n",
    "    X_val1_it = X_val1[(len(X_val1)//iteration)*i:(len(X_val1)//iteration)*(i+1)]\n",
    "    y_train1_it = y_train1[(len(y_train1)//iteration)*i:(len(y_train1)//iteration)*(i+1)]\n",
    "    y_val1_it = y_val1[(len(y_val1)//iteration)*i:(len(y_val1)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen1 = DataGenerator(X_train1_it, y_train1_it, batch_size)   \n",
    "    val_gen1 = DataGenerator(X_val1_it, y_val1_it, batch_size)\n",
    "    \n",
    "    for layer in model1.layers[2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    train_summary1 = model1.fit(train_gen1, epochs = epochs1, verbose = 0, callbacks = None, validation_data = val_gen1, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen1)//batch_size, \n",
    "                              validation_steps = len(val_gen1)//batch_size)\n",
    "    \n",
    "    #print(\"iteration:\", i)\n",
    "    print('model 1 is trained:', 'training acc:', train_summary1.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary1.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary1.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary1.history['val_loss'][-1])\n",
    "    \n",
    "    weights_list1 = Model(inputs = model1.input, outputs = model1.layers[-1].output).get_weights()\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, np.matmul(weights_list1[0], weights_list1[2])), test_seen_sig)\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0]), test_seen_sig)\n",
    "    \n",
    "    outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0][0]), test_seen_sig)\n",
    "    preds_seen = np.array([np.argmax(output) for output in outputs_seen])\n",
    "    \n",
    "    cm_seen = confusion_matrix(new_labels_test_seen, preds_seen)\n",
    "    #print(cm)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_seen_micro = cm_seen.astype('float') / cm_seen.sum(axis=1)[:, np.newaxis]\n",
    "    #print(cm)\n",
    "    avg_seen_micro = (sum(cm_seen_micro.diagonal())/len(unique_labels_test_seen))*100\n",
    "\n",
    "    avg_seen_macro = (sum(cm_seen.diagonal())/len(new_labels_test_seen))*100\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0]), test_unseen_sig)\n",
    "    outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0][0]), test_unseen_sig)\n",
    "    \n",
    "    preds_unseen = np.array([np.argmax(output) for output in outputs_unseen])\n",
    "    \n",
    "    cm_unseen = confusion_matrix(new_labels_test_unseen, preds_unseen)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_unseen_micro = cm_unseen.astype('float') / cm_unseen.sum(axis=1)[:, np.newaxis]\n",
    "    avg_unseen_micro = (sum(cm_unseen_micro.diagonal())/len(unique_labels_test_unseen))*100\n",
    "\n",
    "    avg_unseen_macro = (sum(cm_unseen.diagonal())/len(new_labels_test_unseen))*100\n",
    "    \n",
    "    harmonic_micro = (2*avg_seen_micro*avg_unseen_micro) / (avg_seen_micro + avg_unseen_micro)\n",
    "    harmonic_macro = (2*avg_seen_macro*avg_unseen_macro) / (avg_seen_macro + avg_unseen_macro)\n",
    "    \n",
    "    print('micro average')\n",
    "    print('seen accuracy:', avg_seen_micro, 'unseen accuracy:', avg_unseen_micro, 'harmonic mean:', harmonic_micro)\n",
    "    \n",
    "    print('macro average')\n",
    "    print('seen accuracy:', avg_seen_macro, 'unseen accuracy:', avg_unseen_macro, 'harmonic mean:', harmonic_macro)\n",
    "    \n",
    "    if harmonic_micro > best_performance_micro[2]:\n",
    "        best_performance_micro = [avg_seen_micro, avg_unseen_micro, harmonic_micro]\n",
    "        model1.save_weights(save_path + 'bw_micro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    if harmonic_macro > best_performance_macro[2]:\n",
    "        best_performance_macro = [avg_seen_macro, avg_unseen_macro, harmonic_macro]\n",
    "        model1.save_weights(save_path + 'bw_macro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    print('best accuracy micro','seen accuracy:', best_performance_micro[0], 'unseen accuracy:', best_performance_micro[1], 'harmonic mean:', best_performance_micro[2])\n",
    "    print('best accuracy macro', 'seen accuracy:', best_performance_macro[0], 'unseen accuracy:', best_performance_macro[1], 'harmonic mean:', best_performance_macro[2])\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    weights_list3 = model1.get_weights()\n",
    "    model2.set_weights(weights_list3[2:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9ec0bcd",
   "metadata": {},
   "source": [
    "Step 7 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58bc47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (1764, 150)\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 355.0552 - accuracy: 0.5544\n",
      "cce =  7.106239\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.7914 - accuracy: 0.8917\n",
      "cce =  0.79142594\n"
     ]
    }
   ],
   "source": [
    "gt_test_seen = to_categorical(new_labels_test_seen, z1_test_seen)\n",
    "\n",
    "print(gt_test_seen, gt_test_seen.shape)\n",
    "\n",
    "test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "res1 = model1.evaluate(test_seen_vec, gt_test_seen)\n",
    "\n",
    "p1 = model1.predict(test_seen_vec, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p1).numpy())\n",
    "\n",
    "\n",
    "test_seen_attributes_2 = np.reshape(test_seen_attributes_2, [test_seen_attributes_2.shape[0], 1, 1, test_seen_attributes_2.shape[1]])\n",
    "\n",
    "res2 = model2.evaluate(test_seen_attributes_2, gt_test_seen)\n",
    "\n",
    "p2 = model2.predict(test_seen_attributes_2, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p2).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe3280c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.775637333320034\n",
      "72.30725586414337\n",
      "55.596696301500465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_seen_updated = res1[1]*100\n",
    "unseen_accuracy = 45.16\n",
    "h = (2*accuracy_seen_updated*unseen_accuracy) / (accuracy_seen_updated + unseen_accuracy)\n",
    "print(h)\n",
    "\n",
    "\n",
    "accuracy_seen_updated2 = ((res1[1]*100)+(res2[1]*100))/2\n",
    "print(accuracy_seen_updated2)\n",
    "h = (2*accuracy_seen_updated2*unseen_accuracy) / (accuracy_seen_updated2 + unseen_accuracy)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67a434d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_seen_macro 0.7223954705903752 recall_seen_macro 0.728215698517169 f1_seen_macro 0.7045671900000485\n",
      "precision_seen_micro 0.7230725623582765 recall_seen_micro 0.7230725623582765 f1_seen_micro 0.7230725623582765\n",
      "precision_unseen_macro 0.4882035120134374 recall_unseen_macro 0.45071142005405535 f1_unseen_macro 0.421438299255895\n",
      "precision_unseen_micro 0.4516346477923829 recall_unseen_micro 0.4516346477923829 f1_unseen_micro 0.45163464779238294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pp1 = np.array([np.argmax(output) for output in p1])\n",
    "pp2 = np.array([np.argmax(output) for output in p2])\n",
    "\n",
    "seen_macro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'macro')\n",
    "seen_macro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'macro')\n",
    "print('precision_seen_macro', (seen_macro1[0] + seen_macro2[0])/2, 'recall_seen_macro', (seen_macro1[1] + seen_macro2[1])/2, 'f1_seen_macro', (seen_macro1[2] + seen_macro2[2])/2)\n",
    "\n",
    "\n",
    "seen_micro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'micro')\n",
    "seen_micro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'micro')\n",
    "print('precision_seen_micro', (seen_micro1[0] + seen_micro2[0])/2, 'recall_seen_micro', (seen_micro1[1] + seen_micro2[1])/2, 'f1_seen_micro', (seen_micro1[2] + seen_micro2[2])/2)\n",
    "\n",
    "unseen_macro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'macro')\n",
    "unseen_micro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'micro')\n",
    "\n",
    "print('precision_unseen_macro', unseen_macro[0], 'recall_unseen_macro', unseen_macro[1], 'f1_unseen_macro', unseen_macro[2])\n",
    "print('precision_unseen_micro', unseen_micro[0], 'recall_unseen_micro', unseen_micro[1], 'f1_unseen_micro', unseen_micro[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae461f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
