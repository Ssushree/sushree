{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bb0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'AWA2'\n",
    "path = 'E:/Sushree/Dataset/data/xlsa17/data/'\n",
    "\n",
    "res101 = scipy.io.loadmat(path + dataset + '/res101.mat')\n",
    "att_splits = scipy.io.loadmat(path + dataset + '/att_splits.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6082508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 50)\n",
      "[[0.         0.         0.         ... 0.00882092 0.03640974 0.03145501]\n",
      " [0.12045618 0.00426584 0.         ... 0.17996306 0.0618086  0.03495531]\n",
      " [0.26584459 0.20652363 0.         ... 0.05026822 0.04274552 0.04915256]\n",
      " ...\n",
      " [0.22516498 0.15266022 0.         ... 0.12733492 0.10009694 0.01771   ]\n",
      " [0.19613947 0.1966714  0.         ... 0.01787277 0.06698743 0.25883601]\n",
      " [0.03819588 0.08046548 0.10363715 ... 0.01479997 0.05250999 0.14194515]] (50, 85)\n"
     ]
    }
   ],
   "source": [
    "signature = att_splits['att']\n",
    "#signature = att_splits['original_att']\n",
    "#signature = signature/100\n",
    "print(signature.shape) #(85, 50)\n",
    "\n",
    "attribute = signature.transpose()\n",
    "attribute[attribute<0] = 0\n",
    "print(attribute, attribute.shape)#(50, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2293d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "print('Load pretrain w2v model')\n",
    "\n",
    "#model_name = 'word2vec-google-news-300' # length = 300\n",
    "#model = api.load(model_name)\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel, BartTokenizer, TFBartModel, GPT2Tokenizer, TFGPT2Model\n",
    "from transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\n",
    "\n",
    "# Bert model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # length 768\n",
    "#model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased') # 1024\n",
    "model = TFBertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Bart model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-base') # length 768\n",
    "#model = TFBartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-large') # length 1024\n",
    "#model = TFBartModel.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# GPT2 model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # length 768\n",
    "#model = TFGPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# GPT model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')# length 768\n",
    "#model = TFOpenAIGPTModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1832ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "#model = TFOpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "#encoded_input = tokenizer('sushree', return_tensors='tf')\n",
    "#print(encoded_input)\n",
    "#length = encoded_input.input_ids.shape[1]\n",
    "#print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25be9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer)\n",
    "#out = model(encoded_input).last_hidden_state[:, 2, :]\n",
    "#print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab0196a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done replacing OOD words\n",
      "Done preprocessing attribute des\n",
      "black\n",
      "white\n",
      "blue\n",
      "brown\n",
      "gray\n",
      "orange\n",
      "red\n",
      "yellow\n",
      "patches\n",
      "spots\n",
      "stripes\n",
      "furry\n",
      "hairless\n",
      "tough skin\n",
      "big\n",
      "small\n",
      "bulbous\n",
      "lean\n",
      "flippers\n",
      "hands\n",
      "hooves\n",
      "pads\n",
      "paws\n",
      "long leg\n",
      "long neck\n",
      "tail\n",
      "chew teeth\n",
      "meat teeth\n",
      "buckteeth\n",
      "strain teeth\n",
      "horns\n",
      "claws\n",
      "tusks\n",
      "smelly\n",
      "flys\n",
      "hops\n",
      "swims\n",
      "tunnels\n",
      "walks\n",
      "fast\n",
      "slow\n",
      "strong\n",
      "weak\n",
      "muscle\n",
      "bipedal\n",
      "quadrupedal\n",
      "active\n",
      "inactive\n",
      "nocturnal\n",
      "hibernate\n",
      "agility\n",
      "fish\n",
      "meat\n",
      "plankton\n",
      "vegetation\n",
      "insects\n",
      "forager\n",
      "grazer\n",
      "hunter\n",
      "scavenger\n",
      "skimmer\n",
      "stalker\n",
      "new world\n",
      "old world\n",
      "arctic\n",
      "coastal\n",
      "desert\n",
      "bush\n",
      "plains\n",
      "forest\n",
      "fields\n",
      "jungle\n",
      "mountains\n",
      "ocean\n",
      "ground\n",
      "water\n",
      "tree\n",
      "cave\n",
      "fierce\n",
      "timid\n",
      "smart\n",
      "group\n",
      "solitary\n",
      "nest spot\n",
      "domestic\n",
      "counter  85\n",
      "[[-0.09144888 -0.13553846 -0.12541848 ... -0.02989655 -0.38680694\n",
      "   0.24406666]\n",
      " [-0.29638147  0.14530022  0.20919298 ... -0.13767773 -0.49952391\n",
      "  -0.02497395]\n",
      " [-0.17159468  0.01720696 -0.01027012 ...  0.11106262  0.08405223\n",
      "   0.08059736]\n",
      " ...\n",
      " [-0.36489192  0.06430094 -0.17526677 ...  0.12331312 -0.36982071\n",
      "   0.07205145]\n",
      " [-0.55117345 -0.27137217 -0.57242227 ...  0.39393783  0.25118184\n",
      "   0.23015559]\n",
      " [-0.34205812 -0.0323039   0.13938682 ... -0.19463865 -0.57925606\n",
      "   0.4089973 ]] (85, 1024)\n",
      "[[ 0.          0.          0.         ... -0.00612236 -0.01822051\n",
      "   0.01286502]\n",
      " [-0.01101558 -0.01632644 -0.01510743 ... -0.00680365 -0.02024807\n",
      "   0.01429663]\n",
      " [-0.02431119 -0.03603217 -0.03334183 ... -0.00956699 -0.02847192\n",
      "   0.02010326]\n",
      " ...\n",
      " [-0.02059108 -0.03051851 -0.02823985 ... -0.00344705 -0.01025863\n",
      "   0.00724334]\n",
      " [-0.01793673 -0.02658444 -0.02459951 ... -0.05037949 -0.14993233\n",
      "   0.10586323]\n",
      " [-0.00349297 -0.00517701 -0.00479047 ... -0.02762801 -0.08222259\n",
      "   0.05805518]] (50, 87040)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim_w2v = 1024\n",
    "\n",
    "#%%\n",
    "replace_word = [('newworld','new world'),('oldworld','old world'),('nestspot','nest spot'),('toughskin','tough skin'),\n",
    "                ('longleg','long leg'), ('longneck', 'long neck'), ('chewteeth','chew teeth'),('meatteeth','meat teeth'),('strainteeth','strain teeth'),\n",
    "                ('quadrapedal','quadrupedal')]  # for AWA2\n",
    "\n",
    "\n",
    "#For AWA2\n",
    "path = 'E:/Sushree/Dataset/Animals_with_Attributes2/attribute/predicates.txt'\n",
    "df=pd.read_csv(path,sep='\\t',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('E:/Sushree/Dataset/Animals_with_Attributes2/attribute/new_des.csv')\n",
    "print('Done preprocessing attribute des')\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for word2vec\n",
    "# -----------------------------------------------------------------------------------\n",
    "#counter_err = 0\n",
    "#\n",
    "#all_w2v = []\n",
    "#for s in des:\n",
    "##    print(s)\n",
    "#    words = s.split(' ')\n",
    "#    if words[-1] == '':     #remove empty element\n",
    "#        words = words[:-1]\n",
    "#    w2v = np.zeros(dim_w2v)\n",
    "#    for w in words:\n",
    "#        try:\n",
    "#            w2v += model[w]\n",
    "#        except Exception as e:\n",
    "#            print(e)\n",
    "#            counter_err += 1\n",
    "#    w2v = w2v / len(words)  \n",
    "#    all_w2v.append(w2v[np.newaxis,:])\n",
    "#    \n",
    "#print('counter_err ',counter_err)\n",
    "\n",
    "#w2v_att = np.concatenate(all_w2v,axis=0)\n",
    "#print(w2v_att, w2v_att.shape)\n",
    "\n",
    "# for Bert attributes\n",
    "# -----------------------------------------------------------------------------------\n",
    "counter = 0\n",
    "w2v_att = np.zeros((signature.shape[0], dim_w2v))\n",
    "for s in des:\n",
    "    print(s)\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    encoded_input = tokenizer(s, return_tensors='tf')\n",
    "    length = encoded_input.input_ids.shape[1]\n",
    "    #print(length)\n",
    "    for i in range(length-2): # for Bert, Bart\n",
    "    #for i in range(length): # for GPT2\n",
    "        w2v = w2v + model(encoded_input).last_hidden_state[:, i+1, :] # for Bert, Bart\n",
    "        #w2v = w2v + model(encoded_input).last_hidden_state[:, i, :] # for GPT2\n",
    "        #print(model(encoded_input).last_hidden_state[:, i+1, :][:,1:2])\n",
    "        #print(w2v[:, 1:2])\n",
    "        #print(w2v.shape)\n",
    "\n",
    "    w2v = w2v / (length - 2)# for Bert, Bart\n",
    "    #w2v = w2v / length # for GPT2\n",
    "    #print(w2v[:, 1:2])\n",
    "    w2v_att[counter] = w2v\n",
    "    counter = counter + 1\n",
    "\n",
    "print('counter ',counter)\n",
    "print(w2v_att, w2v_att.shape)\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "attribute_new = np.einsum('ij,jl->ijl', attribute, w2v_att)\n",
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], attribute_new.shape[1]* attribute_new.shape[2]])\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "#attribute_new[attribute_new<0] = 0\n",
    "#print(attribute_new, attribute_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237b819e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4     5 ... 37318 37320 37321] 37321\n",
      "labels [[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " ...\n",
      " [38]\n",
      " [38]\n",
      " [38]] (37322, 1)\n",
      "unique_labels [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50] (50,)\n",
      "labels_trainval [[43]\n",
      " [22]\n",
      " [43]\n",
      " ...\n",
      " [40]\n",
      " [19]\n",
      " [46]] (23527, 1)\n",
      "unique_labels_trainval [ 1  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28\n",
      " 29 32 33 35 36 37 38 39 40 42 43 44 45 46 48 49] (40,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# total number of instances or images = 37322: ranges from 0 to 37321\n",
    "\n",
    "trainval_loc = np.squeeze(att_splits['trainval_loc']-1) # -1: to consider the overflow problem\n",
    "print(np.unique(trainval_loc), np.max(np.unique(trainval_loc))) # smallest location: 2, largest location 37321\n",
    "\n",
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 37322 x 1\n",
    "\n",
    "print('unique_labels', np.unique(labels), np.unique(labels).shape)# class labels range from 1 to 50, 50 classes\n",
    "\n",
    "# get the labels for trainval, test seen and test unseen sets\n",
    "\n",
    "labels_trainval = labels[trainval_loc]\n",
    "print('labels_trainval', labels_trainval, labels_trainval.shape)\n",
    "\n",
    "unique_labels_trainval = np.unique(labels_trainval) # labels min:1 max:49\n",
    "print('unique_labels_trainval', unique_labels_trainval, unique_labels_trainval.shape)# 40 classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88acec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for trainval: (23527, 2048)\n",
      "[[-0.00052664 -0.00078054 -0.00072226 ... -0.00223581 -0.00665391\n",
      "   0.00469815]\n",
      " [ 0.          0.          0.         ... -0.00277371 -0.00825473\n",
      "   0.00582844]\n",
      " [-0.00052664 -0.00078054 -0.00072226 ... -0.00223581 -0.00665391\n",
      "   0.00469815]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.0064743  -0.01926791\n",
      "   0.01360456]\n",
      " [-0.00076979 -0.00114092 -0.00105574 ... -0.00450891 -0.01341877\n",
      "   0.00947463]\n",
      " [-0.00354577 -0.00525526 -0.00486288 ... -0.05893629 -0.17539784\n",
      "   0.12384375]] (23527, 87040)\n"
     ]
    }
   ],
   "source": [
    "X_features = res101['features']\n",
    "\n",
    "# locations are already subtracted by 1, so they range from 0 to 37321\n",
    "trainval_vec = X_features[:, trainval_loc].transpose()\n",
    "print(\"Features for trainval:\", trainval_vec.shape) #(23527, 2048)\n",
    "\n",
    "# attribute is defined for all 50 classes, so we cant use locations directly, instead we have to use labels \n",
    "# that range from 1 to 50, so we have to subtract 1\n",
    "\n",
    "trainval_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    trainval_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "print(trainval_attributes, trainval_attributes.shape)# (23527, 85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc71b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87040\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 87040)]           0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 4096)              356519936 \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 2048)              8390656   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 364,910,592\n",
      "Trainable params: 0\n",
      "Non-trainable params: 364,910,592\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "[[-0.02441875 -0.04424096 -0.03395573 ... -0.00347495 -0.01306062\n",
      "  -0.04554387]\n",
      " [-0.08653303  0.10618562 -0.08255497 ...  0.08588152  0.10869464\n",
      "  -0.01979019]\n",
      " [-0.00494439  0.13001351 -0.00553821 ...  0.02849794  0.0241299\n",
      "  -0.14023863]\n",
      " ...\n",
      " [-0.07260355  0.05324867 -0.03911749 ...  0.05601903  0.11930488\n",
      "  -0.0277946 ]\n",
      " [-0.06891863  0.0634909  -0.06055539 ...  0.02131391  0.0067736\n",
      "  -0.02116128]\n",
      " [ 0.02327658  0.12030195 -0.00441829 ...  0.06180324 -0.01066447\n",
      "  -0.12026272]] (50, 2048)\n",
      "[[-0.02915581  0.03583307 -0.04544851 ...  0.08293824  0.05737066\n",
      "  -0.11550324]\n",
      " [-0.08981273  0.04723711 -0.03711235 ...  0.08584847  0.10291453\n",
      "  -0.07636312]\n",
      " [-0.02915581  0.03583307 -0.04544851 ...  0.08293824  0.05737066\n",
      "  -0.11550324]\n",
      " ...\n",
      " [-0.09000376 -0.01162128 -0.03335503 ...  0.03013332 -0.01220742\n",
      "  -0.03087169]\n",
      " [-0.01062597 -0.01813508  0.03244135 ...  0.03644632 -0.00822072\n",
      "   0.06604749]\n",
      " [-0.05316092  0.08254419 -0.0407727  ...  0.02681481  0.086616\n",
      "  -0.08526231]] (23527, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "import keras.backend as K\n",
    "\n",
    "attribute_shape_new = attribute_new.shape[1]\n",
    "print(attribute_shape_new)\n",
    "\n",
    "# define model for attribute transformation\n",
    "\n",
    "inputt = Input(shape = attribute_shape_new)\n",
    "hidden = Dense(4096, name=\"layer1\", activation='linear')(inputt)\n",
    "output = Dense(2048, name=\"layer3\", activation='linear')(hidden)\n",
    "\n",
    "model_transform_attribute = Model(inputs = inputt, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model_transform_attribute.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for layer in model_transform_attribute.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model_transform_attribute.summary()\n",
    "\n",
    "attribute_new = model_transform_attribute.predict(attribute_new)\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "\n",
    "trainval_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    trainval_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "print(trainval_attributes, trainval_attributes.shape)# (23527, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e99b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23527, 1, 1, 2048)\n",
      "(23527, 1, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = np.reshape(trainval_vec, [trainval_vec.shape[0], 1, 1, trainval_vec.shape[1]])\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = np.reshape(trainval_attributes, [trainval_attributes.shape[0], 1, 1, trainval_attributes.shape[1]])\n",
    "print(trainval_input2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d22fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(trainval_input1, trainval_input2, test_size = 0.2, random_state = 42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 16\n",
    "train_gen = DataGenerator(X_train, y_train, batch_size)   \n",
    "val_gen = DataGenerator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74a76d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2D)             (None, 1, 1, 2048)        4196352   \n",
      "                                                                 \n",
      " layer2 (Conv2D)             (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      " layer3 (Conv2D)             (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " layer4 (Conv2D)             (None, 1, 1, 2048)        2099200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,443,328\n",
      "Trainable params: 9,443,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "# -----------------------------------------------------------------------------------\n",
    "inputt = Input(shape = (1, 1, trainval_vec.shape[1]))\n",
    "hidden1 = Conv2D(2048, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "output1 = Conv2D(1024, 1, name=\"layer2\", activation='relu')(hidden1)\n",
    "hidden2 = Conv2D(1024, 1, name=\"layer3\", activation='relu')(output1)\n",
    "output2 = Conv2D(trainval_attributes.shape[1], 1, name=\"layer4\", activation='linear')(hidden2)\n",
    "\n",
    "embedding_model = Model(inputs = inputt, outputs = output2)\n",
    "\n",
    "# embedding model variants\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "#inputt = Input(shape = (1, 1, trainval_vec.shape[1]))\n",
    "#hidden1 = Conv2D(1024, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "#output1 = Conv2D(1024, 1, name=\"layer2\", activation='relu')(hidden1)\n",
    "#hidden2 = Conv2D(1024, 1, name=\"layer3\", activation='relu')(output1)\n",
    "#output2 = Conv2D(trainval_attributes.shape[1], 1, name=\"layer4\", activation='linear')(hidden2)\n",
    "\n",
    "#embedding_model = Model(inputs = inputt, outputs = output2)\n",
    "\n",
    "\n",
    "adam = Adam(learning_rate = 0.0001, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "embedding_model.compile(adam, loss = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.AUTO), metrics = ['accuracy'])\n",
    "#embedding_model.compile(adam, loss = cosine_loss, metrics = ['accuracy'])\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c663ebb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#CosineSimilarity is a number between -1 and 1. When it is a negative number between -1 and 0, 0 indicates orthogonality and \n",
    "#values closer to -1 indicate greater similarity. The values closer to 1 indicate greater dissimilarity. \n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/AWA2/BertAttribute/with_LE/Embedding Model/'\n",
    "name = 'embedding_model_Bert2_2048_1024_1024_AWA2_200eph_adam_cos_16bch_0.0001lr'\n",
    "\n",
    "file_path = save_path + 'bw_' + name + '.h5'\n",
    "\n",
    "#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath = file_path,\n",
    "#    monitor = 'val_accuracy',\n",
    "#    mode = 'max',\n",
    "#    save_best_only=True)\n",
    "\n",
    "#train_summary = embedding_model.fit(train_gen, epochs = 200, verbose = 1, callbacks = [model_checkpoint_callback], validation_data = val_gen, \n",
    "#                              shuffle = True, steps_per_epoch = len(train_gen)//batch_size, \n",
    "#                              validation_steps = len(val_gen)//batch_size)\n",
    "\n",
    "embedding_model.load_weights(save_path + 'bw_' + name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34cf8bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2D)             (None, 1, 1, 2048)        4196352   \n",
      "                                                                 \n",
      " layer2 (Conv2D)             (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,294,528\n",
      "Trainable params: 6,294,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "736/736 [==============================] - 1s 986us/step\n",
      "[[[[0.         0.646958   0.5096992  ... 1.2153103  0.66170496\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.06897884 0.         0.8608626  ... 2.8148031  2.0006714\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         1.8988659  ... 2.3388317  2.5795538\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.2842354  2.585671   ... 2.5207908  1.1933193\n",
      "    0.16591284]]]\n",
      "\n",
      "\n",
      " [[[0.         0.36422437 1.8683327  ... 1.1956234  1.0739142\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.2863497  ... 2.068993   1.3655761\n",
      "    0.        ]]]] (23527, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "embedding_model_0 = Model(inputs = embedding_model.input, outputs = embedding_model.layers[-3].output) # for embedding model\n",
    "#embedding_model_0 = Model(inputs = embedding_model.input, outputs = embedding_model.layers[-2].output) # for embedding model variants\n",
    "embedding_model_0.summary()\n",
    "\n",
    "trainval_vec = embedding_model_0.predict(trainval_input1)\n",
    "print(trainval_vec, trainval_vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d47c5d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2DTranspose)    (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,098,176\n",
      "Trainable params: 2,098,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "736/736 [==============================] - 1s 715us/step\n",
      "[[[[0.         0.         0.         ... 0.         0.03712775\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.04815801 ... 0.         0.\n",
      "    0.00713369]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.03712775\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.00537123\n",
      "    0.11065411]]]\n",
      "\n",
      "\n",
      " [[[0.00204216 0.         0.         ... 0.         0.05897139\n",
      "    0.12832287]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.03351367 ... 0.         0.\n",
      "    0.07115065]]]] (23527, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "inputt = Input(shape = (1, 1, trainval_attributes.shape[1]))\n",
    "hidden1 = Conv2DTranspose(1024, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "\n",
    "embedding_model_1 = Model(inputs = inputt, outputs = hidden1)\n",
    "embedding_model_1.summary()\n",
    "\n",
    "w_list = Model(inputs = embedding_model_0.layers[-2].output, outputs = embedding_model_0.output).get_weights()\n",
    "#print(weights_list2)\n",
    "\n",
    "embedding_model_1.set_weights([np.transpose(w_list[0], (0, 1, 3, 2)), w_list[1]])\n",
    "\n",
    "trainval_attributes = embedding_model_1.predict(trainval_input2)\n",
    "print(trainval_attributes, trainval_attributes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c3e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 997us/step\n",
      "[[[[0.         0.         0.         ... 0.         0.\n",
      "    0.09497304]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.0461937  ... 0.         0.03758804\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.16253974 0.         0.04259068 ... 0.         0.\n",
      "    0.03446209]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.05730452 ... 0.         0.01130792\n",
      "    0.07006513]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.13900313]]]\n",
      "\n",
      "\n",
      " [[[0.08072292 0.         0.04088895 ... 0.         0.00929634\n",
      "    0.        ]]]] (50, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], 1, 1, attribute_new.shape[1]])\n",
    "\n",
    "# as labels range from 1 to 50, we have subtract 1\n",
    "attribute_2 = embedding_model_1.predict(attribute_new)\n",
    "print(attribute_2, attribute_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ecaa989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.16253974 ... 0.         0.         0.08072292]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.0461937  0.04259068 ... 0.05730452 0.         0.04088895]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.03758804 0.         ... 0.01130792 0.         0.00929634]\n",
      " [0.09497304 0.         0.03446209 ... 0.07006513 0.13900313 0.        ]] (1024, 50)\n",
      "Signature for trainval: (1024, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "signature_2 = attribute_2.transpose()\n",
    "signature_2 = np.reshape(signature_2, [signature_2.shape[0], signature_2.shape[3]])\n",
    "print(signature_2, signature_2.shape)#(50, 300)\n",
    "\n",
    "trainval_sig = signature_2[:, (unique_labels_trainval)-1]\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "581a4c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34]\n",
      " [19]\n",
      " [34]\n",
      " ...\n",
      " [32]\n",
      " [16]\n",
      " [37]] (23527, 1)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] (40,)\n"
     ]
    }
   ],
   "source": [
    "# by doing this modification, we are changing the range of trainval and test seen labels from 0 to 39 \n",
    "# and test unseen labels from 0 to 9\n",
    "\n",
    "k = 0\n",
    "new_labels_trainval = np.zeros((len(labels_trainval), 1), dtype = 'int')\n",
    "for labels in unique_labels_trainval:\n",
    "    new_labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "    \n",
    "print(new_labels_trainval, new_labels_trainval.shape)#(23527, 1)\n",
    "print(np.unique(new_labels_trainval), np.unique(new_labels_trainval).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee1341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23527\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "#params for trainval and test set\n",
    "m_trainval = new_labels_trainval.shape[0]# number of instances in training set: 23527\n",
    "print(m_trainval)\n",
    "\n",
    "z_trainval = len(unique_labels_trainval)# number of classes in training set: 40\n",
    "print(z_trainval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "845b8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]] (23527, 40)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "gt_trainval = to_categorical(new_labels_trainval, z_trainval)\n",
    "\n",
    "print(gt_trainval, gt_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08067319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "input1_shape = trainval_vec.shape[3]\n",
    "print(input1_shape)\n",
    "\n",
    "attribute_shape = trainval_sig.shape[0]\n",
    "print(attribute_shape)\n",
    "\n",
    "output_shape = z_trainval\n",
    "print(output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96792076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1, 1, 1024)]      0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 40)                41000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,000\n",
      "Trainable params: 41,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "\n",
    "# define model2 for attribute to class label mapping\n",
    "\n",
    "input2 = Input(shape = (1, 1, attribute_shape))\n",
    "flat = Flatten()(input2)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model2 = Model(inputs = input2, outputs = output)\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "#opt = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model2.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b285876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1, 1, 1024)]      0         \n",
      "                                                                 \n",
      " intermediate (Conv1D)       (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 40)                41000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,090,600\n",
      "Trainable params: 1,090,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model1 for resnet feature to class label mapping\n",
    "\n",
    "input1 = Input(shape = (1,1, input1_shape))\n",
    "#inter_pre = Dense(512, name=\"intermediate_previous\", activation='relu')(input1)\n",
    "inter = Conv1D(attribute_shape, kernel_size = 1, name = \"intermediate\", activation = 'linear')(input1)\n",
    "flat = Flatten()(inter)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model1 = Model(inputs = input1, outputs = output)\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "#opt = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model1.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fd4ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23527, 1, 1, 1024)\n",
      "(23527, 1, 1, 1024)\n",
      "(23527, 40)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = trainval_vec\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = trainval_attributes\n",
    "print(trainval_input2.shape)\n",
    "\n",
    "trainval_output = gt_trainval\n",
    "print(trainval_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cec915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 16\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(trainval_input1, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen1 = DataGenerator(X_train1, y_train1, batch_size)   \n",
    "#val_gen1 = DataGenerator(X_val1, y_val1, batch_size)\n",
    "\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(trainval_input2, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen2 = DataGenerator(X_train2, y_train2, batch_size)   \n",
    "#val_gen2 = DataGenerator(X_val2, y_val2, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3fb3ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " ...\n",
      " [38]\n",
      " [38]\n",
      " [38]] (37322, 1)\n",
      "[    0     1     3 ... 37306 37307 37319] 37319\n",
      "labels_test_seen [[22]\n",
      " [49]\n",
      " [14]\n",
      " ...\n",
      " [25]\n",
      " [15]\n",
      " [27]] (5882, 1)\n",
      "unique_labels_test_seen [ 1  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28\n",
      " 29 32 33 35 36 37 38 39 40 42 43 44 45 46 48 49] (40,)\n",
      "Features for test seen: (5882, 2048)\n",
      "184/184 [==============================] - 0s 2ms/step\n",
      "(5882, 1, 1, 1024)\n",
      "[[[[0.         0.         0.04815801 ... 0.         0.\n",
      "    0.00713369]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.13900313]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.02527175\n",
      "    0.08780369]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.06821018\n",
      "    0.01246027]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.06597862\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.07640158]]]] (5882, 1, 1, 1024)\n",
      "Signature for test seen: (1024, 40)\n"
     ]
    }
   ],
   "source": [
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 37322 x 1\n",
    "\n",
    "test_seen_loc = np.squeeze(att_splits['test_seen_loc']-1)\n",
    "print(np.unique(test_seen_loc), np.max(np.unique(test_seen_loc))) # smallest location: 0, largest location 37319\n",
    "\n",
    "labels_test_seen = labels[test_seen_loc]\n",
    "print('labels_test_seen', labels_test_seen, labels_test_seen.shape)\n",
    "\n",
    "unique_labels_test_seen = np.unique(labels_test_seen) # labels min:1 max:49\n",
    "print('unique_labels_test_seen', unique_labels_test_seen, unique_labels_test_seen.shape)# 40 classes\n",
    "\n",
    "test_seen_vec = X_features[:, test_seen_loc].transpose()\n",
    "print(\"Features for test seen:\", test_seen_vec.shape)# (5882, 2048)\n",
    "\n",
    "test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "test_seen_vec = embedding_model_0.predict(test_seen_vec)\n",
    "print(test_seen_vec.shape)\n",
    "\n",
    "\n",
    "test_seen_attributes = np.zeros((len(test_seen_loc), attribute_2.shape[1], attribute_2.shape[2], attribute_2.shape[3]))\n",
    "for i in range(len(test_seen_loc)):\n",
    "    test_seen_attributes[i] = attribute_2[int(labels_test_seen[i])-1]\n",
    "\n",
    "print(test_seen_attributes, test_seen_attributes.shape)# (5882, 85)\n",
    "\n",
    "\n",
    "\n",
    "test_seen_sig = signature_2[:, (unique_labels_test_seen)-1]\n",
    "print(\"Signature for test seen:\", test_seen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a33b196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1046  1047  1048 ... 35288 35289 35290] 35290\n",
      "labels_test_unseen [[30]\n",
      " [30]\n",
      " [30]\n",
      " ...\n",
      " [47]\n",
      " [47]\n",
      " [47]] (7913, 1)\n",
      "unique_labels_test_unseen [ 7  9 23 24 30 31 34 41 47 50] (10,)\n",
      "Features for test unseen: (7913, 2048)\n",
      "248/248 [==============================] - 0s 1ms/step\n",
      "(7913, 1, 1, 1024)\n",
      "Signature for test unseen: (1024, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_unseen_loc = np.squeeze(att_splits['test_unseen_loc']-1)\n",
    "print(np.unique(test_unseen_loc), np.max(np.unique(test_unseen_loc))) # smallest location: 1046, largest location 35290\n",
    "\n",
    "labels_test_unseen = labels[test_unseen_loc]\n",
    "print('labels_test_unseen', labels_test_unseen, labels_test_unseen.shape)\n",
    "\n",
    "unique_labels_test_unseen = np.unique(labels_test_unseen) # labels min:7 max:50\n",
    "print('unique_labels_test_unseen', unique_labels_test_unseen, unique_labels_test_unseen.shape)# 10 classes\n",
    "\n",
    "\n",
    "test_unseen_vec = X_features[:, test_unseen_loc].transpose()\n",
    "print(\"Features for test unseen:\", test_unseen_vec.shape) #(7913, 2048)\n",
    "\n",
    "\n",
    "test_unseen_vec = np.reshape(test_unseen_vec, [test_unseen_vec.shape[0], 1, 1, test_unseen_vec.shape[1]])\n",
    "test_unseen_vec = embedding_model_0.predict(test_unseen_vec)\n",
    "print(test_unseen_vec.shape)\n",
    "\n",
    "#test_unseen_attributes = np.zeros((len(test_unseen_loc), attribute_new.shape[1]))\n",
    "#for i in range(len(test_unseen_loc)):\n",
    "#    test_unseen_attributes[i] = attribute_new[int(labels_test_unseen[i])-1]\n",
    "\n",
    "#print(test_unseen_attributes, test_unseen_attributes.shape)# (7913, 85)\n",
    "\n",
    "\n",
    "#test_unseen_attributes_2 = model0.predict(test_unseen_attributes)\n",
    "#print(test_unseen_attributes_2, test_unseen_attributes_2.shape)\n",
    "\n",
    "test_unseen_sig = signature_2[:, (unique_labels_test_unseen)-1]\n",
    "\n",
    "print(\"Signature for test unseen:\", test_unseen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00ef41ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19]\n",
      " [39]\n",
      " [11]\n",
      " ...\n",
      " [20]\n",
      " [12]\n",
      " [22]] (5882, 1)\n",
      "5882\n",
      "40\n",
      "[[4]\n",
      " [4]\n",
      " [4]\n",
      " ...\n",
      " [8]\n",
      " [8]\n",
      " [8]] (7913, 1)\n",
      "7913\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l = 0\n",
    "new_labels_test_seen = np.zeros((len(labels_test_seen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_seen:\n",
    "    new_labels_test_seen[labels_test_seen == labels] = l\n",
    "    l = l+1\n",
    "    \n",
    "print(new_labels_test_seen, new_labels_test_seen.shape)# (5882, 1)\n",
    "\n",
    "\n",
    "\n",
    "n_test_seen = new_labels_test_seen.shape[0]# 5882\n",
    "print(n_test_seen)\n",
    "\n",
    "z1_test_seen = len(unique_labels_test_seen)# 40\n",
    "print(z1_test_seen)\n",
    "\n",
    "\n",
    "m = 0\n",
    "new_labels_test_unseen = np.zeros((len(labels_test_unseen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_unseen:\n",
    "    new_labels_test_unseen[labels_test_unseen == labels] = m\n",
    "    m = m+1  \n",
    "\n",
    "print(new_labels_test_unseen, new_labels_test_unseen.shape) #  (7913, 1)  \n",
    "\n",
    "n_test_unseen = new_labels_test_unseen.shape[0]# 7913\n",
    "print(n_test_unseen)\n",
    "\n",
    "z1_test_unseen = len(unique_labels_test_unseen)# 10\n",
    "print(z1_test_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f527990a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro average\n",
      "seen accuracy: 3.0188047282536545 unseen accuracy: 17.025054046210496 harmonic mean: 5.128285349820147\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/AWA2/BertAttribute/with_LE/'\n",
    "name = 'model1_conv_AWA2_LE_Bert2attT_1024_lin_it15_200eph_adam_cce_16bch_1e-2lr_model2_adam_lr-2_200'\n",
    "\n",
    "model1.load_weights(save_path + 'bw_micro_' + name + '.h5')\n",
    "\n",
    "weights_list1 = Model(inputs = model1.input, outputs = model1.layers[-1].output).get_weights()\n",
    "\n",
    "#predictions\n",
    "outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0][0]), test_seen_sig)\n",
    "preds_seen = np.array([np.argmax(output) for output in outputs_seen])\n",
    "\n",
    "cm_seen = confusion_matrix(new_labels_test_seen, preds_seen)\n",
    "#print(cm)\n",
    "# Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "# then averaging these class-specific metrics)\n",
    "cm_seen_micro = cm_seen.astype('float') / cm_seen.sum(axis=1)[:, np.newaxis]\n",
    "#print(cm)\n",
    "avg_seen_micro = (sum(cm_seen_micro.diagonal())/len(unique_labels_test_seen))*100\n",
    "\n",
    "#predictions\n",
    "#outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0]), test_unseen_sig)\n",
    "outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0][0]), test_unseen_sig)\n",
    "\n",
    "preds_unseen = np.array([np.argmax(output) for output in outputs_unseen])\n",
    "\n",
    "cm_unseen = confusion_matrix(new_labels_test_unseen, preds_unseen)\n",
    "# Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "# then averaging these class-specific metrics)\n",
    "\n",
    "cm_unseen_micro = cm_unseen.astype('float') / cm_unseen.sum(axis=1)[:, np.newaxis]\n",
    "avg_unseen_micro = (sum(cm_unseen_micro.diagonal())/len(unique_labels_test_unseen))*100\n",
    "\n",
    "harmonic_micro = (2*avg_seen_micro*avg_unseen_micro) / (avg_seen_micro + avg_unseen_micro)\n",
    "\n",
    "print('micro average')\n",
    "print('seen accuracy:', avg_seen_micro, 'unseen accuracy:', avg_unseen_micro, 'harmonic mean:', harmonic_micro)\n",
    "\n",
    "\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "weights_list3 = model1.get_weights()\n",
    "model2.set_weights(weights_list3[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "485be21c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_weights() got an unexpected keyword argument 'overwrite'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbw_macro_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m outputs_seen \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(np\u001b[38;5;241m.\u001b[39mmatmul(test_seen_vec, weights_list1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]), test_seen_sig)\n\u001b[0;32m      4\u001b[0m preds_seen \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39margmax(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs_seen])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mTypeError\u001b[0m: load_weights() got an unexpected keyword argument 'overwrite'"
     ]
    }
   ],
   "source": [
    "model1.load_weights(save_path + 'bw_macro_' + name + '.h5')\n",
    "\n",
    "outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0][0]), test_seen_sig)\n",
    "preds_seen = np.array([np.argmax(output) for output in outputs_seen])\n",
    "\n",
    "cm_seen = confusion_matrix(new_labels_test_seen, preds_seen)\n",
    "\n",
    "avg_seen_macro = (sum(cm_seen.diagonal())/len(new_labels_test_seen))*100\n",
    "avg_unseen_macro = (sum(cm_unseen.diagonal())/len(new_labels_test_unseen))*100\n",
    "\n",
    "outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0][0]), test_unseen_sig)\n",
    "\n",
    "preds_unseen = np.array([np.argmax(output) for output in outputs_unseen])\n",
    "\n",
    "cm_unseen = confusion_matrix(new_labels_test_unseen, preds_unseen)\n",
    "\n",
    "harmonic_macro = (2*avg_seen_macro*avg_unseen_macro) / (avg_seen_macro + avg_unseen_macro)\n",
    "\n",
    "print('macro average')\n",
    "print('seen accuracy:', avg_seen_macro, 'unseen accuracy:', avg_unseen_macro, 'harmonic mean:', harmonic_macro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58bc47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (5882, 40)\n",
      "184/184 [==============================] - 1s 2ms/step - loss: 1295.4611 - accuracy: 0.0281\n",
      "cce =  15.646198\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 3.9205 - accuracy: 0.0000e+00\n",
      "cce =  3.92047\n"
     ]
    }
   ],
   "source": [
    "gt_test_seen = to_categorical(new_labels_test_seen, z1_test_seen)\n",
    "\n",
    "print(gt_test_seen, gt_test_seen.shape)\n",
    "\n",
    "#test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "res1 = model1.evaluate(test_seen_vec, gt_test_seen)\n",
    "\n",
    "p1 = model1.predict(test_seen_vec, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p1).numpy())\n",
    "\n",
    "#test_seen_attributes = np.reshape(train_attributes, [test_seen_attributes.shape[0], 1, train_attributes.shape[1]])\n",
    "#test_seen_attributes = np.reshape(test_seen_attributes, [test_seen_attributes.shape[0], 1, 1, test_seen_attributes.shape[1]])\n",
    "\n",
    "res2 = model2.evaluate(test_seen_attributes, gt_test_seen)\n",
    "\n",
    "p2 = model2.predict(test_seen_attributes, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p2).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy_seen_updated = res1[1]*100\n",
    "unseen_accuracy = 58.29\n",
    "h = (2*accuracy_seen_updated*unseen_accuracy) / (accuracy_seen_updated + unseen_accuracy)\n",
    "print(h)\n",
    "\n",
    "\n",
    "accuracy_seen_updated2 = ((res1[1]*100)+(res2[1]*100))/2\n",
    "print(accuracy_seen_updated2)\n",
    "h = (2*accuracy_seen_updated2*unseen_accuracy) / (accuracy_seen_updated2 + unseen_accuracy)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735fa6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pp1 = np.array([np.argmax(output) for output in p1])\n",
    "pp2 = np.array([np.argmax(output) for output in p2])\n",
    "\n",
    "seen_macro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'macro')\n",
    "seen_macro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'macro')\n",
    "print('precision_seen_macro', (seen_macro1[0] + seen_macro2[0])/2, 'recall_seen_macro', (seen_macro1[1] + seen_macro2[1])/2, 'f1_seen_macro', (seen_macro1[2] + seen_macro2[2])/2)\n",
    "\n",
    "\n",
    "seen_micro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'micro')\n",
    "seen_micro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'micro')\n",
    "print('precision_seen_micro', (seen_micro1[0] + seen_micro2[0])/2, 'recall_seen_micro', (seen_micro1[1] + seen_micro2[1])/2, 'f1_seen_micro', (seen_micro1[2] + seen_micro2[2])/2)\n",
    "\n",
    "unseen_macro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'macro')\n",
    "unseen_micro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'micro')\n",
    "\n",
    "print('precision_unseen_macro', unseen_macro[0], 'recall_unseen_macro', unseen_macro[1], 'f1_unseen_macro', unseen_macro[2])\n",
    "print('precision_unseen_micro', unseen_micro[0], 'recall_unseen_micro', unseen_micro[1], 'f1_unseen_micro', unseen_micro[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda60741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
