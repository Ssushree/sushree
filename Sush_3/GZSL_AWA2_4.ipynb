{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e42017e8",
   "metadata": {},
   "source": [
    "GZSL_AWA2_4: Experiments with New attributes, w2v is replaced by Bert (New attribute length = 85*embedding length, then transform it to length 512)\n",
    "\tStep 1 - Class wise continous attributes are extracted\n",
    "\tStep 2 - Word vectors are extracted for each semantic attribute using pretrained language models (Bert) \n",
    "                 New attributes are formed by aggregating the continous attribute values and word vectors\n",
    "\tStep 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n",
    "\tStep 4 - Visual features are transformed into lower dimensional space using 'model00' (optional)\n",
    "\tStep 5 - New attribute vectors are transformed into a lower dimensional space using 'model0'\n",
    "\tStep 6 - Define 'model2' for attribute to class label mapping\n",
    "\t\t Define 'model1' for visual feature to class label mapping\n",
    "\t\t Train 'model2' and 'model1' through the iterative process\n",
    "\tStep 7 - Evaluate for seen and unseen categories\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bb0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'AWA2'\n",
    "path = 'E:/Sushree/Dataset/data/xlsa17/data/'\n",
    "\n",
    "res101 = scipy.io.loadmat(path + dataset + '/res101.mat')\n",
    "att_splits = scipy.io.loadmat(path + dataset + '/att_splits.mat')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d850dce",
   "metadata": {},
   "source": [
    "Step 1 - Class wise continous attributes are extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6082508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 50)\n",
      "[[0.         0.         0.         ... 0.00882092 0.03640974 0.03145501]\n",
      " [0.12045618 0.00426584 0.         ... 0.17996306 0.0618086  0.03495531]\n",
      " [0.26584459 0.20652363 0.         ... 0.05026822 0.04274552 0.04915256]\n",
      " ...\n",
      " [0.22516498 0.15266022 0.         ... 0.12733492 0.10009694 0.01771   ]\n",
      " [0.19613947 0.1966714  0.         ... 0.01787277 0.06698743 0.25883601]\n",
      " [0.03819588 0.08046548 0.10363715 ... 0.01479997 0.05250999 0.14194515]] (50, 85)\n"
     ]
    }
   ],
   "source": [
    "signature = att_splits['att']\n",
    "#signature = att_splits['original_att']\n",
    "#signature = signature/100\n",
    "print(signature.shape) #(85, 50)\n",
    "\n",
    "attribute = signature.transpose()\n",
    "attribute[attribute<0] = 0\n",
    "print(attribute, attribute.shape)#(50, 85)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8090ce0e",
   "metadata": {},
   "source": [
    "Step 2 - Word vectors are extracted for each semantic attribute using pretrained language models (Bert) \n",
    "                 New attributes are formed by aggregating the continous attribute values and word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab0196a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done replacing OOD words\n",
      "Done preprocessing attribute des\n",
      "black\n",
      "white\n",
      "blue\n",
      "brown\n",
      "gray\n",
      "orange\n",
      "red\n",
      "yellow\n",
      "patches\n",
      "spots\n",
      "stripes\n",
      "furry\n",
      "hairless\n",
      "tough skin\n",
      "big\n",
      "small\n",
      "bulbous\n",
      "lean\n",
      "flippers\n",
      "hands\n",
      "hooves\n",
      "pads\n",
      "paws\n",
      "long leg\n",
      "long neck\n",
      "tail\n",
      "chew teeth\n",
      "meat teeth\n",
      "buckteeth\n",
      "strain teeth\n",
      "horns\n",
      "claws\n",
      "tusks\n",
      "smelly\n",
      "flys\n",
      "hops\n",
      "swims\n",
      "tunnels\n",
      "walks\n",
      "fast\n",
      "slow\n",
      "strong\n",
      "weak\n",
      "muscle\n",
      "bipedal\n",
      "quadrupedal\n",
      "active\n",
      "inactive\n",
      "nocturnal\n",
      "hibernate\n",
      "agility\n",
      "fish\n",
      "meat\n",
      "plankton\n",
      "vegetation\n",
      "insects\n",
      "forager\n",
      "grazer\n",
      "hunter\n",
      "scavenger\n",
      "skimmer\n",
      "stalker\n",
      "new world\n",
      "old world\n",
      "arctic\n",
      "coastal\n",
      "desert\n",
      "bush\n",
      "plains\n",
      "forest\n",
      "fields\n",
      "jungle\n",
      "mountains\n",
      "ocean\n",
      "ground\n",
      "water\n",
      "tree\n",
      "cave\n",
      "fierce\n",
      "timid\n",
      "smart\n",
      "group\n",
      "solitary\n",
      "nest spot\n",
      "domestic\n",
      "counter  85\n",
      "[[-0.09144888 -0.13553846 -0.12541848 ... -0.02989655 -0.38680694\n",
      "   0.24406666]\n",
      " [-0.29638147  0.14530022  0.20919298 ... -0.13767773 -0.49952391\n",
      "  -0.02497395]\n",
      " [-0.17159468  0.01720696 -0.01027012 ...  0.11106262  0.08405223\n",
      "   0.08059736]\n",
      " ...\n",
      " [-0.36489192  0.06430094 -0.17526677 ...  0.12331312 -0.36982071\n",
      "   0.07205145]\n",
      " [-0.55117345 -0.27137217 -0.57242227 ...  0.39393783  0.25118184\n",
      "   0.23015559]\n",
      " [-0.34205812 -0.0323039   0.13938682 ... -0.19463865 -0.57925606\n",
      "   0.4089973 ]] (85, 1024)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "print('Load pretrain w2v model')\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "dim_w2v = 1024\n",
    "\n",
    "#%%\n",
    "replace_word = [('newworld','new world'),('oldworld','old world'),('nestspot','nest spot'),('toughskin','tough skin'),\n",
    "                ('longleg','long leg'), ('longneck', 'long neck'), ('chewteeth','chew teeth'),('meatteeth','meat teeth'),('strainteeth','strain teeth'),\n",
    "                ('quadrapedal','quadrupedal')]  # for AWA2\n",
    "\n",
    "\n",
    "#For AWA2\n",
    "path = 'E:/Sushree/Dataset/Animals_with_Attributes2/attribute/predicates.txt'\n",
    "df=pd.read_csv(path,sep='\\t',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('E:/Sushree/Dataset/Animals_with_Attributes2/attribute/new_des.csv')\n",
    "print('Done preprocessing attribute des')\n",
    "\n",
    "import pickle\n",
    "\n",
    "counter = 0\n",
    "\n",
    "w2v_att = np.zeros((signature.shape[0], dim_w2v))\n",
    "for s in des:\n",
    "    print(s)\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    encoded_input = tokenizer(s, return_tensors='tf')\n",
    "    length = encoded_input.input_ids.shape[1]\n",
    "    #print(length)\n",
    "    for i in range(length-2):\n",
    "        w2v = w2v + model(encoded_input).last_hidden_state[:, i+1, :]\n",
    "        #print(model(encoded_input).last_hidden_state[:, i+1, :][:,1:2])\n",
    "        #print(w2v[:, 1:2])\n",
    "        #print(w2v.shape)\n",
    "\n",
    "    w2v = w2v / (length - 2)\n",
    "    #print(w2v[:, 1:2])\n",
    "    w2v_att[counter] = w2v\n",
    "    counter = counter + 1\n",
    "\n",
    "print('counter ',counter)\n",
    "\n",
    "print(w2v_att, w2v_att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5facf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ... -0.00612236 -0.01822051\n",
      "   0.01286502]\n",
      " [-0.01101558 -0.01632644 -0.01510743 ... -0.00680365 -0.02024807\n",
      "   0.01429663]\n",
      " [-0.02431119 -0.03603217 -0.03334183 ... -0.00956699 -0.02847192\n",
      "   0.02010326]\n",
      " ...\n",
      " [-0.02059108 -0.03051851 -0.02823985 ... -0.00344705 -0.01025863\n",
      "   0.00724334]\n",
      " [-0.01793673 -0.02658444 -0.02459951 ... -0.05037949 -0.14993233\n",
      "   0.10586323]\n",
      " [-0.00349297 -0.00517701 -0.00479047 ... -0.02762801 -0.08222259\n",
      "   0.05805518]] (50, 87040)\n"
     ]
    }
   ],
   "source": [
    "attribute_new = np.einsum('ij,jl->ijl', attribute, w2v_att)\n",
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], attribute_new.shape[1]* attribute_new.shape[2]])\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "#attribute_new[attribute_new<0] = 0\n",
    "#print(attribute_new, attribute_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237b819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4     5 ... 37318 37320 37321] 37321\n",
      "[    0     1     3 ... 37306 37307 37319] 37319\n",
      "[ 1046  1047  1048 ... 35288 35289 35290] 35290\n"
     ]
    }
   ],
   "source": [
    "# total number of instances or images = 37322: ranges from 0 to 37321\n",
    "\n",
    "trainval_loc = np.squeeze(att_splits['trainval_loc']-1) # -1: to consider the overflow problem\n",
    "print(np.unique(trainval_loc), np.max(np.unique(trainval_loc))) # smallest location: 2, largest location 37321\n",
    "\n",
    "test_seen_loc = np.squeeze(att_splits['test_seen_loc']-1)\n",
    "print(np.unique(test_seen_loc), np.max(np.unique(test_seen_loc))) # smallest location: 0, largest location 37319\n",
    "\n",
    "test_unseen_loc = np.squeeze(att_splits['test_unseen_loc']-1)\n",
    "print(np.unique(test_unseen_loc), np.max(np.unique(test_unseen_loc))) # smallest location: 1046, largest location 35290\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c148c5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " ...\n",
      " [38]\n",
      " [38]\n",
      " [38]] (37322, 1)\n",
      "unique_labels [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50] (50,)\n",
      "labels_trainval [[43]\n",
      " [22]\n",
      " [43]\n",
      " ...\n",
      " [40]\n",
      " [19]\n",
      " [46]] (23527, 1)\n",
      "unique_labels_trainval [ 1  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28\n",
      " 29 32 33 35 36 37 38 39 40 42 43 44 45 46 48 49] (40,)\n",
      "labels_test_seen [[22]\n",
      " [49]\n",
      " [14]\n",
      " ...\n",
      " [25]\n",
      " [15]\n",
      " [27]] (5882, 1)\n",
      "unique_labels_test_seen [ 1  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28\n",
      " 29 32 33 35 36 37 38 39 40 42 43 44 45 46 48 49] (40,)\n",
      "labels_test_unseen [[30]\n",
      " [30]\n",
      " [30]\n",
      " ...\n",
      " [47]\n",
      " [47]\n",
      " [47]] (7913, 1)\n",
      "unique_labels_test_unseen [ 7  9 23 24 30 31 34 41 47 50] (10,)\n",
      "correct number of instances for training, test seen and test unseen categories\n",
      "Number of overlapping classes between trainval and test seen: 40\n",
      "Number of overlapping classes between trainval and test unseen: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 37322 x 1\n",
    "\n",
    "print('unique_labels', np.unique(labels), np.unique(labels).shape)# class labels range from 1 to 50, 50 classes\n",
    "\n",
    "# get the labels for trainval, test seen and test unseen sets\n",
    "\n",
    "labels_trainval = labels[trainval_loc]\n",
    "print('labels_trainval', labels_trainval, labels_trainval.shape)\n",
    "\n",
    "unique_labels_trainval = np.unique(labels_trainval) # labels min:1 max:49\n",
    "print('unique_labels_trainval', unique_labels_trainval, unique_labels_trainval.shape)# 40 classes\n",
    "\n",
    "\n",
    "labels_test_seen = labels[test_seen_loc]\n",
    "print('labels_test_seen', labels_test_seen, labels_test_seen.shape)\n",
    "\n",
    "unique_labels_test_seen = np.unique(labels_test_seen) # labels min:1 max:49\n",
    "print('unique_labels_test_seen', unique_labels_test_seen, unique_labels_test_seen.shape)# 40 classes\n",
    "\n",
    "\n",
    "labels_test_unseen = labels[test_unseen_loc]\n",
    "print('labels_test_unseen', labels_test_unseen, labels_test_unseen.shape)\n",
    "\n",
    "unique_labels_test_unseen = np.unique(labels_test_unseen) # labels min:7 max:50\n",
    "print('unique_labels_test_unseen', unique_labels_test_unseen, unique_labels_test_unseen.shape)# 10 classes\n",
    "\n",
    "\n",
    "if len(labels) == len(labels_trainval) + len(labels_test_seen) + len(labels_test_unseen):\n",
    "    print('correct number of instances for training, test seen and test unseen categories')\n",
    "    \n",
    "print(\"Number of overlapping classes between trainval and test seen:\",len(set(unique_labels_trainval).intersection(set(unique_labels_test_seen))))\n",
    "\n",
    "print(\"Number of overlapping classes between trainval and test unseen:\",len(set(unique_labels_trainval).intersection(set(unique_labels_test_unseen))))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca83ed30",
   "metadata": {},
   "source": [
    "# Step 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88acec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for trainval: (23527, 2048)\n",
      "Features for test seen: (5882, 2048)\n",
      "Features for test unseen: (7913, 2048)\n"
     ]
    }
   ],
   "source": [
    "X_features = res101['features']\n",
    "\n",
    "# locations are already subtracted by 1, so they range from 0 to 37321\n",
    "trainval_vec = X_features[:, trainval_loc].transpose()\n",
    "test_seen_vec = X_features[:, test_seen_loc].transpose()\n",
    "test_unseen_vec = X_features[:, test_unseen_loc].transpose()\n",
    "\n",
    "print(\"Features for trainval:\", trainval_vec.shape) #(23527, 2048)\n",
    "print(\"Features for test seen:\", test_seen_vec.shape)# (5882, 2048)\n",
    "print(\"Features for test unseen:\", test_unseen_vec.shape) #(7913, 2048)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a651a471",
   "metadata": {},
   "source": [
    "Step 4 - Visual features are transformed into lower dimensional space using 'model00' (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c87a4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.layers import *\n",
    "#from keras.optimizers import SGD, Adam, Adagrad\n",
    "#import keras.backend as K\n",
    "\n",
    "#inputt = Input(shape = trainval_vec.shape[1])\n",
    "#hidden = Dense(2048, name=\"layer1\", activation='linear')(inputt)\n",
    "#output = Dense(1024, name=\"layer3\", activation='linear')(hidden)\n",
    "\n",
    "#model00 = Model(inputs = inputt, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "#opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "#model00.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#for layer in model00.layers:\n",
    " #       layer.trainable = False\n",
    "        \n",
    "#model00.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c1b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainval_vec = model00.predict(trainval_vec)\n",
    "#print(trainval_vec, trainval_vec.shape)\n",
    "\n",
    "#test_seen_vec = model00.predict(test_seen_vec)\n",
    "#print(test_seen_vec, test_seen_vec.shape)\n",
    "\n",
    "#test_unseen_vec = model00.predict(test_unseen_vec)\n",
    "#print(test_unseen_vec, test_unseen_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e89686f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00052664 -0.00078054 -0.00072226 ... -0.00223581 -0.00665391\n",
      "   0.00469815]\n",
      " [ 0.          0.          0.         ... -0.00277371 -0.00825473\n",
      "   0.00582844]\n",
      " [-0.00052664 -0.00078054 -0.00072226 ... -0.00223581 -0.00665391\n",
      "   0.00469815]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.0064743  -0.01926791\n",
      "   0.01360456]\n",
      " [-0.00076979 -0.00114092 -0.00105574 ... -0.00450891 -0.01341877\n",
      "   0.00947463]\n",
      " [-0.00354577 -0.00525526 -0.00486288 ... -0.05893629 -0.17539784\n",
      "   0.12384375]] (23527, 87040)\n",
      "[[ 0.          0.          0.         ... -0.00277371 -0.00825473\n",
      "   0.00582844]\n",
      " [-0.01793673 -0.02658444 -0.02459951 ... -0.05037949 -0.14993233\n",
      "   0.10586323]\n",
      " [-0.00176324 -0.00261333 -0.00241821 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.01226115 -0.01817253 -0.01681568 ... -0.02011478 -0.05986277\n",
      "   0.04226751]\n",
      " [-0.01070116 -0.01586043 -0.01467621 ... -0.00348217 -0.01036316\n",
      "   0.00731715]\n",
      " [-0.00328189 -0.00486417 -0.00450099 ... -0.00976332 -0.02905622\n",
      "   0.02051582]] (5882, 87040)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# attribute is defined for all 50 classes, so we cant use locations directly, instead we have to use labels \n",
    "# that range from 1 to 50, so we have to subtract 1\n",
    "\n",
    "train_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    train_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "print(train_attributes, train_attributes.shape)# (23527, 85)\n",
    "\n",
    "test_seen_attributes = np.zeros((len(test_seen_loc), attribute_new.shape[1]))\n",
    "for i in range(len(test_seen_loc)):\n",
    "    test_seen_attributes[i] = attribute_new[int(labels_test_seen[i])-1]\n",
    "\n",
    "print(test_seen_attributes, test_seen_attributes.shape)# (5882, 85)\n",
    "\n",
    "#test_unseen_attributes = np.zeros((len(test_unseen_loc), attribute_new.shape[1]))\n",
    "#for i in range(len(test_unseen_loc)):\n",
    "#    test_unseen_attributes[i] = attribute_new[int(labels_test_unseen[i])-1]\n",
    "\n",
    "#print(test_unseen_attributes, test_unseen_attributes.shape)# (7913, 85)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e974ea3e",
   "metadata": {},
   "source": [
    "Step 5 - New attribute vectors are transformed into a lower dimensional space using 'model0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85741854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87040\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 87040)]           0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 2048)              178259968 \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 512)               1049088   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 179,309,056\n",
      "Trainable params: 0\n",
      "Non-trainable params: 179,309,056\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "import keras.backend as K\n",
    "\n",
    "attribute_shape_new = attribute_new.shape[1]\n",
    "print(attribute_shape_new)\n",
    "\n",
    "# define model for attribute transformation\n",
    "\n",
    "inputt = Input(shape = attribute_shape_new)\n",
    "hidden = Dense(2048, name=\"layer1\", activation='linear')(inputt)\n",
    "#norm_layer = Lambda(lambda x: K.l2_normalize(x,axis=1))\n",
    "#batchnorm = BatchNormalization()(hidden)\n",
    "#batchnorm = norm_layer(hidden)\n",
    "#hidden2 = Dense(1024, name=\"layer2\", activation='linear')(hidden)\n",
    "output = Dense(512, name=\"layer3\", activation='linear')(hidden)\n",
    "\n",
    "model0 = Model(inputs = inputt, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model0.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for layer in model0.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d6ebf9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 4s 5ms/step\n",
      "[[-0.01060587  0.16492726  0.13291374 ... -0.12251601  0.0395489\n",
      "   0.02072064]\n",
      " [-0.015154    0.03513538  0.05185136 ... -0.09346648 -0.01137062\n",
      "   0.03519592]\n",
      " [-0.01060587  0.16492726  0.13291374 ... -0.12251601  0.0395489\n",
      "   0.02072064]\n",
      " ...\n",
      " [-0.0617771   0.01114547  0.01065801 ... -0.09507618 -0.02953161\n",
      "   0.17195356]\n",
      " [-0.01827106  0.04678195  0.07453287 ... -0.11931957  0.03731982\n",
      "   0.17031279]\n",
      " [-0.0517026   0.10743117  0.01382869 ... -0.02287627 -0.03851479\n",
      "   0.05824069]] (23527, 512)\n",
      "184/184 [==============================] - 1s 5ms/step\n",
      "[[-0.015154    0.03513538  0.05185136 ... -0.09346648 -0.01137062\n",
      "   0.03519592]\n",
      " [-0.04835539  0.08162045  0.05941594 ... -0.06856067 -0.03110444\n",
      "   0.20423575]\n",
      " [-0.01049893  0.03613483  0.06606321 ... -0.12659201 -0.00203105\n",
      "   0.10977834]\n",
      " ...\n",
      " [ 0.05245933  0.05708604  0.04050728 ... -0.10841252  0.11985397\n",
      "   0.07459512]\n",
      " [-0.0106774   0.14739555  0.09137649 ... -0.08296686  0.04688121\n",
      "   0.02429413]\n",
      " [ 0.06129791 -0.03575002 -0.04878271 ... -0.11773747 -0.00877306\n",
      "   0.13104066]] (5882, 512)\n"
     ]
    }
   ],
   "source": [
    "train_attributes_2 = model0.predict(train_attributes)\n",
    "print(train_attributes_2, train_attributes_2.shape)\n",
    "\n",
    "test_seen_attributes_2 = model0.predict(test_seen_attributes)\n",
    "print(test_seen_attributes_2, test_seen_attributes_2.shape)\n",
    "\n",
    "#test_unseen_attributes_2 = model0.predict(test_unseen_attributes)\n",
    "#print(test_unseen_attributes_2, test_unseen_attributes_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1457e426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "[[-0.04613888 -0.06162656  0.06225181 ... -0.08769844 -0.00829026\n",
      "   0.16642572]\n",
      " [-0.00463901  0.21757042  0.07664593 ... -0.01963473 -0.07946007\n",
      "  -0.01018086]\n",
      " [-0.00319296  0.08191028  0.11915039 ... -0.03012314  0.07096008\n",
      "   0.1303697 ]\n",
      " ...\n",
      " [ 0.00663882  0.02915052  0.01089406 ... -0.06127711 -0.02300003\n",
      "   0.06792159]\n",
      " [-0.04835549  0.08162059  0.05941591 ... -0.06856066 -0.03110447\n",
      "   0.20423594]\n",
      " [ 0.03697511 -0.06499453  0.06677884 ... -0.07991131  0.08740322\n",
      "   0.19930871]] (50, 512)\n",
      "[[-0.04613888 -0.00463901 -0.00319296 ...  0.00663882 -0.04835549\n",
      "   0.03697511]\n",
      " [-0.06162656  0.21757042  0.08191028 ...  0.02915052  0.08162059\n",
      "  -0.06499453]\n",
      " [ 0.06225181  0.07664593  0.11915039 ...  0.01089406  0.05941591\n",
      "   0.06677884]\n",
      " ...\n",
      " [-0.08769844 -0.01963473 -0.03012314 ... -0.06127711 -0.06856066\n",
      "  -0.07991131]\n",
      " [-0.00829026 -0.07946007  0.07096008 ... -0.02300003 -0.03110447\n",
      "   0.08740322]\n",
      " [ 0.16642572 -0.01018086  0.1303697  ...  0.06792159  0.20423594\n",
      "   0.19930871]] (512, 50)\n",
      "Signature for trainval: (512, 40)\n",
      "Signature for test seen: (512, 40)\n",
      "Signature for test unseen: (512, 10)\n"
     ]
    }
   ],
   "source": [
    "# as labels range from 1 to 50, we have subtract 1\n",
    "attribute_2 = model0.predict(attribute_new)\n",
    "print(attribute_2, attribute_2.shape)\n",
    "\n",
    "signature_2 = attribute_2.transpose()\n",
    "print(signature_2, signature_2.shape)#(50, 300)\n",
    "\n",
    "trainval_sig = signature_2[:, (unique_labels_trainval)-1]\n",
    "test_seen_sig = signature_2[:, (unique_labels_test_seen)-1]\n",
    "test_unseen_sig = signature_2[:, (unique_labels_test_unseen)-1]\n",
    "\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)\n",
    "print(\"Signature for test seen:\", test_seen_sig.shape)\n",
    "print(\"Signature for test unseen:\", test_unseen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581a4c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34]\n",
      " [19]\n",
      " [34]\n",
      " ...\n",
      " [32]\n",
      " [16]\n",
      " [37]] (23527, 1)\n",
      "[[19]\n",
      " [39]\n",
      " [11]\n",
      " ...\n",
      " [20]\n",
      " [12]\n",
      " [22]] (5882, 1)\n",
      "[[4]\n",
      " [4]\n",
      " [4]\n",
      " ...\n",
      " [8]\n",
      " [8]\n",
      " [8]] (7913, 1)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] (40,)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] (40,)\n",
      "[0 1 2 3 4 5 6 7 8 9] (10,)\n"
     ]
    }
   ],
   "source": [
    "# by doing this modification, we are changing the range of trainval and test seen labels from 0 to 39 \n",
    "# and test unseen labels from 0 to 9\n",
    "\n",
    "k = 0\n",
    "new_labels_trainval = np.zeros((len(labels_trainval), 1), dtype = 'int')\n",
    "for labels in unique_labels_trainval:\n",
    "    new_labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "    \n",
    "print(new_labels_trainval, new_labels_trainval.shape)#(23527, 1)\n",
    "\n",
    "l = 0\n",
    "new_labels_test_seen = np.zeros((len(labels_test_seen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_seen:\n",
    "    new_labels_test_seen[labels_test_seen == labels] = l\n",
    "    l = l+1\n",
    "    \n",
    "print(new_labels_test_seen, new_labels_test_seen.shape)# (5882, 1)\n",
    "\n",
    "m = 0\n",
    "new_labels_test_unseen = np.zeros((len(labels_test_unseen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_unseen:\n",
    "    new_labels_test_unseen[labels_test_unseen == labels] = m\n",
    "    m = m+1  \n",
    "\n",
    "print(new_labels_test_unseen, new_labels_test_unseen.shape) #  (7913, 1)  \n",
    "\n",
    "\n",
    "print(np.unique(new_labels_trainval), np.unique(new_labels_trainval).shape)\n",
    "\n",
    "print(np.unique(new_labels_test_seen), np.unique(new_labels_test_seen).shape)\n",
    "\n",
    "print(np.unique(new_labels_test_unseen), np.unique(new_labels_test_unseen).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fee1341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23527\n",
      "40\n",
      "5882\n",
      "40\n",
      "7913\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#params for trainval and test set\n",
    "m_trainval = new_labels_trainval.shape[0]# number of instances in training set: 23527\n",
    "print(m_trainval)\n",
    "\n",
    "z_trainval = len(unique_labels_trainval)# number of classes in training set: 40\n",
    "print(z_trainval)\n",
    "\n",
    "\n",
    "n_test_seen = new_labels_test_seen.shape[0]# 5882\n",
    "print(n_test_seen)\n",
    "\n",
    "z1_test_seen = len(unique_labels_test_seen)# 40\n",
    "print(z1_test_seen)\n",
    "\n",
    "\n",
    "n_test_unseen = new_labels_test_unseen.shape[0]# 7913\n",
    "print(n_test_unseen)\n",
    "\n",
    "z1_test_unseen = len(unique_labels_test_unseen)# 10\n",
    "print(z1_test_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "845b8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]] (23527, 40)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "gt_trainval = to_categorical(new_labels_trainval, z_trainval)\n",
    "\n",
    "print(gt_trainval, gt_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08067319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "512\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "input1_shape = trainval_vec.shape[1]\n",
    "print(input1_shape)\n",
    "\n",
    "attribute_shape = trainval_sig.shape[0]\n",
    "print(attribute_shape)\n",
    "\n",
    "output_shape = z_trainval\n",
    "print(output_shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb9d2473",
   "metadata": {},
   "source": [
    "Step 6 - Define 'model2' for attribute to class label mapping\n",
    "\t\t Define 'model1' for visual feature to class label mapping\n",
    "\t\t Train 'model2' and 'model1' through the iterative process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96792076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 1, 512)]       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 40)                20520     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,520\n",
      "Trainable params: 20,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "\n",
    "# define model2 for attribute to class label mapping\n",
    "\n",
    "input2 = Input(shape = (1, 1, attribute_shape))\n",
    "flat = Flatten()(input2)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model2 = Model(inputs = input2, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model2.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b285876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " intermediate (Conv1D)       (None, 1, 1, 512)         1049088   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 40)                20520     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,069,608\n",
      "Trainable params: 1,069,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model1 for resnet feature to class label mapping\n",
    "\n",
    "input1 = Input(shape = (1, 1, input1_shape))\n",
    "#inter_pre = Dense(512, name=\"intermediate_previous\", activation='relu')(input1)\n",
    "inter = Conv1D(attribute_shape, kernel_size = 1, name = \"intermediate\", activation = 'linear')(input1)\n",
    "flat = Flatten()(inter)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model1 = Model(inputs = input1, outputs = output)\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model1.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fd4ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23527, 1, 1, 2048)\n",
      "(23527, 1, 1, 512)\n",
      "(23527, 40)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = np.reshape(trainval_vec, [trainval_vec.shape[0], 1, 1, trainval_vec.shape[1]])\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = np.reshape(train_attributes_2, [train_attributes_2.shape[0], 1, 1, train_attributes_2.shape[1]])\n",
    "print(trainval_input2.shape)\n",
    "\n",
    "trainval_output = gt_trainval\n",
    "print(trainval_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cec915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 16\n",
    "from sklearn.model_selection import train_test_split    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7be0da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(trainval_input1, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen1 = DataGenerator(X_train1, y_train1, batch_size)   \n",
    "#val_gen1 = DataGenerator(X_val1, y_val1, batch_size)\n",
    "\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(trainval_input2, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen2 = DataGenerator(X_train2, y_train2, batch_size)   \n",
    "#val_gen2 = DataGenerator(X_val2, y_val2, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f527990a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "model 2 is trained: training acc: 0.984375 , training loss: 0.39786297082901 , validation acc: 0.9375 , validation_loss: 0.44718706607818604\n",
      "model 1 is trained: training acc: 0.9375 , training loss: 4.351791858673096 , validation acc: 0.8125 , validation_loss: 36.395423889160156\n",
      "micro average\n",
      "seen accuracy: 72.3143098317929 unseen accuracy: 47.21888032397265 harmonic mean: 57.13226154524093\n",
      "macro average\n",
      "seen accuracy: 77.3206392383543 unseen accuracy: 41.273853153039305 harmonic mean: 53.819037381445185\n",
      "best accuracy micro seen accuracy: 72.3143098317929 unseen accuracy: 47.21888032397265 harmonic mean: 57.13226154524093\n",
      "best accuracy macro seen accuracy: 77.3206392383543 unseen accuracy: 41.273853153039305 harmonic mean: 53.819037381445185\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 1\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.1548878252506256 , validation acc: 1.0 , validation_loss: 0.21341504156589508\n",
      "model 1 is trained: training acc: 0.953125 , training loss: 4.886017322540283 , validation acc: 0.8125 , validation_loss: 89.86460876464844\n",
      "micro average\n",
      "seen accuracy: 76.06143662589884 unseen accuracy: 43.74722573421149 harmonic mean: 55.54651512158914\n",
      "macro average\n",
      "seen accuracy: 79.66678000680041 unseen accuracy: 40.81890559838241 harmonic mean: 53.980035156720724\n",
      "best accuracy micro seen accuracy: 72.3143098317929 unseen accuracy: 47.21888032397265 harmonic mean: 57.13226154524093\n",
      "best accuracy macro seen accuracy: 79.66678000680041 unseen accuracy: 40.81890559838241 harmonic mean: 53.980035156720724\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 2\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.0977930799126625 , validation acc: 1.0 , validation_loss: 0.09860597550868988\n",
      "model 1 is trained: training acc: 0.9375 , training loss: 5.610898971557617 , validation acc: 1.0 , validation_loss: 0.0\n",
      "micro average\n",
      "seen accuracy: 77.28664834975112 unseen accuracy: 50.08780148577146 harmonic mean: 60.783278044249826\n",
      "macro average\n",
      "seen accuracy: 80.09180550833051 unseen accuracy: 45.330468848729936 harmonic mean: 57.89400827317296\n",
      "best accuracy micro seen accuracy: 77.28664834975112 unseen accuracy: 50.08780148577146 harmonic mean: 60.783278044249826\n",
      "best accuracy macro seen accuracy: 80.09180550833051 unseen accuracy: 45.330468848729936 harmonic mean: 57.89400827317296\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 3\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.0809488296508789 , validation acc: 1.0 , validation_loss: 0.04837411642074585\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 1.0866203308105469 , validation acc: 1.0 , validation_loss: 0.0\n",
      "micro average\n",
      "seen accuracy: 82.30378563766988 unseen accuracy: 44.99592928010688 harmonic mean: 58.18293183813749\n",
      "macro average\n",
      "seen accuracy: 86.5691941516491 unseen accuracy: 44.458486035637556 harmonic mean: 58.746904528970255\n",
      "best accuracy micro seen accuracy: 77.28664834975112 unseen accuracy: 50.08780148577146 harmonic mean: 60.783278044249826\n",
      "best accuracy macro seen accuracy: 86.5691941516491 unseen accuracy: 44.458486035637556 harmonic mean: 58.746904528970255\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 4\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.0638454258441925 , validation acc: 1.0 , validation_loss: 0.05303936451673508\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 0.8001461029052734 , validation acc: 0.9375 , validation_loss: 35.63329315185547\n",
      "micro average\n",
      "seen accuracy: 80.516262621259 unseen accuracy: 47.945742892938256 harmonic mean: 60.10200464934417\n",
      "macro average\n",
      "seen accuracy: 85.60013600816049 unseen accuracy: 46.87223556173385 harmonic mean: 60.575193023913265\n",
      "best accuracy micro seen accuracy: 77.28664834975112 unseen accuracy: 50.08780148577146 harmonic mean: 60.783278044249826\n",
      "best accuracy macro seen accuracy: 85.60013600816049 unseen accuracy: 46.87223556173385 harmonic mean: 60.575193023913265\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 5\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.05880998075008392 , validation acc: 1.0 , validation_loss: 0.04781878739595413\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 9.148323059082031 , validation acc: 0.875 , validation_loss: 23.769073486328125\n",
      "micro average\n",
      "seen accuracy: 82.9092129864498 unseen accuracy: 51.21580968802761 harmonic mean: 63.31797585606194\n",
      "macro average\n",
      "seen accuracy: 86.31417885073105 unseen accuracy: 47.352457980538354 harmonic mean: 61.15495420616096\n",
      "best accuracy micro seen accuracy: 82.9092129864498 unseen accuracy: 51.21580968802761 harmonic mean: 63.31797585606194\n",
      "best accuracy macro seen accuracy: 86.31417885073105 unseen accuracy: 47.352457980538354 harmonic mean: 61.15495420616096\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 6\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.05779525637626648 , validation acc: 1.0 , validation_loss: 0.04550381004810333\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 4.404899597167969 , validation acc: 1.0 , validation_loss: 0.0\n",
      "micro average\n",
      "seen accuracy: 85.45707738642726 unseen accuracy: 53.6853654525013 harmonic mean: 65.94385345532373\n",
      "macro average\n",
      "seen accuracy: 88.28629717783068 unseen accuracy: 47.87059269556426 harmonic mean: 62.08011033050631\n",
      "best accuracy micro seen accuracy: 85.45707738642726 unseen accuracy: 53.6853654525013 harmonic mean: 65.94385345532373\n",
      "best accuracy macro seen accuracy: 88.28629717783068 unseen accuracy: 47.87059269556426 harmonic mean: 62.08011033050631\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 7\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.04800259321928024 , validation acc: 1.0 , validation_loss: 0.048067450523376465\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 2.3878555297851562 , validation acc: 0.8125 , validation_loss: 43.708839416503906\n",
      "micro average\n",
      "seen accuracy: 85.41733077419066 unseen accuracy: 52.306239703000045 harmonic mean: 64.88155023551629\n",
      "macro average\n",
      "seen accuracy: 88.76232573954437 unseen accuracy: 48.26235308985215 harmonic mean: 62.527111794978666\n",
      "best accuracy micro seen accuracy: 85.45707738642726 unseen accuracy: 53.6853654525013 harmonic mean: 65.94385345532373\n",
      "best accuracy macro seen accuracy: 88.76232573954437 unseen accuracy: 48.26235308985215 harmonic mean: 62.527111794978666\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 8\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.0457262247800827 , validation acc: 1.0 , validation_loss: 0.043404512107372284\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 3.76617431640625 , validation acc: 0.9375 , validation_loss: 7.28204345703125\n",
      "micro average\n",
      "seen accuracy: 86.05785487242682 unseen accuracy: 54.232326656416696 harmonic mean: 66.53520076645418\n",
      "macro average\n",
      "seen accuracy: 89.20435226113567 unseen accuracy: 55.023379249336536 harmonic mean: 68.06353887358328\n",
      "best accuracy micro seen accuracy: 86.05785487242682 unseen accuracy: 54.232326656416696 harmonic mean: 66.53520076645418\n",
      "best accuracy macro seen accuracy: 89.20435226113567 unseen accuracy: 55.023379249336536 harmonic mean: 68.06353887358328\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 9\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.05445597320795059 , validation acc: 1.0 , validation_loss: 0.04800853878259659\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 0.9476356506347656 , validation acc: 1.0 , validation_loss: 0.0\n",
      "micro average\n",
      "seen accuracy: 84.07831660716758 unseen accuracy: 44.38942631637134 harmonic mean: 58.103118415643365\n",
      "macro average\n",
      "seen accuracy: 87.70826249574975 unseen accuracy: 41.22330342474409 harmonic mean: 56.08594438306882\n",
      "best accuracy micro seen accuracy: 86.05785487242682 unseen accuracy: 54.232326656416696 harmonic mean: 66.53520076645418\n",
      "best accuracy macro seen accuracy: 89.20435226113567 unseen accuracy: 55.023379249336536 harmonic mean: 68.06353887358328\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.035067081451416016 , validation acc: 1.0 , validation_loss: 0.04605497047305107\n",
      "model 1 is trained: training acc: 0.984375 , training loss: 0.6672477722167969 , validation acc: 0.875 , validation_loss: 56.8167724609375\n",
      "micro average\n",
      "seen accuracy: 84.76937541375807 unseen accuracy: 48.02523876923816 harmonic mean: 61.31377420103535\n",
      "macro average\n",
      "seen accuracy: 88.72832369942196 unseen accuracy: 43.6876026791356 harmonic mean: 58.54775717966477\n",
      "best accuracy micro seen accuracy: 86.05785487242682 unseen accuracy: 54.232326656416696 harmonic mean: 66.53520076645418\n",
      "best accuracy macro seen accuracy: 89.20435226113567 unseen accuracy: 55.023379249336536 harmonic mean: 68.06353887358328\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 11\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.04050927981734276 , validation acc: 1.0 , validation_loss: 0.0389481857419014\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.75 , validation_loss: 155.426513671875\n",
      "micro average\n",
      "seen accuracy: 83.96497057505188 unseen accuracy: 41.31022068350052 harmonic mean: 55.37587177943697\n",
      "macro average\n",
      "seen accuracy: 87.55525331519891 unseen accuracy: 40.02274737773284 harmonic mean: 54.93426399527762\n",
      "best accuracy micro seen accuracy: 86.05785487242682 unseen accuracy: 54.232326656416696 harmonic mean: 66.53520076645418\n",
      "best accuracy macro seen accuracy: 89.20435226113567 unseen accuracy: 55.023379249336536 harmonic mean: 68.06353887358328\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 12\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.0422452837228775 , validation acc: 1.0 , validation_loss: 0.02295682206749916\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.9375 , validation_loss: 53.659507751464844\n",
      "micro average\n",
      "seen accuracy: 86.67188013331752 unseen accuracy: 48.60844089907948 harmonic mean: 62.285259687749225\n",
      "macro average\n",
      "seen accuracy: 89.3573614416865 unseen accuracy: 42.941994186781244 harmonic mean: 58.007588583437354\n",
      "best accuracy micro seen accuracy: 86.05785487242682 unseen accuracy: 54.232326656416696 harmonic mean: 66.53520076645418\n",
      "best accuracy macro seen accuracy: 89.20435226113567 unseen accuracy: 55.023379249336536 harmonic mean: 68.06353887358328\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 13\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.03668086603283882 , validation acc: 1.0 , validation_loss: 0.033446647226810455\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 1.0 , validation_loss: 0.0\n",
      "micro average\n",
      "seen accuracy: 85.46063057772096 unseen accuracy: 41.944370948136836 harmonic mean: 56.27082685111218\n",
      "macro average\n",
      "seen accuracy: 88.55831349880992 unseen accuracy: 34.04524200682421 harmonic mean: 49.18273703154783\n",
      "best accuracy micro seen accuracy: 86.05785487242682 unseen accuracy: 54.232326656416696 harmonic mean: 66.53520076645418\n",
      "best accuracy macro seen accuracy: 89.20435226113567 unseen accuracy: 55.023379249336536 harmonic mean: 68.06353887358328\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 14\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.04216920584440231 , validation acc: 1.0 , validation_loss: 0.03441338241100311\n",
      "model 1 is trained: training acc: 1.0 , training loss: 0.0 , validation acc: 0.875 , validation_loss: 31.47833251953125\n",
      "micro average\n",
      "seen accuracy: 85.79687954710134 unseen accuracy: 47.35773858458637 harmonic mean: 61.028993961688755\n",
      "macro average\n",
      "seen accuracy: 89.25535532131927 unseen accuracy: 42.310122583091115 harmonic mean: 57.407233037015075\n",
      "best accuracy micro seen accuracy: 86.05785487242682 unseen accuracy: 54.232326656416696 harmonic mean: 66.53520076645418\n",
      "best accuracy macro seen accuracy: 89.20435226113567 unseen accuracy: 55.023379249336536 harmonic mean: 68.06353887358328\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iteration = 15\n",
    "epochs1 = 100\n",
    "epochs2 = 200\n",
    "\n",
    "best_performance_micro = [0, 0, 0]\n",
    "best_performance_macro = [0, 0, 0]\n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/'\n",
    "name = 'model1_conv_AWA2_Bert2attT_1024_lin_it15_100eph_adam_cce_16bch_1e-2lr_model2_adam_lr-2_200'\n",
    "\n",
    "for i in range(iteration):\n",
    "    X_train2_it = X_train2[(len(X_train2)//iteration)*i:(len(X_train2)//iteration)*(i+1)]\n",
    "    X_val2_it = X_val2[(len(X_val2)//iteration)*i:(len(X_val2)//iteration)*(i+1)]\n",
    "    y_train2_it = y_train2[(len(y_train2)//iteration)*i:(len(y_train2)//iteration)*(i+1)]\n",
    "    y_val2_it = y_val2[(len(y_val2)//iteration)*i:(len(y_val2)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen2 = DataGenerator(X_train2_it, y_train2_it, batch_size)   \n",
    "    val_gen2 = DataGenerator(X_val2_it, y_val2_it, batch_size)\n",
    "\n",
    "    train_summary2 = model2.fit(train_gen2, epochs = epochs2, verbose = 0, callbacks = None, validation_data = val_gen2, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen2)//batch_size, \n",
    "                              validation_steps = len(val_gen2)//batch_size)\n",
    "\n",
    "    print(\"iteration:\", i)\n",
    "    print('model 2 is trained:', 'training acc:', train_summary2.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary2.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary2.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary2.history['val_loss'][-1])\n",
    "\n",
    "    weights_list2 = model2.get_weights()\n",
    "    #print(weights_list2)\n",
    "\n",
    "    model1.layers[-1].set_weights(weights_list2)\n",
    "\n",
    "    X_train1_it = X_train1[(len(X_train1)//iteration)*i:(len(X_train1)//iteration)*(i+1)]\n",
    "    X_val1_it = X_val1[(len(X_val1)//iteration)*i:(len(X_val1)//iteration)*(i+1)]\n",
    "    y_train1_it = y_train1[(len(y_train1)//iteration)*i:(len(y_train1)//iteration)*(i+1)]\n",
    "    y_val1_it = y_val1[(len(y_val1)//iteration)*i:(len(y_val1)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen1 = DataGenerator(X_train1_it, y_train1_it, batch_size)   \n",
    "    val_gen1 = DataGenerator(X_val1_it, y_val1_it, batch_size)\n",
    "    \n",
    "    for layer in model1.layers[2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    train_summary1 = model1.fit(train_gen1, epochs = epochs1, verbose = 0, callbacks = None, validation_data = val_gen1, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen1)//batch_size, \n",
    "                              validation_steps = len(val_gen1)//batch_size)\n",
    "    \n",
    "    #print(\"iteration:\", i)\n",
    "    print('model 1 is trained:', 'training acc:', train_summary1.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary1.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary1.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary1.history['val_loss'][-1])\n",
    "    \n",
    "    weights_list1 = Model(inputs = model1.input, outputs = model1.layers[-1].output).get_weights()\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, np.matmul(weights_list1[0], weights_list1[2])), test_seen_sig)\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0]), test_seen_sig)\n",
    "    \n",
    "    outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0][0]), test_seen_sig)\n",
    "    preds_seen = np.array([np.argmax(output) for output in outputs_seen])\n",
    "    \n",
    "    cm_seen = confusion_matrix(new_labels_test_seen, preds_seen)\n",
    "    #print(cm)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_seen_micro = cm_seen.astype('float') / cm_seen.sum(axis=1)[:, np.newaxis]\n",
    "    #print(cm)\n",
    "    avg_seen_micro = (sum(cm_seen_micro.diagonal())/len(unique_labels_test_seen))*100\n",
    "\n",
    "    avg_seen_macro = (sum(cm_seen.diagonal())/len(new_labels_test_seen))*100\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0]), test_unseen_sig)\n",
    "    outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0][0]), test_unseen_sig)\n",
    "    \n",
    "    preds_unseen = np.array([np.argmax(output) for output in outputs_unseen])\n",
    "    \n",
    "    cm_unseen = confusion_matrix(new_labels_test_unseen, preds_unseen)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_unseen_micro = cm_unseen.astype('float') / cm_unseen.sum(axis=1)[:, np.newaxis]\n",
    "    avg_unseen_micro = (sum(cm_unseen_micro.diagonal())/len(unique_labels_test_unseen))*100\n",
    "\n",
    "    avg_unseen_macro = (sum(cm_unseen.diagonal())/len(new_labels_test_unseen))*100\n",
    "    \n",
    "    harmonic_micro = (2*avg_seen_micro*avg_unseen_micro) / (avg_seen_micro + avg_unseen_micro)\n",
    "    harmonic_macro = (2*avg_seen_macro*avg_unseen_macro) / (avg_seen_macro + avg_unseen_macro)\n",
    "    \n",
    "    print('micro average')\n",
    "    print('seen accuracy:', avg_seen_micro, 'unseen accuracy:', avg_unseen_micro, 'harmonic mean:', harmonic_micro)\n",
    "    \n",
    "    print('macro average')\n",
    "    print('seen accuracy:', avg_seen_macro, 'unseen accuracy:', avg_unseen_macro, 'harmonic mean:', harmonic_macro)\n",
    "    \n",
    "    if harmonic_micro > best_performance_micro[2]:\n",
    "        best_performance_micro = [avg_seen_micro, avg_unseen_micro, harmonic_micro]\n",
    "        model1.save_weights(save_path + 'bw_micro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    if harmonic_macro > best_performance_macro[2]:\n",
    "        best_performance_macro = [avg_seen_macro, avg_unseen_macro, harmonic_macro]\n",
    "        model1.save_weights(save_path + 'bw_macro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    print('best accuracy micro','seen accuracy:', best_performance_micro[0], 'unseen accuracy:', best_performance_micro[1], 'harmonic mean:', best_performance_micro[2])\n",
    "    print('best accuracy macro', 'seen accuracy:', best_performance_macro[0], 'unseen accuracy:', best_performance_macro[1], 'harmonic mean:', best_performance_macro[2])\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    weights_list3 = model1.get_weights()\n",
    "    model2.set_weights(weights_list3[2:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da36182a",
   "metadata": {},
   "source": [
    "Step 7 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58bc47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (5882, 40)\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 80.2680 - accuracy: 0.9004\n",
      "cce =  1.5862501\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 1.0000\n",
      "cce =  0.050263677\n"
     ]
    }
   ],
   "source": [
    "gt_test_seen = to_categorical(new_labels_test_seen, z1_test_seen)\n",
    "\n",
    "print(gt_test_seen, gt_test_seen.shape)\n",
    "\n",
    "test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "res1 = model1.evaluate(test_seen_vec, gt_test_seen)\n",
    "\n",
    "p1 = model1.predict(test_seen_vec, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p1).numpy())\n",
    "\n",
    "\n",
    "test_seen_attributes_2 = np.reshape(test_seen_attributes_2, [test_seen_attributes_2.shape[0], 1, 1, test_seen_attributes_2.shape[1]])\n",
    "\n",
    "res2 = model2.evaluate(test_seen_attributes_2, gt_test_seen)\n",
    "\n",
    "p2 = model2.predict(test_seen_attributes_2, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p2).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe3280c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.30203406651643\n",
      "95.01869976520538\n",
      "69.68773882022109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_seen_updated = res1[1]*100\n",
    "unseen_accuracy =55.02\n",
    "h = (2*accuracy_seen_updated*unseen_accuracy) / (accuracy_seen_updated + unseen_accuracy)\n",
    "print(h)\n",
    "\n",
    "\n",
    "accuracy_seen_updated2 = ((res1[1]*100)+(res2[1]*100))/2\n",
    "print(accuracy_seen_updated2)\n",
    "h = (2*accuracy_seen_updated2*unseen_accuracy) / (accuracy_seen_updated2 + unseen_accuracy)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67a434d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_seen_macro 0.9528142769618739 recall_seen_macro 0.9338405808003536 f1_seen_macro 0.9399977738067852\n",
      "precision_seen_micro 0.9501870112206732 recall_seen_micro 0.9501870112206732 f1_seen_micro 0.9501870112206732\n",
      "precision_unseen_macro 0.42959423649411715 recall_unseen_macro 0.47357738584586373 f1_unseen_macro 0.36739309652072827\n",
      "precision_unseen_micro 0.42310122583091114 recall_unseen_micro 0.42310122583091114 f1_unseen_micro 0.42310122583091114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pp1 = np.array([np.argmax(output) for output in p1])\n",
    "pp2 = np.array([np.argmax(output) for output in p2])\n",
    "\n",
    "seen_macro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'macro')\n",
    "seen_macro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'macro')\n",
    "print('precision_seen_macro', (seen_macro1[0] + seen_macro2[0])/2, 'recall_seen_macro', (seen_macro1[1] + seen_macro2[1])/2, 'f1_seen_macro', (seen_macro1[2] + seen_macro2[2])/2)\n",
    "\n",
    "\n",
    "seen_micro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'micro')\n",
    "seen_micro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'micro')\n",
    "print('precision_seen_micro', (seen_micro1[0] + seen_micro2[0])/2, 'recall_seen_micro', (seen_micro1[1] + seen_micro2[1])/2, 'f1_seen_micro', (seen_micro1[2] + seen_micro2[2])/2)\n",
    "\n",
    "unseen_macro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'macro')\n",
    "unseen_micro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'micro')\n",
    "\n",
    "print('precision_unseen_macro', unseen_macro[0], 'recall_unseen_macro', unseen_macro[1], 'f1_unseen_macro', unseen_macro[2])\n",
    "print('precision_unseen_micro', unseen_micro[0], 'recall_unseen_micro', unseen_micro[1], 'f1_unseen_micro', unseen_micro[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
