{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aacdede7",
   "metadata": {},
   "source": [
    "Code Description: \n",
    "\n",
    "GZSL_CUB_5: Experiments with Bert attributes and latent embedding\n",
    "\tStep 1 - Class wise continous attributes are extracted (for CUB: 200 categories, and each category has attribute vectors of length 312) \n",
    "\tStep 2 - Word vectors are extracted for each semantic attribute using pretrained language models (w2v, Bert, Bart, GPT) \n",
    "             New attributes are formed by aggregating the continous attribute values and word vectors\n",
    "\tStep 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n",
    "\tStep 4 - New attribute vectors are transformed into a lower dimensional space using 'model_transform_attribute'\n",
    "\tStep 5 - 'embedding_model' is trained to perform embedding between visual features and attribute vectors (or semantic features)\n",
    "\t         embedding_model_0 is used to extract embedded visual features\n",
    "\t\t     embedding_model_1 is used to extract embedded semantic features\n",
    "\tStep 6 - Define 'model2' for attribute to class label mapping\n",
    "\t\t     Define 'model1' for visual feature to class label mapping\n",
    "\t\t     Train 'model2' and 'model1' through the iterative process\n",
    "\tStep 7 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bb0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'CUB'\n",
    "path = 'E:/Sushree/Dataset/data/xlsa17/data/'\n",
    "\n",
    "res101 = scipy.io.loadmat(path + dataset + '/res101.mat')\n",
    "att_splits = scipy.io.loadmat(path + dataset + '/att_splits.mat')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6c562e4",
   "metadata": {},
   "source": [
    "Step 1 - Class wise continous attributes are extracted (for CUB: 200 categories, and each category has attribute vectors of length 312) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6082508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 200)\n",
      "[[0.0106384  0.0106384  0.00709227 ... 0.00918617 0.02526198 0.02066889]\n",
      " [0.         0.01133243 0.00944369 ... 0.00266542 0.02132333 0.05863916]\n",
      " [0.         0.         0.00742474 ... 0.         0.00885258 0.01770516]\n",
      " ...\n",
      " [0.         0.00334966 0.         ... 0.00556558 0.         0.15027069]\n",
      " [0.         0.11184146 0.         ... 0.08207164 0.05836206 0.01823814]\n",
      " [0.04378019 0.02814441 0.         ... 0.06022509 0.07695428 0.06189801]] (200, 312)\n"
     ]
    }
   ],
   "source": [
    "signature = att_splits['att']\n",
    "#signature = att_splits['original_att']\n",
    "#signature = signature/100\n",
    "print(signature.shape) #(312, 200)\n",
    "\n",
    "attribute = signature.transpose()\n",
    "attribute[attribute<0] = 0\n",
    "print(attribute, attribute.shape)#(200, 312)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2366d4b5",
   "metadata": {},
   "source": [
    "Step 2 - Word vectors are extracted for each semantic attribute using pretrained language models (w2v, Bert, Bart, GPT) \n",
    "         New attributes are formed by aggregating the continous attribute values and word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2293d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrain w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "print('Load pretrain w2v model')\n",
    "\n",
    "#model_name = 'word2vec-google-news-300' # length = 300\n",
    "#model = api.load(model_name)\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel, BartTokenizer, TFBartModel, GPT2Tokenizer, TFGPT2Model\n",
    "from transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\n",
    "\n",
    "# Bert model\n",
    "#---------------------------------------------------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # length 768\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased') # 1024\n",
    "#model = TFBertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Bart model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-base') # length 768\n",
    "#model = TFBartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-large') # length 1024\n",
    "#model = TFBartModel.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# GPT2 model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # length 768\n",
    "#model = TFGPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# GPT model\n",
    "#---------------------------------------------------------------------------\n",
    "#tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')# length 768\n",
    "#model = TFOpenAIGPTModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d5899c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "#model = TFOpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "#encoded_input = tokenizer('sushree', return_tensors='tf')\n",
    "#print(encoded_input)\n",
    "#length = encoded_input.input_ids.shape[1]\n",
    "#print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25be9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer)\n",
    "#out = model(encoded_input).last_hidden_state[:, 2, :]\n",
    "#print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab0196a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done replacing OOD words\n",
      "Done preprocessing attribute des\n",
      "has_bill_shape::curved_(up_or_down)\n",
      "has_bill_shape::dagger\n",
      "has_bill_shape::hooked\n",
      "has_bill_shape::needle\n",
      "has_bill_shape::hooked_seabird\n",
      "has_bill_shape::broad\n",
      "has_bill_shape::all-purpose\n",
      "has_bill_shape::cone\n",
      "has_bill_shape::specialized\n",
      "has_wing_color::blue\n",
      "has_wing_color::brown\n",
      "has_wing_color::iridescent\n",
      "has_wing_color::purple\n",
      "has_wing_color::rufous\n",
      "has_wing_color::gray\n",
      "has_wing_color::yellow\n",
      "has_wing_color::olive\n",
      "has_wing_color::green\n",
      "has_wing_color::pink\n",
      "has_wing_color::orange\n",
      "has_wing_color::black\n",
      "has_wing_color::white\n",
      "has_wing_color::red\n",
      "has_wing_color::buff\n",
      "has_upper parts_color::blue\n",
      "has_upper parts_color::brown\n",
      "has_upper parts_color::iridescent\n",
      "has_upper parts_color::purple\n",
      "has_upper parts_color::rufous\n",
      "has_upper parts_color::gray\n",
      "has_upper parts_color::yellow\n",
      "has_upper parts_color::olive\n",
      "has_upper parts_color::green\n",
      "has_upper parts_color::pink\n",
      "has_upper parts_color::orange\n",
      "has_upper parts_color::black\n",
      "has_upper parts_color::white\n",
      "has_upper parts_color::red\n",
      "has_upper parts_color::buff\n",
      "has_underparts_color::blue\n",
      "has_underparts_color::brown\n",
      "has_underparts_color::iridescent\n",
      "has_underparts_color::purple\n",
      "has_underparts_color::rufous\n",
      "has_underparts_color::gray\n",
      "has_underparts_color::yellow\n",
      "has_underparts_color::olive\n",
      "has_underparts_color::green\n",
      "has_underparts_color::pink\n",
      "has_underparts_color::orange\n",
      "has_underparts_color::black\n",
      "has_underparts_color::white\n",
      "has_underparts_color::red\n",
      "has_underparts_color::buff\n",
      "has_breast_pattern::solid\n",
      "has_breast_pattern::spotted\n",
      "has_breast_pattern::striped\n",
      "has_breast_pattern::multi-colored\n",
      "has_back_color::blue\n",
      "has_back_color::brown\n",
      "has_back_color::iridescent\n",
      "has_back_color::purple\n",
      "has_back_color::rufous\n",
      "has_back_color::gray\n",
      "has_back_color::yellow\n",
      "has_back_color::olive\n",
      "has_back_color::green\n",
      "has_back_color::pink\n",
      "has_back_color::orange\n",
      "has_back_color::black\n",
      "has_back_color::white\n",
      "has_back_color::red\n",
      "has_back_color::buff\n",
      "has_tail_shape::forked_tail\n",
      "has_tail_shape::rounded_tail\n",
      "has_tail_shape::notched_tail\n",
      "has_tail_shape::fan-shaped_tail\n",
      "has_tail_shape::pointed_tail\n",
      "has_tail_shape::squared_tail\n",
      "has_upper_tail_color::blue\n",
      "has_upper_tail_color::brown\n",
      "has_upper_tail_color::iridescent\n",
      "has_upper_tail_color::purple\n",
      "has_upper_tail_color::rufous\n",
      "has_upper_tail_color::gray\n",
      "has_upper_tail_color::yellow\n",
      "has_upper_tail_color::olive\n",
      "has_upper_tail_color::green\n",
      "has_upper_tail_color::pink\n",
      "has_upper_tail_color::orange\n",
      "has_upper_tail_color::black\n",
      "has_upper_tail_color::white\n",
      "has_upper_tail_color::red\n",
      "has_upper_tail_color::buff\n",
      "has_head_pattern::spotted\n",
      "has_head_pattern::malar\n",
      "has_head_pattern::crested\n",
      "has_head_pattern::masked\n",
      "has_head_pattern::unique_pattern\n",
      "has_head_pattern::eyebrow\n",
      "has_head_pattern::eye ring\n",
      "has_head_pattern::plain\n",
      "has_head_pattern::eyeline\n",
      "has_head_pattern::striped\n",
      "has_head_pattern::capped\n",
      "has_breast_color::blue\n",
      "has_breast_color::brown\n",
      "has_breast_color::iridescent\n",
      "has_breast_color::purple\n",
      "has_breast_color::rufous\n",
      "has_breast_color::gray\n",
      "has_breast_color::yellow\n",
      "has_breast_color::olive\n",
      "has_breast_color::green\n",
      "has_breast_color::pink\n",
      "has_breast_color::orange\n",
      "has_breast_color::black\n",
      "has_breast_color::white\n",
      "has_breast_color::red\n",
      "has_breast_color::buff\n",
      "has_throat_color::blue\n",
      "has_throat_color::brown\n",
      "has_throat_color::iridescent\n",
      "has_throat_color::purple\n",
      "has_throat_color::rufous\n",
      "has_throat_color::gray\n",
      "has_throat_color::yellow\n",
      "has_throat_color::olive\n",
      "has_throat_color::green\n",
      "has_throat_color::pink\n",
      "has_throat_color::orange\n",
      "has_throat_color::black\n",
      "has_throat_color::white\n",
      "has_throat_color::red\n",
      "has_throat_color::buff\n",
      "has_eye_color::blue\n",
      "has_eye_color::brown\n",
      "has_eye_color::purple\n",
      "has_eye_color::rufous\n",
      "has_eye_color::gray\n",
      "has_eye_color::yellow\n",
      "has_eye_color::olive\n",
      "has_eye_color::green\n",
      "has_eye_color::pink\n",
      "has_eye_color::orange\n",
      "has_eye_color::black\n",
      "has_eye_color::white\n",
      "has_eye_color::red\n",
      "has_eye_color::buff\n",
      "has_bill_length::about_the_same_as_head\n",
      "has_bill_length::longer_than_head\n",
      "has_bill_length::shorter_than_head\n",
      "has_forehead_color::blue\n",
      "has_forehead_color::brown\n",
      "has_forehead_color::iridescent\n",
      "has_forehead_color::purple\n",
      "has_forehead_color::rufous\n",
      "has_forehead_color::gray\n",
      "has_forehead_color::yellow\n",
      "has_forehead_color::olive\n",
      "has_forehead_color::green\n",
      "has_forehead_color::pink\n",
      "has_forehead_color::orange\n",
      "has_forehead_color::black\n",
      "has_forehead_color::white\n",
      "has_forehead_color::red\n",
      "has_forehead_color::buff\n",
      "has_under_tail_color::blue\n",
      "has_under_tail_color::brown\n",
      "has_under_tail_color::iridescent\n",
      "has_under_tail_color::purple\n",
      "has_under_tail_color::rufous\n",
      "has_under_tail_color::gray\n",
      "has_under_tail_color::yellow\n",
      "has_under_tail_color::olive\n",
      "has_under_tail_color::green\n",
      "has_under_tail_color::pink\n",
      "has_under_tail_color::orange\n",
      "has_under_tail_color::black\n",
      "has_under_tail_color::white\n",
      "has_under_tail_color::red\n",
      "has_under_tail_color::buff\n",
      "has_nape_color::blue\n",
      "has_nape_color::brown\n",
      "has_nape_color::iridescent\n",
      "has_nape_color::purple\n",
      "has_nape_color::rufous\n",
      "has_nape_color::gray\n",
      "has_nape_color::yellow\n",
      "has_nape_color::olive\n",
      "has_nape_color::green\n",
      "has_nape_color::pink\n",
      "has_nape_color::orange\n",
      "has_nape_color::black\n",
      "has_nape_color::white\n",
      "has_nape_color::red\n",
      "has_nape_color::buff\n",
      "has_belly_color::blue\n",
      "has_belly_color::brown\n",
      "has_belly_color::iridescent\n",
      "has_belly_color::purple\n",
      "has_belly_color::rufous\n",
      "has_belly_color::gray\n",
      "has_belly_color::yellow\n",
      "has_belly_color::olive\n",
      "has_belly_color::green\n",
      "has_belly_color::pink\n",
      "has_belly_color::orange\n",
      "has_belly_color::black\n",
      "has_belly_color::white\n",
      "has_belly_color::red\n",
      "has_belly_color::buff\n",
      "has_wing_shape::rounded-wings\n",
      "has_wing_shape::pointed-wings\n",
      "has_wing_shape::broad-wings\n",
      "has_wing_shape::tapered-wings\n",
      "has_wing_shape::long-wings\n",
      "has_size::large_(16_-_32_in)\n",
      "has_size::small_(5_-_9_in)\n",
      "has_size::very_large_(32_-_72_in)\n",
      "has_size::medium_(9_-_16_in)\n",
      "has_size::very_small_(3_-_5_in)\n",
      "has_shape::upright-perching_water-like\n",
      "has_shape::chicken-like-marsh\n",
      "has_shape::long-legged-like\n",
      "has_shape::duck-like\n",
      "has_shape::owl-like\n",
      "has_shape::gull-like\n",
      "has_shape::hummingbird-like\n",
      "has_shape::pigeon-like\n",
      "has_shape::tree-clinging-like\n",
      "has_shape::hawk-like\n",
      "has_shape::sandpiper-like\n",
      "has_shape::upland-ground-like\n",
      "has_shape::swallow-like\n",
      "has_shape::perching-like\n",
      "has_back_pattern::solid\n",
      "has_back_pattern::spotted\n",
      "has_back_pattern::striped\n",
      "has_back_pattern::multi-colored\n",
      "has_tail_pattern::solid\n",
      "has_tail_pattern::spotted\n",
      "has_tail_pattern::striped\n",
      "has_tail_pattern::multi-colored\n",
      "has_belly_pattern::solid\n",
      "has_belly_pattern::spotted\n",
      "has_belly_pattern::striped\n",
      "has_belly_pattern::multi-colored\n",
      "has_primary_color::blue\n",
      "has_primary_color::brown\n",
      "has_primary_color::iridescent\n",
      "has_primary_color::purple\n",
      "has_primary_color::rufous\n",
      "has_primary_color::gray\n",
      "has_primary_color::yellow\n",
      "has_primary_color::olive\n",
      "has_primary_color::green\n",
      "has_primary_color::pink\n",
      "has_primary_color::orange\n",
      "has_primary_color::black\n",
      "has_primary_color::white\n",
      "has_primary_color::red\n",
      "has_primary_color::buff\n",
      "has_leg_color::blue\n",
      "has_leg_color::brown\n",
      "has_leg_color::iridescent\n",
      "has_leg_color::purple\n",
      "has_leg_color::rufous\n",
      "has_leg_color::gray\n",
      "has_leg_color::yellow\n",
      "has_leg_color::olive\n",
      "has_leg_color::green\n",
      "has_leg_color::pink\n",
      "has_leg_color::orange\n",
      "has_leg_color::black\n",
      "has_leg_color::white\n",
      "has_leg_color::red\n",
      "has_leg_color::buff\n",
      "has_bill_color::blue\n",
      "has_bill_color::brown\n",
      "has_bill_color::iridescent\n",
      "has_bill_color::purple\n",
      "has_bill_color::rufous\n",
      "has_bill_color::gray\n",
      "has_bill_color::yellow\n",
      "has_bill_color::olive\n",
      "has_bill_color::green\n",
      "has_bill_color::pink\n",
      "has_bill_color::orange\n",
      "has_bill_color::black\n",
      "has_bill_color::white\n",
      "has_bill_color::red\n",
      "has_bill_color::buff\n",
      "has_crown_color::blue\n",
      "has_crown_color::brown\n",
      "has_crown_color::iridescent\n",
      "has_crown_color::purple\n",
      "has_crown_color::rufous\n",
      "has_crown_color::gray\n",
      "has_crown_color::yellow\n",
      "has_crown_color::olive\n",
      "has_crown_color::green\n",
      "has_crown_color::pink\n",
      "has_crown_color::orange\n",
      "has_crown_color::black\n",
      "has_crown_color::white\n",
      "has_crown_color::red\n",
      "has_crown_color::buff\n",
      "has_wing_pattern::solid\n",
      "has_wing_pattern::spotted\n",
      "has_wing_pattern::striped\n",
      "has_wing_pattern::multi-colored\n",
      "counter_err  0\n",
      "[[ 0.05212105  0.0075245   0.49733996 ... -0.33823124 -0.2336414\n",
      "  -0.08391333]\n",
      " [ 0.07815347 -0.01515857  0.07717857 ... -0.48531315 -0.01718637\n",
      "  -0.06311301]\n",
      " [ 0.38475081 -0.17769523  0.42568105 ... -0.38018307  0.05972182\n",
      "   0.13356039]\n",
      " ...\n",
      " [-0.13154849 -0.19325779 -0.19377798 ... -0.15703391 -0.24170311\n",
      "   0.11968013]\n",
      " [ 0.0707561  -0.31268418 -0.09783425 ... -0.23413777 -0.16987358\n",
      "   0.22994283]\n",
      " [ 0.27138266 -0.30414492  0.11182283 ... -0.10643208 -0.14129114\n",
      "   0.04606724]] (312, 768)\n",
      "[[ 5.54484646e-04  8.00486601e-05  5.29090167e-03 ... -2.19983320e-03\n",
      "  -2.92033137e-03  9.52158774e-04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -6.24108737e-03\n",
      "  -8.28519330e-03  2.70134395e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.88439686e-03\n",
      "  -2.50158207e-03  8.15627754e-04]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.59936223e-02\n",
      "  -2.12319176e-02  6.92255570e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.94112346e-03\n",
      "  -2.57688799e-03  8.40180854e-04]\n",
      " [ 2.28186951e-03  3.29424085e-04  2.17736367e-02 ... -6.58793416e-03\n",
      "  -8.74564075e-03  2.85147043e-03]] (200, 239616)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim_w2v = 768\n",
    "\n",
    "#%%\n",
    "#For CUB\n",
    "replace_word = [('spatulate','broad'),('upperparts','upper parts'),('grey','gray'), ('eyering', 'eye ring')] # for CUB\n",
    "\n",
    "\n",
    "path = 'E:/Sushree/Dataset/CUB_200_2011/CUB_200_2011/attributes/attributes.txt'\n",
    "df=pd.read_csv(path,sep=' ',header = None, names = ['idx','des'])\n",
    "des = df['des'].values\n",
    "\n",
    "#%% replace out of dictionary (OOD) words\n",
    "for pair in replace_word:\n",
    "    for idx,s in enumerate(des):\n",
    "        des[idx] = s.replace(pair[0],pair[1])\n",
    "print('Done replacing OOD words')\n",
    "\n",
    "#%% filter\n",
    "new_des = [' '.join(i.split('_')) for i in des]\n",
    "new_des = [' '.join(i.split('-')) for i in new_des]\n",
    "new_des = [' '.join(i.split('::')) for i in new_des]\n",
    "new_des = [i.split('(')[0] for i in new_des]\n",
    "new_des = [i[4:] for i in new_des]\n",
    "\n",
    "df['new_des'] = des\n",
    "df.to_csv('E:/Sushree/Dataset/CUB_200_2011/CUB_200_2011/attributes/new_des.csv')\n",
    "print('Done preprocessing attribute des')\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for word2vec\n",
    "# -----------------------------------------------------------------------------------\n",
    "#counter_err = 0\n",
    "#\n",
    "#all_w2v = []\n",
    "#for s in new_des:\n",
    "#    print(s)\n",
    "#    words = s.split(' ')\n",
    "#    if words[-1] == '':     #remove empty element\n",
    "#        words = words[:-1]\n",
    "#    w2v = np.zeros(dim_w2v)\n",
    "#    for w in words:\n",
    "#        try:\n",
    "#            w2v += model[w]\n",
    "#        except Exception as e:\n",
    "#            print(e)\n",
    "#            counter_err += 1\n",
    "#    w2v = w2v / len(words)  \n",
    "#    all_w2v.append(w2v[np.newaxis,:])\n",
    "    \n",
    "#print('counter_err ',counter_err)\n",
    "\n",
    "#w2v_att = np.concatenate(all_w2v,axis=0)\n",
    "#print(w2v_att, w2v_att.shape)\n",
    "\n",
    "# for Bert attributes\n",
    "# -----------------------------------------------------------------------------------\n",
    "counter_err = 0\n",
    "counter = 0\n",
    "w2v_att = np.zeros((signature.shape[0], dim_w2v))\n",
    "for s in des:\n",
    "    print(s)\n",
    "    w2v = np.zeros(dim_w2v)\n",
    "    encoded_input = tokenizer(s, return_tensors='tf')\n",
    "    length = encoded_input.input_ids.shape[1]\n",
    "    #print(length)\n",
    "    for i in range(length-2): # for Bert, Bart\n",
    "    #for i in range(length): # for GPT2\n",
    "        try:\n",
    "            w2v = w2v + model(encoded_input).last_hidden_state[:, i+1, :] # for Bert, Bart\n",
    "            #w2v = w2v + model(encoded_input).last_hidden_state[:, i, :] # for GPT2\n",
    "        #print(model(encoded_input).last_hidden_state[:, i+1, :][:,1:2])\n",
    "        #print(w2v[:, 1:2])\n",
    "        #print(w2v.shape)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            counter_err += 1\n",
    "    w2v = w2v / (length - 2)# for Bert, Bart\n",
    "    #w2v = w2v / length # for GPT2\n",
    "    #print(w2v[:, 1:2])\n",
    "    w2v_att[counter] = w2v\n",
    "    counter = counter + 1\n",
    "\n",
    "print('counter_err ',counter_err)\n",
    "print(w2v_att, w2v_att.shape)\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "attribute_new = np.einsum('ij,jl->ijl', attribute, w2v_att)\n",
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], attribute_new.shape[1]* attribute_new.shape[2]])\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "#attribute_new[attribute_new<0] = 0\n",
    "#print(attribute_new, attribute_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237b819e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1     2     4 ... 11724 11725 11726] 11726\n",
      "labels [[151]\n",
      " [151]\n",
      " [151]\n",
      " ...\n",
      " [150]\n",
      " [150]\n",
      " [150]] (11788, 1)\n",
      "unique_labels [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200] (200,)\n",
      "labels_trainval [[197]\n",
      " [198]\n",
      " [ 31]\n",
      " ...\n",
      " [ 65]\n",
      " [147]\n",
      " [ 22]] (7057, 1)\n",
      "unique_labels_trainval [  1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  20\n",
      "  22  23  24  25  26  27  28  30  31  32  33  35  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  51  52  53  54  55  57  58  59  60  61  63\n",
      "  64  65  66  67  70  71  73  74  75  76  77  78  81  82  83  84  85  86\n",
      "  89  90  92  93  94  96  97  99 101 102 103 105 106 107 109 110 111 112\n",
      " 113 114 115 117 118 119 121 123 126 127 128 130 131 132 133 134 135 136\n",
      " 137 138 140 143 144 145 146 147 148 149 151 153 154 155 156 158 161 162\n",
      " 163 164 165 168 169 170 172 173 175 177 178 180 181 183 184 186 188 190\n",
      " 194 196 197 198 199 200] (150,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# total number of instances or images = 11788: ranges from 0 to 11787\n",
    "\n",
    "trainval_loc = np.squeeze(att_splits['trainval_loc']-1) # -1: to consider the overflow problem\n",
    "print(np.unique(trainval_loc), np.max(np.unique(trainval_loc))) # smallest location: 2, largest location 11787\n",
    "\n",
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 11788 x 1\n",
    "\n",
    "print('unique_labels', np.unique(labels), np.unique(labels).shape)# class labels range from 1 to 200, 200 classes\n",
    "\n",
    "# get the labels for trainval, test seen and test unseen sets\n",
    "\n",
    "labels_trainval = labels[trainval_loc]\n",
    "print('labels_trainval', labels_trainval, labels_trainval.shape)\n",
    "\n",
    "unique_labels_trainval = np.unique(labels_trainval) # labels min:1 max:200\n",
    "print('unique_labels_trainval', unique_labels_trainval, unique_labels_trainval.shape)# 200 classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb503206",
   "metadata": {},
   "source": [
    "Step 3 - Visual features are extracted from pre-trained ResNet101 (without finetuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88acec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for trainval: (7057, 2048)\n",
      "[[ 1.76200075e-04  2.54372776e-05  1.68130403e-03 ... -1.62484619e-02\n",
      "  -2.15702232e-02  7.03285847e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.59936223e-02\n",
      "  -2.12319176e-02  6.92255570e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.31450454e-02\n",
      "  -1.74503634e-02  5.68959972e-03]\n",
      " ...\n",
      " [ 5.50317675e-04  7.94470917e-05  5.25114036e-03 ... -1.58951398e-02\n",
      "  -2.11011797e-02  6.87992929e-03]\n",
      " [ 3.32486331e-04  4.79996795e-05  3.17259008e-03 ... -1.64188769e-03\n",
      "  -2.17964532e-03  7.10661960e-04]\n",
      " [ 2.44718696e-04  3.53290283e-05  2.33510985e-03 ... -1.48075870e-03\n",
      "  -1.96574272e-03  6.40920134e-04]] (7057, 239616)\n"
     ]
    }
   ],
   "source": [
    "X_features = res101['features']\n",
    "\n",
    "# locations are already subtracted by 1, so they range from 0 to 11787\n",
    "trainval_vec = X_features[:, trainval_loc].transpose()\n",
    "print(\"Features for trainval:\", trainval_vec.shape) #(7057, 2048)\n",
    "\n",
    "# attribute is defined for all 200 classes, so we cant use locations directly, instead we have to use labels \n",
    "# that range from 1 to 200, so we have to subtract 1\n",
    "\n",
    "trainval_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    trainval_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "print(trainval_attributes, trainval_attributes.shape)# (7057, 319488)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e6b0183",
   "metadata": {},
   "source": [
    "Step 4 - New attribute vectors are transformed into a lower dimensional space using 'model_transform_attribute'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc71b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239616\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 239616)]          0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 4096)              981471232 \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 2048)              8390656   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 989,861,888\n",
      "Trainable params: 0\n",
      "Non-trainable params: 989,861,888\n",
      "_________________________________________________________________\n",
      "7/7 [==============================] - 0s 17ms/step\n",
      "[[-0.00659391  0.01807893  0.04017409 ... -0.0203063  -0.01138855\n",
      "   0.00085992]\n",
      " [-0.00271799 -0.01195218 -0.00141738 ... -0.03540748  0.01226223\n",
      "   0.00674735]\n",
      " [-0.00342696 -0.00230486 -0.01704866 ... -0.00140131  0.02639913\n",
      "  -0.04940924]\n",
      " ...\n",
      " [ 0.03092108  0.02796881  0.00458616 ...  0.0044677   0.04604499\n",
      "  -0.02038608]\n",
      " [ 0.01024084 -0.01208649  0.01471819 ... -0.00021282  0.00834993\n",
      "   0.01887791]\n",
      " [-0.00087847  0.00370738  0.0203206  ...  0.03452467  0.02700523\n",
      "   0.0046643 ]] (200, 2048)\n",
      "[[ 0.03462765  0.00879232 -0.00607726 ... -0.04044306  0.03902161\n",
      "  -0.03325852]\n",
      " [ 0.03092108  0.02796881  0.00458616 ...  0.0044677   0.04604499\n",
      "  -0.02038608]\n",
      " [ 0.06294927  0.00217706 -0.03738231 ...  0.00795531 -0.00916672\n",
      "  -0.09707019]\n",
      " ...\n",
      " [ 0.0207426   0.00249695 -0.00014649 ...  0.00655612 -0.01046089\n",
      "   0.00070989]\n",
      " [-0.02260757  0.01331258  0.01383336 ...  0.03095383  0.02341046\n",
      "   0.01984811]\n",
      " [-0.00190217  0.00831895 -0.04916832 ... -0.00477144  0.00060466\n",
      "  -0.0116683 ]] (7057, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "import keras.backend as K\n",
    "\n",
    "attribute_shape_new = attribute_new.shape[1]\n",
    "print(attribute_shape_new)\n",
    "\n",
    "# define model for attribute transformation\n",
    "\n",
    "inputt = Input(shape = attribute_shape_new)\n",
    "hidden = Dense(4096, name=\"layer1\", activation='linear')(inputt)\n",
    "output = Dense(2048, name=\"layer3\", activation='linear')(hidden)\n",
    "\n",
    "model_transform_attribute = Model(inputs = inputt, outputs = output)\n",
    "\n",
    "#sgd = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "\n",
    "model_transform_attribute.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for layer in model_transform_attribute.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model_transform_attribute.summary()\n",
    "\n",
    "attribute_new = model_transform_attribute.predict(attribute_new)\n",
    "print(attribute_new, attribute_new.shape)\n",
    "\n",
    "\n",
    "trainval_attributes = np.zeros((len(trainval_loc), attribute_new.shape[1]))\n",
    "for i in range(len(trainval_loc)):\n",
    "    trainval_attributes[i] = attribute_new[int(labels_trainval[i])-1]\n",
    "\n",
    "print(trainval_attributes, trainval_attributes.shape)# (7057, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e99b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7057, 1, 1, 2048)\n",
      "(7057, 1, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = np.reshape(trainval_vec, [trainval_vec.shape[0], 1, 1, trainval_vec.shape[1]])\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = np.reshape(trainval_attributes, [trainval_attributes.shape[0], 1, 1, trainval_attributes.shape[1]])\n",
    "print(trainval_input2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d22fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(trainval_input1, trainval_input2, test_size = 0.2, random_state = 42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 8\n",
    "train_gen = DataGenerator(X_train, y_train, batch_size)   \n",
    "val_gen = DataGenerator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36fff63b",
   "metadata": {},
   "source": [
    "Step 5 - 'embedding_model' is trained to perform embedding between visual features and attribute vectors (or semantic features)\n",
    "         embedding_model_0 is used to extract embedded visual features\n",
    "         embedding_model_1 is used to extract embedded semantic features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74a76d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2D)             (None, 1, 1, 2048)        4196352   \n",
      "                                                                 \n",
      " layer2 (Conv2D)             (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      " layer3 (Conv2D)             (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " layer4 (Conv2D)             (None, 1, 1, 2048)        2099200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,443,328\n",
      "Trainable params: 9,443,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "# -----------------------------------------------------------------------------------\n",
    "inputt = Input(shape = (1, 1, trainval_vec.shape[1]))\n",
    "hidden1 = Conv2D(2048, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "output1 = Conv2D(1024, 1, name=\"layer2\", activation='relu')(hidden1)\n",
    "hidden2 = Conv2D(1024, 1, name=\"layer3\", activation='relu')(output1)\n",
    "output2 = Conv2D(trainval_attributes.shape[1], 1, name=\"layer4\", activation='linear')(hidden2)\n",
    "\n",
    "embedding_model = Model(inputs = inputt, outputs = output2)\n",
    "\n",
    "# embedding model variants\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "#inputt = Input(shape = (1, 1, trainval_vec.shape[1]))\n",
    "#hidden1 = Conv2D(1024, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "#output1 = Conv2D(1024, 1, name=\"layer2\", activation='relu')(hidden1)\n",
    "#hidden2 = Conv2D(1024, 1, name=\"layer3\", activation='relu')(output1)\n",
    "#output2 = Conv2D(trainval_attributes.shape[1], 1, name=\"layer4\", activation='linear')(hidden2)\n",
    "\n",
    "#embedding_model = Model(inputs = inputt, outputs = output2)\n",
    "\n",
    "\n",
    "adam = Adam(learning_rate = 0.0001, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "embedding_model.compile(adam, loss = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.AUTO), metrics = ['accuracy'])\n",
    "#embedding_model.compile(adam, loss = cosine_loss, metrics = ['accuracy'])\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c663ebb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#CosineSimilarity is a number between -1 and 1. When it is a negative number between -1 and 0, 0 indicates orthogonality and \n",
    "#values closer to -1 indicate greater similarity. The values closer to 1 indicate greater dissimilarity. \n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/CUB/BertAttribute/with_LE/Embedding Model/'\n",
    "name = 'Reembedding_model_Bert_2048_1024_1024_CUB_200eph_adam_cos_16bch_0.0001lr'\n",
    "\n",
    "\n",
    "#file_path = save_path + 'bw_' + name + '.h5'\n",
    "\n",
    "#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath = file_path,\n",
    "#    monitor = 'val_accuracy',\n",
    "#    mode = 'max',\n",
    "#    save_best_only=True)\n",
    "\n",
    "#train_summary = embedding_model.fit(train_gen, epochs = 200, verbose = 1, callbacks = [model_checkpoint_callback], validation_data = val_gen, \n",
    "#                              shuffle = True, steps_per_epoch = len(train_gen)//batch_size, \n",
    "#                              validation_steps = len(val_gen)//batch_size)\n",
    "\n",
    "embedding_model.load_weights(save_path + 'bw_' + name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34cf8bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2D)             (None, 1, 1, 2048)        4196352   \n",
      "                                                                 \n",
      " layer2 (Conv2D)             (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,294,528\n",
      "Trainable params: 6,294,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "221/221 [==============================] - 0s 748us/step\n",
      "[[[[0.96928316 0.         1.6717384  ... 0.         0.\n",
      "    0.9863841 ]]]\n",
      "\n",
      "\n",
      " [[[1.4740046  0.         2.089198   ... 0.         0.\n",
      "    1.1985756 ]]]\n",
      "\n",
      "\n",
      " [[[1.2754202  0.         1.7703605  ... 0.         0.\n",
      "    0.1944209 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.4740258  0.         2.3032613  ... 0.         0.\n",
      "    1.0879954 ]]]\n",
      "\n",
      "\n",
      " [[[1.9626757  0.         2.1782138  ... 0.         0.\n",
      "    2.3894672 ]]]\n",
      "\n",
      "\n",
      " [[[1.9315006  0.         2.8737228  ... 0.         0.\n",
      "    1.5394818 ]]]] (7057, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "embedding_model_0 = Model(inputs = embedding_model.input, outputs = embedding_model.layers[-3].output) # for embedding model\n",
    "#embedding_model_0 = Model(inputs = embedding_model.input, outputs = embedding_model.layers[-2].output) # for embedding model variants\n",
    "embedding_model_0.summary()\n",
    "\n",
    "trainval_vec = embedding_model_0.predict(trainval_input1)\n",
    "print(trainval_vec, trainval_vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d47c5d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 1, 2048)]      0         \n",
      "                                                                 \n",
      " layer1 (Conv2DTranspose)    (None, 1, 1, 1024)        2098176   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,098,176\n",
      "Trainable params: 2,098,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "221/221 [==============================] - 0s 659us/step\n",
      "[[[[0.         0.         0.02944161 ... 0.00646748 0.028888\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.02042671 0.         0.04095376 ... 0.         0.04360095\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.01596911 ... 0.00080441 0.03779721\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.03696621 ... 0.01328099 0.02166263\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.02109868 0.         0.03102086 ... 0.         0.05733657\n",
      "    0.00202982]]]\n",
      "\n",
      "\n",
      " [[[0.02214308 0.01180177 0.02688769 ... 0.03007103 0.03275917\n",
      "    0.        ]]]] (7057, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "inputt = Input(shape = (1, 1, trainval_attributes.shape[1]))\n",
    "hidden1 = Conv2DTranspose(1024, 1, name=\"layer1\", activation='relu')(inputt)\n",
    "\n",
    "embedding_model_1 = Model(inputs = inputt, outputs = hidden1)\n",
    "embedding_model_1.summary()\n",
    "\n",
    "w_list = Model(inputs = embedding_model_0.layers[-2].output, outputs = embedding_model_0.output).get_weights()\n",
    "#print(weights_list2)\n",
    "\n",
    "embedding_model_1.set_weights([np.transpose(w_list[0], (0, 1, 3, 2)), w_list[1]])\n",
    "\n",
    "trainval_attributes = embedding_model_1.predict(trainval_input2)\n",
    "print(trainval_attributes, trainval_attributes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c3e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 997us/step\n",
      "[[[[0.01001195 0.00122673 0.         ... 0.00130253 0.06237516\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.01138241 ... 0.03856681 0.05814604\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.02133051 0.         0.04888634 ... 0.01241675 0.05244125\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.02042671 0.         0.04095376 ... 0.         0.04360095\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.04200585 ... 0.         0.03273274\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.02872183 0.         0.03969611 ... 0.         0.07809225\n",
      "    0.01184638]]]] (200, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attribute_new = np.reshape(attribute_new, [attribute_new.shape[0], 1, 1, attribute_new.shape[1]])\n",
    "\n",
    "# as labels range from 1 to 200, we have subtract 1\n",
    "attribute_2 = embedding_model_1.predict(attribute_new)\n",
    "print(attribute_2, attribute_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ecaa989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01001195 0.         0.02133051 ... 0.02042671 0.         0.02872183]\n",
      " [0.00122673 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01138241 0.04888634 ... 0.04095376 0.04200585 0.03969611]\n",
      " ...\n",
      " [0.00130253 0.03856681 0.01241675 ... 0.         0.         0.        ]\n",
      " [0.06237516 0.05814604 0.05244125 ... 0.04360095 0.03273274 0.07809225]\n",
      " [0.         0.         0.         ... 0.         0.         0.01184638]] (1024, 200)\n",
      "Signature for trainval: (1024, 150)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "signature_2 = attribute_2.transpose()\n",
    "signature_2 = np.reshape(signature_2, [signature_2.shape[0], signature_2.shape[3]])\n",
    "print(signature_2, signature_2.shape)#(1024, 200)\n",
    "\n",
    "trainval_sig = signature_2[:, (unique_labels_trainval)-1]\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)# (1024, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "581a4c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[146]\n",
      " [147]\n",
      " [ 26]\n",
      " ...\n",
      " [ 55]\n",
      " [115]\n",
      " [ 18]] (7057, 1)\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149] (150,)\n"
     ]
    }
   ],
   "source": [
    "# by doing this modification, we are changing the range of trainval and test seen labels from 0 to 149 \n",
    "# and test unseen labels from 0 to 49\n",
    "\n",
    "k = 0\n",
    "new_labels_trainval = np.zeros((len(labels_trainval), 1), dtype = 'int')\n",
    "for labels in unique_labels_trainval:\n",
    "    new_labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "    \n",
    "print(new_labels_trainval, new_labels_trainval.shape)#(7057, 1)\n",
    "print(np.unique(new_labels_trainval), np.unique(new_labels_trainval).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee1341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7057\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "#params for trainval and test set\n",
    "m_trainval = new_labels_trainval.shape[0]# number of instances in training set: 7057\n",
    "print(m_trainval)\n",
    "\n",
    "z_trainval = len(unique_labels_trainval)# number of classes in training set: 150\n",
    "print(z_trainval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "845b8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (7057, 150)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "gt_trainval = to_categorical(new_labels_trainval, z_trainval)\n",
    "\n",
    "print(gt_trainval, gt_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08067319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "input1_shape = trainval_vec.shape[3]\n",
    "print(input1_shape)\n",
    "\n",
    "attribute_shape = trainval_sig.shape[0]\n",
    "print(attribute_shape)\n",
    "\n",
    "output_shape = z_trainval\n",
    "print(output_shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b18da34",
   "metadata": {},
   "source": [
    "Step 6 - Define 'model2' for attribute to class label mapping\n",
    "         Define 'model1' for visual feature to class label mapping\n",
    "         Train 'model2' and 'model1' through the iterative process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96792076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1, 1, 1024)]      0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 150)               153750    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 153,750\n",
      "Trainable params: 153,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "\n",
    "# define model2 for attribute to class label mapping\n",
    "\n",
    "input2 = Input(shape = (1, 1, attribute_shape))\n",
    "flat = Flatten()(input2)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model2 = Model(inputs = input2, outputs = output)\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "#opt = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model2.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b285876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1, 1, 1024)]      0         \n",
      "                                                                 \n",
      " intermediate (Conv1D)       (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 150)               153750    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,203,350\n",
      "Trainable params: 1,203,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model1 for visual feature to class label mapping\n",
    "\n",
    "input1 = Input(shape = (1,1, input1_shape))\n",
    "#inter_pre = Dense(512, name=\"intermediate_previous\", activation='relu')(input1)\n",
    "inter = Conv1D(attribute_shape, kernel_size = 1, name = \"intermediate\", activation = 'linear')(input1)\n",
    "flat = Flatten()(inter)\n",
    "output = Dense(output_shape, name=\"output\", activation='softmax')(flat)\n",
    "\n",
    "model1 = Model(inputs = input1, outputs = output)\n",
    "\n",
    "opt = Adam(learning_rate = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0001)\n",
    "#opt = SGD(learning_rate = 1e-2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model1.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fd4ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7057, 1, 1, 1024)\n",
      "(7057, 1, 1, 1024)\n",
      "(7057, 150)\n"
     ]
    }
   ],
   "source": [
    "trainval_input1 = trainval_vec\n",
    "print(trainval_input1.shape)\n",
    "\n",
    "trainval_input2 = trainval_attributes\n",
    "print(trainval_input2.shape)\n",
    "\n",
    "trainval_output = gt_trainval\n",
    "print(trainval_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cec915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "batch_size = 8\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(trainval_input1, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen1 = DataGenerator(X_train1, y_train1, batch_size)   \n",
    "#val_gen1 = DataGenerator(X_val1, y_val1, batch_size)\n",
    "\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(trainval_input2, trainval_output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#train_gen2 = DataGenerator(X_train2, y_train2, batch_size)   \n",
    "#val_gen2 = DataGenerator(X_val2, y_val2, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3fb3ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [[151]\n",
      " [151]\n",
      " [151]\n",
      " ...\n",
      " [150]\n",
      " [150]\n",
      " [150]] (11788, 1)\n",
      "[    0     3     6 ... 11716 11717 11727] 11727\n",
      "labels_test_seen [[113]\n",
      " [107]\n",
      " [ 20]\n",
      " ...\n",
      " [136]\n",
      " [ 94]\n",
      " [180]] (1764, 1)\n",
      "unique_labels_test_seen [  1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  20\n",
      "  22  23  24  25  26  27  28  30  31  32  33  35  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  51  52  53  54  55  57  58  59  60  61  63\n",
      "  64  65  66  67  70  71  73  74  75  76  77  78  81  82  83  84  85  86\n",
      "  89  90  92  93  94  96  97  99 101 102 103 105 106 107 109 110 111 112\n",
      " 113 114 115 117 118 119 121 123 126 127 128 130 131 132 133 134 135 136\n",
      " 137 138 140 143 144 145 146 147 148 149 151 153 154 155 156 158 161 162\n",
      " 163 164 165 168 169 170 172 173 175 177 178 180 181 183 184 186 188 190\n",
      " 194 196 197 198 199 200] (150,)\n",
      "Features for test seen: (1764, 2048)\n",
      "56/56 [==============================] - 0s 753us/step\n",
      "(1764, 1, 1, 1024)\n",
      "[[[[0.         0.         0.00989528 ... 0.         0.05553835\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.04282717 0.01770117 ... 0.00291138 0.02439757\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.00873629 0.         0.02562955 ... 0.01789548 0.04370546\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.02335232 ... 0.         0.04200212\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.01812997 ... 0.         0.08430158\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.04025771 0.         ... 0.         0.03900988\n",
      "    0.        ]]]] (1764, 1, 1, 1024)\n",
      "Signature for test seen: (1024, 150)\n"
     ]
    }
   ],
   "source": [
    "labels = res101['labels']# direct class labels\n",
    "print('labels', labels, labels.shape)# 7057 x 1\n",
    "\n",
    "test_seen_loc = np.squeeze(att_splits['test_seen_loc']-1)\n",
    "print(np.unique(test_seen_loc), np.max(np.unique(test_seen_loc))) # smallest location: 0, largest location 11727\n",
    "\n",
    "labels_test_seen = labels[test_seen_loc]\n",
    "print('labels_test_seen', labels_test_seen, labels_test_seen.shape)\n",
    "\n",
    "unique_labels_test_seen = np.unique(labels_test_seen) # labels min:1 max:200\n",
    "print('unique_labels_test_seen', unique_labels_test_seen, unique_labels_test_seen.shape)# 150 classes\n",
    "\n",
    "test_seen_vec = X_features[:, test_seen_loc].transpose()\n",
    "print(\"Features for test seen:\", test_seen_vec.shape)# (1764, 2048)\n",
    "\n",
    "test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "test_seen_vec = embedding_model_0.predict(test_seen_vec)\n",
    "print(test_seen_vec.shape)\n",
    "\n",
    "\n",
    "test_seen_attributes = np.zeros((len(test_seen_loc), attribute_2.shape[1], attribute_2.shape[2], attribute_2.shape[3]))\n",
    "for i in range(len(test_seen_loc)):\n",
    "    test_seen_attributes[i] = attribute_2[int(labels_test_seen[i])-1]\n",
    "\n",
    "print(test_seen_attributes, test_seen_attributes.shape)# 1764, 1, 1, 1024\n",
    "\n",
    "\n",
    "\n",
    "test_seen_sig = signature_2[:, (unique_labels_test_seen)-1]\n",
    "print(\"Signature for test seen:\", test_seen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a33b196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  178   179   180 ... 11785 11786 11787] 11787\n",
      "labels_test_unseen [[152]\n",
      " [152]\n",
      " [152]\n",
      " ...\n",
      " [150]\n",
      " [150]\n",
      " [150]] (2967, 1)\n",
      "unique_labels_test_unseen [  7  19  21  29  34  36  50  56  62  68  69  72  79  80  87  88  91  95\n",
      "  98 100 104 108 116 120 122 124 125 129 139 141 142 150 152 157 159 160\n",
      " 166 167 171 174 176 179 182 185 187 189 191 192 193 195] (50,)\n",
      "Features for test unseen: (2967, 2048)\n",
      "93/93 [==============================] - 0s 759us/step\n",
      "(2967, 1, 1, 1024)\n",
      "Signature for test unseen: (1024, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_unseen_loc = np.squeeze(att_splits['test_unseen_loc']-1)\n",
    "print(np.unique(test_unseen_loc), np.max(np.unique(test_unseen_loc))) # smallest location: 1046, largest location 35290\n",
    "\n",
    "labels_test_unseen = labels[test_unseen_loc]\n",
    "print('labels_test_unseen', labels_test_unseen, labels_test_unseen.shape)\n",
    "\n",
    "unique_labels_test_unseen = np.unique(labels_test_unseen) # labels min:7 max:50\n",
    "print('unique_labels_test_unseen', unique_labels_test_unseen, unique_labels_test_unseen.shape)# 10 classes\n",
    "\n",
    "\n",
    "test_unseen_vec = X_features[:, test_unseen_loc].transpose()\n",
    "print(\"Features for test unseen:\", test_unseen_vec.shape) #(7913, 2048)\n",
    "\n",
    "\n",
    "test_unseen_vec = np.reshape(test_unseen_vec, [test_unseen_vec.shape[0], 1, 1, test_unseen_vec.shape[1]])\n",
    "test_unseen_vec = embedding_model_0.predict(test_unseen_vec)\n",
    "print(test_unseen_vec.shape)\n",
    "\n",
    "#test_unseen_attributes = np.zeros((len(test_unseen_loc), attribute_new.shape[1]))\n",
    "#for i in range(len(test_unseen_loc)):\n",
    "#    test_unseen_attributes[i] = attribute_new[int(labels_test_unseen[i])-1]\n",
    "\n",
    "#print(test_unseen_attributes, test_unseen_attributes.shape)# (7913, 85)\n",
    "\n",
    "\n",
    "#test_unseen_attributes_2 = model0.predict(test_unseen_attributes)\n",
    "#print(test_unseen_attributes_2, test_unseen_attributes_2.shape)\n",
    "\n",
    "test_unseen_sig = signature_2[:, (unique_labels_test_unseen)-1]\n",
    "\n",
    "print(\"Signature for test unseen:\", test_unseen_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00ef41ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90]\n",
      " [ 85]\n",
      " [ 17]\n",
      " ...\n",
      " [107]\n",
      " [ 76]\n",
      " [137]] (1764, 1)\n",
      "1764\n",
      "150\n",
      "[[32]\n",
      " [32]\n",
      " [32]\n",
      " ...\n",
      " [31]\n",
      " [31]\n",
      " [31]] (2967, 1)\n",
      "2967\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l = 0\n",
    "new_labels_test_seen = np.zeros((len(labels_test_seen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_seen:\n",
    "    new_labels_test_seen[labels_test_seen == labels] = l\n",
    "    l = l+1\n",
    "    \n",
    "print(new_labels_test_seen, new_labels_test_seen.shape)# (5882, 1)\n",
    "\n",
    "\n",
    "\n",
    "n_test_seen = new_labels_test_seen.shape[0]# 5882\n",
    "print(n_test_seen)\n",
    "\n",
    "z1_test_seen = len(unique_labels_test_seen)# 40\n",
    "print(z1_test_seen)\n",
    "\n",
    "\n",
    "m = 0\n",
    "new_labels_test_unseen = np.zeros((len(labels_test_unseen), 1), dtype = 'int')\n",
    "for labels in unique_labels_test_unseen:\n",
    "    new_labels_test_unseen[labels_test_unseen == labels] = m\n",
    "    m = m+1  \n",
    "\n",
    "print(new_labels_test_unseen, new_labels_test_unseen.shape) #  (7913, 1)  \n",
    "\n",
    "n_test_unseen = new_labels_test_unseen.shape[0]# 7913\n",
    "print(n_test_unseen)\n",
    "\n",
    "z1_test_unseen = len(unique_labels_test_unseen)# 10\n",
    "print(z1_test_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f527990a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.3417608439922333 , validation acc: 1.0 , validation_loss: 0.3107287883758545\n",
      "model 1 is trained: training acc: 0.9852941036224365 , training loss: 1.1257739067077637 , validation acc: 0.53125 , validation_loss: 133.14451599121094\n",
      "micro average\n",
      "seen accuracy: 46.94961226137696 unseen accuracy: 43.26442234408404 harmonic mean: 45.03174838927424\n",
      "macro average\n",
      "seen accuracy: 46.03174603174603 unseen accuracy: 43.10751600943714 harmonic mean: 44.52166606649862\n",
      "best accuracy micro seen accuracy: 46.94961226137696 unseen accuracy: 43.26442234408404 harmonic mean: 45.03174838927424\n",
      "best accuracy macro seen accuracy: 46.03174603174603 unseen accuracy: 43.10751600943714 harmonic mean: 44.52166606649862\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 1\n",
      "model 2 is trained: training acc: 0.9852941036224365 , training loss: 0.36965346336364746 , validation acc: 1.0 , validation_loss: 0.28458964824676514\n",
      "model 1 is trained: training acc: 0.970588207244873 , training loss: 2.907560348510742 , validation acc: 0.65625 , validation_loss: 112.2829818725586\n",
      "micro average\n",
      "seen accuracy: 46.58734974176153 unseen accuracy: 41.86252473950334 harmonic mean: 44.09874163304742\n",
      "macro average\n",
      "seen accuracy: 46.20181405895691 unseen accuracy: 41.72564880350522 harmonic mean: 43.8497962923004\n",
      "best accuracy micro seen accuracy: 46.94961226137696 unseen accuracy: 43.26442234408404 harmonic mean: 45.03174838927424\n",
      "best accuracy macro seen accuracy: 46.03174603174603 unseen accuracy: 43.10751600943714 harmonic mean: 44.52166606649862\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 2\n",
      "model 2 is trained: training acc: 0.9767441749572754 , training loss: 0.3763721287250519 , validation acc: 0.96875 , validation_loss: 0.3979935646057129\n",
      "model 1 is trained: training acc: 0.9852941036224365 , training loss: 0.38889381289482117 , validation acc: 0.65625 , validation_loss: 141.27088928222656\n",
      "micro average\n",
      "seen accuracy: 48.046188234423546 unseen accuracy: 41.805098351504945 harmonic mean: 44.70888956351476\n",
      "macro average\n",
      "seen accuracy: 47.67573696145125 unseen accuracy: 41.62453656892484 harmonic mean: 44.44511485012047\n",
      "best accuracy micro seen accuracy: 46.94961226137696 unseen accuracy: 43.26442234408404 harmonic mean: 45.03174838927424\n",
      "best accuracy macro seen accuracy: 46.03174603174603 unseen accuracy: 43.10751600943714 harmonic mean: 44.52166606649862\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 3\n",
      "model 2 is trained: training acc: 1.0 , training loss: 0.3106934428215027 , validation acc: 1.0 , validation_loss: 0.472087562084198\n",
      "model 1 is trained: training acc: 0.9779411554336548 , training loss: 1.318904161453247 , validation acc: 0.5625 , validation_loss: 172.52151489257812\n",
      "micro average\n",
      "seen accuracy: 43.00272581449055 unseen accuracy: 38.19910353859574 harmonic mean: 40.45883175086706\n",
      "macro average\n",
      "seen accuracy: 42.63038548752834 unseen accuracy: 38.01820020222447 harmonic mean: 40.19241047568145\n",
      "best accuracy micro seen accuracy: 46.94961226137696 unseen accuracy: 43.26442234408404 harmonic mean: 45.03174838927424\n",
      "best accuracy macro seen accuracy: 46.03174603174603 unseen accuracy: 43.10751600943714 harmonic mean: 44.52166606649862\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "iteration: 4\n",
      "model 2 is trained: training acc: 0.9926470518112183 , training loss: 0.31810635328292847 , validation acc: 0.96875 , validation_loss: 0.44262081384658813\n",
      "model 1 is trained: training acc: 0.9779411554336548 , training loss: 3.6635186672210693 , validation acc: 0.4375 , validation_loss: 214.96836853027344\n",
      "micro average\n",
      "seen accuracy: 44.92545754898697 unseen accuracy: 42.29350651116948 harmonic mean: 43.56977067635995\n",
      "macro average\n",
      "seen accuracy: 43.82086167800453 unseen accuracy: 42.13009774182676 harmonic mean: 42.958849978804004\n",
      "best accuracy micro seen accuracy: 46.94961226137696 unseen accuracy: 43.26442234408404 harmonic mean: 45.03174838927424\n",
      "best accuracy macro seen accuracy: 46.03174603174603 unseen accuracy: 43.10751600943714 harmonic mean: 44.52166606649862\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iteration = 5\n",
    "epochs1 = 100\n",
    "epochs2 = 200\n",
    "\n",
    "best_performance_micro = [0, 0, 0]\n",
    "best_performance_macro = [0, 0, 0]\n",
    "\n",
    "save_path = 'C:/Users/Admin/Sushree_Codes/Sush_3/Results/CUB/BertAttribute/with_LE/'\n",
    "name = 'Remodel1_conv_CUB_LE_BertattT'\n",
    "    \n",
    "for i in range(iteration):\n",
    "    X_train2_it = X_train2[(len(X_train2)//iteration)*i:(len(X_train2)//iteration)*(i+1)]\n",
    "    X_val2_it = X_val2[(len(X_val2)//iteration)*i:(len(X_val2)//iteration)*(i+1)]\n",
    "    y_train2_it = y_train2[(len(y_train2)//iteration)*i:(len(y_train2)//iteration)*(i+1)]\n",
    "    y_val2_it = y_val2[(len(y_val2)//iteration)*i:(len(y_val2)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen2 = DataGenerator(X_train2_it, y_train2_it, batch_size)   \n",
    "    val_gen2 = DataGenerator(X_val2_it, y_val2_it, batch_size)\n",
    "\n",
    "    train_summary2 = model2.fit(train_gen2, epochs = epochs2, verbose = 0, callbacks = None, validation_data = val_gen2, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen2)//batch_size, \n",
    "                              validation_steps = len(val_gen2)//batch_size)\n",
    "\n",
    "    print(\"iteration:\", i)\n",
    "    print('model 2 is trained:', 'training acc:', train_summary2.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary2.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary2.history['val_accuracy'][-1], ',',\n",
    "          'validation_loss:', train_summary2.history['val_loss'][-1])\n",
    "\n",
    "    weights_list2 = model2.get_weights()\n",
    "    #print(weights_list2)\n",
    "\n",
    "    model1.layers[-1].set_weights(weights_list2)\n",
    "\n",
    "    X_train1_it = X_train1[(len(X_train1)//iteration)*i:(len(X_train1)//iteration)*(i+1)]\n",
    "    X_val1_it = X_val1[(len(X_val1)//iteration)*i:(len(X_val1)//iteration)*(i+1)]\n",
    "    y_train1_it = y_train1[(len(y_train1)//iteration)*i:(len(y_train1)//iteration)*(i+1)]\n",
    "    y_val1_it = y_val1[(len(y_val1)//iteration)*i:(len(y_val1)//iteration)*(i+1)]\n",
    "    \n",
    "    train_gen1 = DataGenerator(X_train1_it, y_train1_it, batch_size)   \n",
    "    val_gen1 = DataGenerator(X_val1_it, y_val1_it, batch_size)\n",
    "    \n",
    "    for layer in model1.layers[2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    train_summary1 = model1.fit(train_gen1, epochs = epochs1, verbose = 0, callbacks = None, validation_data = val_gen1, \n",
    "                              shuffle = True, steps_per_epoch = len(train_gen1)//batch_size, \n",
    "                              validation_steps = len(val_gen1)//batch_size)\n",
    "    \n",
    "    #print(\"iteration:\", i)\n",
    "    print('model 1 is trained:', 'training acc:', train_summary1.history['accuracy'][-1], ',',  \n",
    "          'training loss:', train_summary1.history['loss'][-1], ',', \n",
    "          'validation acc:', train_summary1.history['val_accuracy'][-1], ',',\n",
    "         'validation_loss:', train_summary1.history['val_loss'][-1])\n",
    "    \n",
    "    weights_list1 = Model(inputs = model1.input, outputs = model1.layers[-1].output).get_weights()\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, np.matmul(weights_list1[0], weights_list1[2])), test_seen_sig)\n",
    "    #outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0]), test_seen_sig)\n",
    "    \n",
    "    outputs_seen = np.matmul(np.matmul(test_seen_vec, weights_list1[0][0]), test_seen_sig)\n",
    "    preds_seen = np.array([np.argmax(output) for output in outputs_seen])\n",
    "    \n",
    "    cm_seen = confusion_matrix(new_labels_test_seen, preds_seen)\n",
    "    #print(cm)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_seen_micro = cm_seen.astype('float') / cm_seen.sum(axis=1)[:, np.newaxis]\n",
    "    #print(cm)\n",
    "    avg_seen_micro = (sum(cm_seen_micro.diagonal())/len(unique_labels_test_seen))*100\n",
    "\n",
    "    avg_seen_macro = (sum(cm_seen.diagonal())/len(new_labels_test_seen))*100\n",
    "    \n",
    "    #predictions\n",
    "    #outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0]), test_unseen_sig)\n",
    "    outputs_unseen = np.matmul(np.matmul(test_unseen_vec, weights_list1[0][0]), test_unseen_sig)\n",
    "    \n",
    "    preds_unseen = np.array([np.argmax(output) for output in outputs_unseen])\n",
    "    \n",
    "    cm_unseen = confusion_matrix(new_labels_test_unseen, preds_unseen)\n",
    "    # Compute macro average (averaging performance metrics by first calculating the metric separately for each class and \n",
    "    # then averaging these class-specific metrics)\n",
    "    cm_unseen_micro = cm_unseen.astype('float') / cm_unseen.sum(axis=1)[:, np.newaxis]\n",
    "    avg_unseen_micro = (sum(cm_unseen_micro.diagonal())/len(unique_labels_test_unseen))*100\n",
    "\n",
    "    avg_unseen_macro = (sum(cm_unseen.diagonal())/len(new_labels_test_unseen))*100\n",
    "    \n",
    "    harmonic_micro = (2*avg_seen_micro*avg_unseen_micro) / (avg_seen_micro + avg_unseen_micro)\n",
    "    harmonic_macro = (2*avg_seen_macro*avg_unseen_macro) / (avg_seen_macro + avg_unseen_macro)\n",
    "    \n",
    "    print('micro average')\n",
    "    print('seen accuracy:', avg_seen_micro, 'unseen accuracy:', avg_unseen_micro, 'harmonic mean:', harmonic_micro)\n",
    "    \n",
    "    print('macro average')\n",
    "    print('seen accuracy:', avg_seen_macro, 'unseen accuracy:', avg_unseen_macro, 'harmonic mean:', harmonic_macro)\n",
    "    \n",
    "    if harmonic_micro > best_performance_micro[2]:\n",
    "        best_performance_micro = [avg_seen_micro, avg_unseen_micro, harmonic_micro]\n",
    "        model1.save_weights(save_path + 'bw_micro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    if harmonic_macro > best_performance_macro[2]:\n",
    "        best_performance_macro = [avg_seen_macro, avg_unseen_macro, harmonic_macro]\n",
    "        model1.save_weights(save_path + 'bw_macro_' + name + '.h5', overwrite=True)\n",
    "        \n",
    "    print('best accuracy micro','seen accuracy:', best_performance_micro[0], 'unseen accuracy:', best_performance_micro[1], 'harmonic mean:', best_performance_micro[2])\n",
    "    print('best accuracy macro', 'seen accuracy:', best_performance_macro[0], 'unseen accuracy:', best_performance_macro[1], 'harmonic mean:', best_performance_macro[2])\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    weights_list3 = model1.get_weights()\n",
    "    model2.set_weights(weights_list3[2:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47c29ad6",
   "metadata": {},
   "source": [
    "Step 7 - Evaluate for seen and unseen categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58bc47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (1764, 150)\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 169.2656 - accuracy: 0.5805\n",
      "cce =  6.6388063\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3626 - accuracy: 0.9932\n",
      "cce =  0.36259893\n"
     ]
    }
   ],
   "source": [
    "gt_test_seen = to_categorical(new_labels_test_seen, z1_test_seen)\n",
    "\n",
    "print(gt_test_seen, gt_test_seen.shape)\n",
    "\n",
    "#test_seen_vec = np.reshape(test_seen_vec, [test_seen_vec.shape[0], 1, 1, test_seen_vec.shape[1]])\n",
    "res1 = model1.evaluate(test_seen_vec, gt_test_seen)\n",
    "\n",
    "p1 = model1.predict(test_seen_vec, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p1).numpy())\n",
    "\n",
    "#test_seen_attributes = np.reshape(train_attributes, [test_seen_attributes.shape[0], 1, train_attributes.shape[1]])\n",
    "#test_seen_attributes = np.reshape(test_seen_attributes, [test_seen_attributes.shape[0], 1, 1, test_seen_attributes.shape[1]])\n",
    "\n",
    "res2 = model2.evaluate(test_seen_attributes, gt_test_seen)\n",
    "\n",
    "p2 = model2.predict(test_seen_attributes, verbose = 0)\n",
    "\n",
    "import tensorflow\n",
    "cce = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "print('cce = ', cce(gt_test_seen, p2).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c3d7cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.57538091726869\n",
      "78.68480682373047\n",
      "55.82697339649531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_seen_updated = res1[1]*100\n",
    "unseen_accuracy = 43.26\n",
    "h = (2*accuracy_seen_updated*unseen_accuracy) / (accuracy_seen_updated + unseen_accuracy)\n",
    "print(h)\n",
    "\n",
    "\n",
    "accuracy_seen_updated2 = ((res1[1]*100)+(res2[1]*100))/2\n",
    "print(accuracy_seen_updated2)\n",
    "h = (2*accuracy_seen_updated2*unseen_accuracy) / (accuracy_seen_updated2 + unseen_accuracy)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adb1f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_seen_macro 0.8107142469382072 recall_seen_macro 0.7941405568832038 f1_seen_macro 0.78318607639317\n",
      "precision_seen_micro 0.7868480725623583 recall_seen_micro 0.7868480725623583 f1_seen_micro 0.7868480725623583\n",
      "precision_unseen_macro 0.4734833441734912 recall_unseen_macro 0.4229350651116947 f1_unseen_macro 0.37675080224968105\n",
      "precision_unseen_micro 0.4213009774182676 recall_unseen_micro 0.4213009774182676 f1_unseen_micro 0.4213009774182676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\en3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pp1 = np.array([np.argmax(output) for output in p1])\n",
    "pp2 = np.array([np.argmax(output) for output in p2])\n",
    "\n",
    "seen_macro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'macro')\n",
    "seen_macro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'macro')\n",
    "print('precision_seen_macro', (seen_macro1[0] + seen_macro2[0])/2, 'recall_seen_macro', (seen_macro1[1] + seen_macro2[1])/2, 'f1_seen_macro', (seen_macro1[2] + seen_macro2[2])/2)\n",
    "\n",
    "\n",
    "seen_micro1 = precision_recall_fscore_support(new_labels_test_seen, pp1, average = 'micro')\n",
    "seen_micro2 = precision_recall_fscore_support(new_labels_test_seen, pp2, average = 'micro')\n",
    "print('precision_seen_micro', (seen_micro1[0] + seen_micro2[0])/2, 'recall_seen_micro', (seen_micro1[1] + seen_micro2[1])/2, 'f1_seen_micro', (seen_micro1[2] + seen_micro2[2])/2)\n",
    "\n",
    "unseen_macro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'macro')\n",
    "unseen_micro = precision_recall_fscore_support(new_labels_test_unseen, preds_unseen, average = 'micro')\n",
    "\n",
    "print('precision_unseen_macro', unseen_macro[0], 'recall_unseen_macro', unseen_macro[1], 'f1_unseen_macro', unseen_macro[2])\n",
    "print('precision_unseen_micro', unseen_micro[0], 'recall_unseen_micro', unseen_micro[1], 'f1_unseen_micro', unseen_micro[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc438ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
