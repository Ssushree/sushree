{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e38d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from time import gmtime, strftime\n",
    "#from models import *\n",
    "#from dataset_GBU import FeatDataLayer, DATA_LOADER\n",
    "from utils import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.backends.cudnn as cudnn\n",
    "import classifier\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd5f3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  5606\n",
      "Running parameters:\n"
     ]
    }
   ],
   "source": [
    "dataset_name ='SUN'#(default) #help='dataset: CUB, AWA2, APY, FLO, SUN'\n",
    "dataroot = 'C:/Sushree/Jio_Institute/Dataset/SDGZSL_data'#(default) #help='path to dataset'\n",
    "\n",
    "workers = 1# type=int, help='number of data loading workers', default=1\n",
    "\n",
    "image_embedding ='res101'#(default) # type=str\n",
    "class_embedding ='att'#(default) #type=str\n",
    "\n",
    "gen_nepoch = 400 # type=int, default=400, help='number of epochs to train for'\n",
    "lr = 0.0001 # type=float, default=0.0001, help='learning rate to train generater'\n",
    "\n",
    "zsl = False # type=bool, default=False, help='Evaluate ZSL or GZSL'\n",
    "finetune = False # type=bool, default=False, help='Use fine-tuned feature'\n",
    "ga = 15 # type=float, default=15, help='relationNet weight')\n",
    "beta = 1 #', type=float, default=1, help='tc weight')\n",
    "weight_decay = 1e-6 #', type=float, default=1e-6, help='weight_decay')\n",
    "dis = 3#', type=float, default=3, help='Discriminator weight')\n",
    "dis_step = 2#', type=float, default=2, help='Discriminator update interval')\n",
    "kl_warmup = 0.01#', type=float, default=0.01, help='kl warm-up for VAE')\n",
    "tc_warmup = 0.001#', type=float, default=0.001, help='tc warm-up')\n",
    "\n",
    "vae_dec_drop = 0.5#', type=float, default=0.5, help='dropout rate in the VAE decoder')\n",
    "vae_enc_drop = 0.4#', type=float, default=0.4, help='dropout rate in the VAE encoder')\n",
    "ae_drop = 0.2#', type=float, default=0.2, help='dropout rate in the auto-encoder')\n",
    "\n",
    "classifier_lr = 0.001#', type=float, default=0.001, help='learning rate to train softmax classifier')\n",
    "classifier_steps = 50#', type=int, default=50, help='training steps of the classifier')\n",
    "\n",
    "batchsize = 64#', type=int, default=64, help='input batch size')\n",
    "nSample = 1200#', type=int, default=1200, help='number features to generate per class')\n",
    "\n",
    "disp_interval = 200#', type=int, default=200)\n",
    "save_interval = 10000#', type=int, default=10000)\n",
    "evl_interval = 400#',  type=int, default=400)\n",
    "evl_start = 0#',  type=int, default=0)\n",
    "manualSeed = 5606#', type=int, default=5606, help='manual seed')\n",
    "\n",
    "latent_dim = 20#', type=int, default=20, help='dimention of latent z')\n",
    "q_z_nn_output_dim = 128#', type=int, default=128, help='dimention of hidden layer in encoder')\n",
    "S_dim = 1024#', type=int, default=1024)\n",
    "NS_dim = 1024#', type=int, default=1024)\n",
    "\n",
    "#parser.add_argument('--gpu', default='0', type=str, help='index of GPU to use')\n",
    "#opt = parser.parse_args()\n",
    "\n",
    "if manualSeed is None:\n",
    "    manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "\n",
    "np.random.seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "print('Running parameters:')\n",
    "#print(json.dumps(vars(opt), indent=4, separators=(',', ': ')))\n",
    "#opt.gpu = torch.device(\"cuda:\"+opt.gpu if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7b1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label(label, classes):\n",
    "    mapped_label = torch.LongTensor(label.size())\n",
    "    for i in range(classes.size(0)):\n",
    "        mapped_label[label == classes[i]] = i\n",
    "    return mapped_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8224c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA_LOADER():\n",
    "    def __init__(self, finetune, dataset_name, dataroot, class_embedding, image_embedding):\n",
    "        self.finetune = finetune\n",
    "        \n",
    "        if dataset_name in ['FLO_EPGN','CUB_STC']:\n",
    "            if self.finetune:\n",
    "                self.read_fine_tune(dataset_name, dataroot, class_embedding, image_embedding)\n",
    "            else:\n",
    "                self.read(dataset_name, dataroot)\n",
    "        elif dataset_name in ['CUB', 'AWA2', 'APY', 'FLO', 'SUN']:\n",
    "            self.read_matdataset(dataroot, dataset_name, image_embedding, class_embedding)\n",
    "            \n",
    "        self.index_in_epoch = 0\n",
    "        self.epochs_completed = 0\n",
    "        self.feature_dim = self.train_feature.shape[1]\n",
    "        self.att_dim = self.attribute.shape[1]\n",
    "        self.text_dim = self.att_dim\n",
    "        self.tr_cls_centroid = np.zeros([self.seenclasses.shape[0], self.feature_dim], np.float32)\n",
    "        \n",
    "        for i in range(self.seenclasses.shape[0]):\n",
    "            self.tr_cls_centroid[i] = np.mean(self.train_feature[self.train_label == i].numpy(), axis=0)\n",
    "\n",
    "    def read_fine_tune(self, dataset_name, dataroot, class_embedding, image_embedding):\n",
    "        if dataset_name == \"CUB_STC\":\n",
    "            dataset_name = \"CUB\"\n",
    "        if dataset_name == \"FLO_EPGN\":\n",
    "            dataset_name = \"FLO\"\n",
    "\n",
    "        # matcontent = sio.loadmat(opt.dataroot + \"/\" + opt.dataset + \"/\" + opt.image_embedding + \".mat\")\n",
    "        # feature = matcontent['features'].T\n",
    "        # label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset_name + \"/\" + class_embedding + \"_splits.mat\")\n",
    "        trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "        train_loc = matcontent['train_loc'].squeeze() - 1\n",
    "        val_unseen_loc = matcontent['val_loc'].squeeze() - 1\n",
    "        test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "        test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "        self.attribute = torch.from_numpy(matcontent['att'].T).float()\n",
    "        # matcontent = sio.loadmat(opt.dataroot + \"/\" + opt.dataset + \"/cub_feat.mat\")\n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset_name + \"/\" + image_embedding + \"_finetuned.mat\")\n",
    "\n",
    "        feature = matcontent['features'].T\n",
    "        label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "        # feature = matcontent['features'].T\n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset_name + \"/data.mat\")\n",
    "        # feature = matcontent['features'].T\n",
    "        # label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "        train_att = matcontent['att_train']\n",
    "        seen_pro = matcontent['seen_pro']\n",
    "        attribute = matcontent['attribute']\n",
    "        unseen_pro = matcontent['unseen_pro']\n",
    "        self.attribute = torch.from_numpy(attribute).float()\n",
    "        self.train_att = seen_pro.astype(np.float32)\n",
    "        self.test_att = unseen_pro.astype(np.float32)\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        _train_feature = scaler.fit_transform(feature[trainval_loc])\n",
    "        _test_seen_feature = scaler.transform(feature[test_seen_loc])\n",
    "        _test_unseen_feature = scaler.transform(feature[test_unseen_loc])\n",
    "        self.train_feature = torch.from_numpy(_train_feature).float()\n",
    "        mx = self.train_feature.max()\n",
    "        self.train_feature.mul_(1 / mx)\n",
    "        self.train_label = torch.from_numpy(label[trainval_loc]).long()\n",
    "        self.test_unseen_feature = torch.from_numpy(_test_unseen_feature).float()\n",
    "        self.test_unseen_feature.mul_(1 / mx)\n",
    "        self.test_unseen_label = torch.from_numpy(label[test_unseen_loc]).long()\n",
    "        self.test_seen_feature = torch.from_numpy(_test_seen_feature).float()\n",
    "        self.test_seen_feature.mul_(1 / mx)\n",
    "        self.test_seen_label = torch.from_numpy(label[test_seen_loc]).long()\n",
    "\n",
    "        self.seenclasses = torch.from_numpy(np.unique(self.train_label.numpy()))\n",
    "        self.unseenclasses = torch.from_numpy(np.unique(self.test_unseen_label.numpy()))\n",
    "        self.ntrain = self.train_feature.size()[0]\n",
    "        self.ntrain_class = self.seenclasses.size(0)\n",
    "        self.ntest_class = self.unseenclasses.size(0)\n",
    "        self.train_class = self.seenclasses.clone()\n",
    "        self.allclasses = torch.arange(0, self.ntrain_class + self.ntest_class).long()\n",
    "\n",
    "        self.train_label = map_label(self.train_label, self.seenclasses)\n",
    "        self.test_unseen_label = map_label(self.test_unseen_label, self.unseenclasses)\n",
    "        self.test_seen_label = map_label(self.test_seen_label, self.seenclasses)\n",
    "\n",
    "        self.train_att = self.attribute[self.seenclasses].numpy()\n",
    "        self.test_att = self.attribute[self.unseenclasses].numpy()\n",
    "\n",
    "    def read(self, dataset_name, dataroot):\n",
    "        if dataset_name == \"CUB_STC\":\n",
    "            dataset_name = \"CUB\"\n",
    "        if dataset_name == \"FLO_EPGN\":\n",
    "            dataset_name = \"FLO\"\n",
    "\n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset_name + \"/data.mat\")\n",
    "        train_att = matcontent['att_train']\n",
    "        seen_pro = matcontent['seen_pro']\n",
    "        attribute = matcontent['attribute']\n",
    "        unseen_pro = matcontent['unseen_pro']\n",
    "        self.attribute = torch.from_numpy(attribute).float()\n",
    "        self.train_att = seen_pro.astype(np.float32)\n",
    "        self.test_att = unseen_pro.astype(np.float32)\n",
    "\n",
    "        train_fea = matcontent['train_fea']\n",
    "        test_seen_fea = matcontent['test_seen_fea']\n",
    "        test_unseen_fea = matcontent['test_unseen_fea']\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        _train_feature = scaler.fit_transform(train_fea)\n",
    "        _test_seen_feature = scaler.transform(test_seen_fea)\n",
    "        _test_unseen_feature = scaler.transform(test_unseen_fea)\n",
    "        mx = _train_feature.max()\n",
    "        train_fea = train_fea * (1 / mx)\n",
    "        test_seen_fea = test_seen_fea * (1 / mx)\n",
    "        test_unseen_fea = test_unseen_fea * (1 / mx)\n",
    "\n",
    "        self.train_feature = torch.from_numpy(train_fea).float()\n",
    "        self.test_seen_feature = torch.from_numpy(test_seen_fea).float()\n",
    "        self.test_unseen_feature = torch.from_numpy(test_unseen_fea).float()\n",
    "\n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset_name + \"/label.mat\")\n",
    "\n",
    "        train_idx = matcontent['train_idx'] - 1\n",
    "        train_label = matcontent['train_label_new']\n",
    "        test_unseen_idex = matcontent['test_unseen_idex'] - 1\n",
    "        test_seen_idex = matcontent['test_seen_idex'] - 1\n",
    "        self.train_label = torch.from_numpy(train_idx.squeeze()).long()\n",
    "        self.test_seen_label = torch.from_numpy(test_seen_idex.squeeze()).long()\n",
    "        self.test_unseen_label = torch.from_numpy(test_unseen_idex.squeeze()).long()\n",
    "\n",
    "        self.seenclasses = torch.from_numpy(np.unique(self.test_seen_label.numpy()))\n",
    "        self.unseenclasses = torch.from_numpy(np.unique(self.test_unseen_label.numpy()))\n",
    "        self.ntrain = self.train_feature.size()[0]\n",
    "        self.ntrain_class = self.seenclasses.size(0)\n",
    "        self.ntest_class = self.unseenclasses.size(0)\n",
    "        self.train_class = self.seenclasses.clone()\n",
    "        self.allclasses = torch.arange(0, self.ntrain_class + self.ntest_class).long()\n",
    "\n",
    "        self.test_unseen_label = map_label(self.test_unseen_label, self.unseenclasses)\n",
    "        self.test_seen_label = map_label(self.test_seen_label, self.seenclasses)\n",
    "\n",
    "    def read_matdataset(self, dataroot, dataset_name, image_embedding, class_embedding):\n",
    "\n",
    "\n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset_name + \"/\" + image_embedding + \".mat\")\n",
    "        label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "        if self.finetune:\n",
    "            matcontent = sio.loadmat(dataroot + \"/\" + dataset_name + \"/\" + image_embedding + \"_finetuned.mat\")\n",
    "            # label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "\n",
    "        feature = matcontent['features'].T\n",
    "        if dataset_name == \"APY\" and self.finetune:\n",
    "            feature = feature.T\n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset_name + \"/\" + class_embedding + \"_splits.mat\")\n",
    "        trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "        train_loc = matcontent['train_loc'].squeeze() - 1\n",
    "        val_unseen_loc = matcontent['val_loc'].squeeze() - 1\n",
    "        test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "        test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "        self.attribute = torch.from_numpy(matcontent['att'].T).float()\n",
    "        # if opt.dataset == \"FLO\":\n",
    "        #     temp_norm = torch.norm(self.attribute, p=2, dim=1).unsqueeze(1).expand_as(self.attribute)\n",
    "        #     self.attribute = self.attribute.div(temp_norm + 1e-5)\n",
    "\n",
    "        #\n",
    "        #\n",
    "        # matcontent = sio.loadmat(opt.dataroot + \"/\" + opt.dataset + \"/data.mat\")\n",
    "        #\n",
    "        # train_att = matcontent['att_train']\n",
    "        # seen_pro = matcontent['seen_pro']\n",
    "        # attribute = matcontent['attribute']\n",
    "        # unseen_pro = matcontent['unseen_pro']\n",
    "        # self.attribute = torch.from_numpy(attribute).float()\n",
    "        # self.train_att = seen_pro.astype(np.float32)\n",
    "        # self.test_att = unseen_pro.astype(np.float32)\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        _train_feature = scaler.fit_transform(feature[trainval_loc])\n",
    "        _test_seen_feature = scaler.transform(feature[test_seen_loc])\n",
    "        _test_unseen_feature = scaler.transform(feature[test_unseen_loc])\n",
    "        self.train_feature = torch.from_numpy(_train_feature).float()\n",
    "        mx = self.train_feature.max()\n",
    "        self.train_feature.mul_(1 / mx)\n",
    "        self.train_label = torch.from_numpy(label[trainval_loc]).long()\n",
    "        self.test_unseen_feature = torch.from_numpy(_test_unseen_feature).float()\n",
    "        self.test_unseen_feature.mul_(1 / mx)\n",
    "        self.test_unseen_label = torch.from_numpy(label[test_unseen_loc]).long()\n",
    "        self.test_seen_feature = torch.from_numpy(_test_seen_feature).float()\n",
    "        self.test_seen_feature.mul_(1 / mx)\n",
    "        self.test_seen_label = torch.from_numpy(label[test_seen_loc]).long()\n",
    "\n",
    "        self.seenclasses = torch.from_numpy(np.unique(self.train_label.numpy()))\n",
    "        self.unseenclasses = torch.from_numpy(np.unique(self.test_unseen_label.numpy()))\n",
    "        self.ntrain = self.train_feature.size()[0]\n",
    "        self.ntrain_class = self.seenclasses.size(0)\n",
    "        self.ntest_class = self.unseenclasses.size(0)\n",
    "        self.train_class = self.seenclasses.clone()\n",
    "        self.allclasses = torch.arange(0, self.ntrain_class + self.ntest_class).long()\n",
    "\n",
    "        self.train_label = map_label(self.train_label, self.seenclasses)\n",
    "        self.test_unseen_label = map_label(self.test_unseen_label, self.unseenclasses)\n",
    "        self.test_seen_label = map_label(self.test_seen_label, self.seenclasses)\n",
    "\n",
    "        self.train_att = self.attribute[self.seenclasses].numpy()\n",
    "        self.test_att = self.attribute[self.unseenclasses].numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatDataLayer():\n",
    "    def __init__(self, label, feat_data, batchsize):\n",
    "        \"\"\"Set the roidb to be used by this layer during training.\"\"\"\n",
    "        assert len(label) == feat_data.shape[0]\n",
    "        #self._opt = opt\n",
    "        self._feat_data = feat_data\n",
    "        self._label = label\n",
    "        self._shuffle_roidb_inds()\n",
    "        self._epoch = 0\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "    def _shuffle_roidb_inds(self):\n",
    "        \"\"\"Randomly permute the training roidb.\"\"\"\n",
    "        self._perm = np.random.permutation(np.arange(len(self._label)))\n",
    "        self._cur = 0\n",
    "\n",
    "    def _get_next_minibatch_inds(self):\n",
    "        \"\"\"Return the roidb indices for the next minibatch.\"\"\"\n",
    "        if self._cur + self.batchsize >= len(self._label):\n",
    "            self._shuffle_roidb_inds()\n",
    "            self._epoch += 1\n",
    "\n",
    "        db_inds = self._perm[self._cur:self._cur + self.batchsize]\n",
    "        self._cur += self.batchsize\n",
    "\n",
    "        return db_inds\n",
    "\n",
    "    def forward(self):\n",
    "        new_epoch = False\n",
    "        if self._cur + self.batchsize >= len(self._label):\n",
    "            self._shuffle_roidb_inds()\n",
    "            self._epoch += 1\n",
    "            new_epoch = True\n",
    "\n",
    "        db_inds = self._perm[self._cur:self._cur + self.batchsize]\n",
    "        self._cur += self.batchsize\n",
    "\n",
    "        minibatch_feat = np.array([self._feat_data[i] for i in db_inds])\n",
    "        minibatch_label = np.array([self._label[i] for i in db_inds])\n",
    "        blobs = {'data': minibatch_feat, 'labels': minibatch_label, 'newEpoch': new_epoch, 'idx': db_inds}\n",
    "        return blobs\n",
    "\n",
    "    def get_whole_data(self):\n",
    "        blobs = {'data': self._feat_data, 'labels': self._label}\n",
    "        return blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c447dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, Z_dim, S_dim, q_z_nn_output_dim, X_dim, C_dim, vae_enc_drop, vae_dec_drop):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_size = Z_dim\n",
    "        self.input_size = S_dim\n",
    "        #self.args = args\n",
    "        self.q_z_nn_output_dim = q_z_nn_output_dim\n",
    "        self.X_dim = X_dim\n",
    "        self.C_dim = C_dim\n",
    "        self.vae_enc_drop = vae_enc_drop\n",
    "        self.vae_dec_drop = vae_dec_drop\n",
    "        self.q_z_nn, self.q_z_mean, self.q_z_var = self.create_encoder()\n",
    "        self.p_x_nn, self.p_x_mean = self.create_decoder()\n",
    "        self.FloatTensor = torch.FloatTensor\n",
    "\n",
    "    def create_encoder(self):\n",
    "        q_z_nn = nn.Sequential(\n",
    "            nn.Linear(self.X_dim + self.C_dim, 2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.Dropout(self.vae_enc_drop),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, self.q_z_nn_output_dim)\n",
    "        )\n",
    "        q_z_mean = nn.Linear(self.q_z_nn_output_dim, self.z_size)\n",
    "\n",
    "        q_z_var = nn.Sequential(\n",
    "            nn.Linear(self.q_z_nn_output_dim, self.z_size),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "        return q_z_nn, q_z_mean, q_z_var\n",
    "\n",
    "\n",
    "    def create_decoder(self):\n",
    "        p_x_nn = nn.Sequential(\n",
    "            nn.Linear(self.z_size + self.C_dim, 2048),\n",
    "            nn.Dropout(self.vae_dec_drop),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048, 0.8),\n",
    "            nn.Dropout(self.vae_dec_drop),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048, 0.8),\n",
    "            nn.Dropout(self.vae_dec_drop),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        p_x_mean = nn.Sequential(\n",
    "            nn.Linear(2048, self.X_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        return p_x_nn, p_x_mean\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, var):\n",
    "        std = var.sqrt()\n",
    "        eps = self.FloatTensor(std.size()).normal_()#.to(self.gpu)\n",
    "        eps = Variable(eps)\n",
    "        z = eps.mul(std).add_(mu)\n",
    "        return z\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        input = torch.cat((x,c),1)\n",
    "        h = self.q_z_nn(input)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        mean = self.q_z_mean(h)\n",
    "        var = self.q_z_var(h)\n",
    "        return mean, var\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        input = torch.cat((z, c), 1)\n",
    "        h = self.p_x_nn(input)\n",
    "        x_mean = self.p_x_mean(h)\n",
    "        return x_mean\n",
    "\n",
    "    def forward(self, x, c, weights=None):\n",
    "        z_mu, z_var = self.encode(x, c)\n",
    "        z = self.reparameterize(z_mu, z_var)\n",
    "        x_mean = self.decode(z, c)\n",
    "        return x_mean, z_mu, z_var, z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34477b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, S_dim, dataset):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.cls = nn.Linear(S_dim, dataset.ntrain_class) #FLO 82\n",
    "        self.logic = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.logic(self.cls(s))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c392dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, X_dim, S_dim, NS_dim, ae_drop):\n",
    "        super(AE, self).__init__()\n",
    "        #self.args = args\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(X_dim, S_dim + NS_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(ae_drop)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(S_dim + NS_dim, 2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(ae_drop),\n",
    "            nn.Linear(2048, X_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(ae_drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.S_dim = S_dim\n",
    "        z = self.encoder(x)\n",
    "        s = z[:, :self.S_dim]\n",
    "        ns = z[:, self.S_dim:]\n",
    "        x1 = self.decoder(z)\n",
    "        return x1, z, s, ns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationNet(nn.Module):\n",
    "    def __init__(self, C_dim, S_dim):\n",
    "        super(RelationNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(C_dim + S_dim, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1)\n",
    "\n",
    "    def forward(self, s, c):\n",
    "\n",
    "        c_ext = c.unsqueeze(0).repeat(s.shape[0], 1, 1)\n",
    "        cls_num = c_ext.shape[1]\n",
    "\n",
    "        s_ext = torch.transpose(s.unsqueeze(0).repeat(cls_num, 1, 1), 0, 1)\n",
    "        relation_pairs = torch.cat((s_ext, c_ext), 2).view(-1, c.shape[1] + s.shape[1])\n",
    "        relation = nn.ReLU()(self.fc1(relation_pairs))\n",
    "        relation = nn.Sigmoid()(self.fc2(relation))\n",
    "        return relation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, S_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(S_dim*2, 2)\n",
    "\n",
    "    def forward(self, s):\n",
    "        score = self.fc1(s)\n",
    "        return nn.Sigmoid()(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd317f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "dataset_name = 'AWA2' \n",
    "ga  = 0.5 \n",
    "beta = 1 \n",
    "dis = 0.3 \n",
    "nSample = 5000 \n",
    "#gpu = 1 \n",
    "S_dim = 1024 \n",
    "NS_dim = 1024 \n",
    "lr = 0.00003\n",
    "classifier_lr = 0.003 \n",
    "kl_warmup = 0.01 \n",
    "tc_warmup = 0.001 \n",
    "vae_dec_drop = 0.5 \n",
    "vae_enc_drop = 0.4 \n",
    "dis_step = 2\n",
    "ae_drop = 0.2 \n",
    "gen_nepoch = 100#220 \n",
    "evl_start = 40000 \n",
    "evl_interval = 100 \n",
    "manualSeed = 6152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5222c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(dataset_name, dataroot, workers, image_embedding, class_embedding, gen_nepoch, lr, zsl, finetune, ga, beta, \n",
    "          weight_decay, dis, dis_step, kl_warmup, tc_warmup, vae_dec_drop, vae_enc_drop, ae_drop, classifier_lr, \n",
    "          classifier_steps, batchsize, nSample, disp_interval, save_interval, evl_interval, evl_start, manualSeed, latent_dim, \n",
    "          q_z_nn_output_dim, S_dim, NS_dim):\n",
    "       \n",
    "    dataset = DATA_LOADER(finetune, dataset_name, dataroot, class_embedding, image_embedding)\n",
    "    C_dim = dataset.att_dim\n",
    "    X_dim = dataset.feature_dim\n",
    "    Z_dim = latent_dim\n",
    "    y_dim = dataset.ntrain_class\n",
    "    \n",
    "    out_dir = 'out/{}/wd-{}_b-{}_g-{}_lr-{}_sd-{}_dis-{}_nS-{}_nZ-{}_bs-{}'.format(dataset_name, weight_decay, beta, ga, lr, S_dim, dis, nSample, Z_dim, batchsize)\n",
    "    print(out_dir)\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(\"The output dictionary is {}\".format(out_dir))\n",
    "\n",
    "    log_dir = out_dir + '/log_{}.txt'.format(dataset_name)\n",
    "    with open(log_dir, 'w') as f:\n",
    "        f.write('Training Start:')\n",
    "        f.write(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime()) + '\\n')\n",
    "\n",
    "    dataset.feature_dim = dataset.train_feature.shape[1]\n",
    "    X_dim = dataset.feature_dim\n",
    "    Z_dim = latent_dim\n",
    "    y_dim = dataset.ntrain_class\n",
    "\n",
    "    data_layer = FeatDataLayer(dataset.train_label.numpy(), dataset.train_feature.cpu().numpy(), batchsize)\n",
    "\n",
    "    niter = int(dataset.ntrain/batchsize) * gen_nepoch\n",
    "\n",
    "    result_gzsl_soft = Result()\n",
    "    result_zsl_soft = Result()\n",
    "\n",
    "    model = VAE(Z_dim, S_dim, q_z_nn_output_dim, X_dim, C_dim, vae_enc_drop, vae_dec_drop)#.to(opt.gpu)\n",
    "    relationNet = RelationNet(C_dim, S_dim)#.to(opt.gpu)\n",
    "    discriminator = Discriminator(S_dim)#.to(opt.gpu)\n",
    "    ae = AE(X_dim, S_dim, NS_dim, ae_drop)#.to(opt.gpu)\n",
    "    print(model)\n",
    "\n",
    "    with open(log_dir, 'a') as f:\n",
    "        f.write('\\n')\n",
    "        f.write('Generative Model Training Start:')\n",
    "        f.write(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime()) + '\\n')\n",
    "\n",
    "    start_step = 0\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    relation_optimizer = optim.Adam(relationNet.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    dis_optimizer = optim.Adam(discriminator.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    ae_optimizer = optim.Adam(ae.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    ones = torch.ones(batchsize, dtype = torch.long)#, device=opt.gpu)\n",
    "    zeros = torch.zeros(batchsize, dtype = torch.long)#, device=opt.gpu)\n",
    "    mse = nn.MSELoss()#.to(opt.gpu)\n",
    "\n",
    "\n",
    "    iters = math.ceil(dataset.ntrain/batchsize)\n",
    "    beta = 0.01\n",
    "    coin = 0\n",
    "    gamma = 0\n",
    "    for it in range(start_step, niter+1):\n",
    "\n",
    "        if it % iters == 0:\n",
    "            beta = min(kl_warmup*(it/iters), 1)\n",
    "            gamma = min(tc_warmup * (it / iters), 1)\n",
    "\n",
    "        blobs = data_layer.forward()\n",
    "        feat_data = blobs['data']\n",
    "        labels_numpy = blobs['labels'].astype(int)\n",
    "        labels = torch.from_numpy(labels_numpy.astype('int'))#.to(opt.gpu)\n",
    "\n",
    "        C = np.array([dataset.train_att[i,:] for i in labels])\n",
    "        C = torch.from_numpy(C.astype('float32'))#.to(opt.gpu)\n",
    "        X = torch.from_numpy(feat_data)#.to(opt.gpu)\n",
    "        \n",
    "        sample_C = torch.from_numpy(np.array([dataset.train_att[i, :] for i in labels.unique()]))#.to(opt.gpu)\n",
    "        sample_C_n = labels.unique().shape[0]\n",
    "        sample_label = labels.unique().cpu()\n",
    "\n",
    "        x_mean, z_mu, z_var, z = model(X, C)\n",
    "        loss, ce, kl = multinomial_loss_function(x_mean, X, z_mu, z_var, z, beta = beta)\n",
    "\n",
    "        sample_labels = np.array(sample_label)\n",
    "        re_batch_labels = []\n",
    "        for label in labels_numpy:\n",
    "            index = np.argwhere(sample_labels == label)\n",
    "            re_batch_labels.append(index[0][0])\n",
    "        re_batch_labels = torch.LongTensor(re_batch_labels)\n",
    "        one_hot_labels = torch.zeros(batchsize, sample_C_n).scatter_(1, re_batch_labels.view(-1, 1), 1)#.to(opt.gpu)\n",
    "\n",
    "        # one_hot_labels = torch.tensor(\n",
    "        #     torch.zeros(opt.batchsize, sample_C_n).scatter_(1, re_batch_labels.view(-1, 1), 1)).to(opt.gpu)\n",
    "\n",
    "        x1, h1, hs1, hn1 = ae(x_mean)\n",
    "        relations = relationNet(hs1, sample_C)\n",
    "        relations = relations.view(-1, labels.unique().cpu().shape[0])\n",
    "\n",
    "        p_loss = ga * mse(relations, one_hot_labels)\n",
    "\n",
    "        x2, h2, hs2, hn2 = ae(X)\n",
    "        relations = relationNet(hs2, sample_C)\n",
    "        relations = relations.view(-1, labels.unique().cpu().shape[0])\n",
    "\n",
    "        p_loss = p_loss + ga * mse(relations, one_hot_labels)\n",
    "\n",
    "        rec = mse(x1, X) + mse(x2, X)\n",
    "        if coin > 0:\n",
    "            s_score = discriminator(h1)\n",
    "            tc_loss = beta * gamma *((s_score[:, :1] - s_score[:, 1:]).mean())\n",
    "            s_score = discriminator(h2)\n",
    "            tc_loss = tc_loss + beta * gamma* ((s_score[:, :1] - s_score[:, 1:]).mean())\n",
    "\n",
    "            loss = loss + p_loss + rec + tc_loss\n",
    "            coin -= 1\n",
    "        else:\n",
    "            s, n = permute_dims(hs1, hn1)\n",
    "            b = torch.cat((s, n), 1).detach()\n",
    "            s_score = discriminator(h1)\n",
    "            n_score = discriminator(b)\n",
    "            tc_loss = dis * (F.cross_entropy(s_score, zeros) + F.cross_entropy(n_score, ones))\n",
    "\n",
    "            s, n = permute_dims(hs2, hn2)\n",
    "            b = torch.cat((s, n), 1).detach()\n",
    "            s_score = discriminator(h2)\n",
    "            n_score = discriminator(b)\n",
    "            tc_loss = tc_loss + dis * (F.cross_entropy(s_score, zeros) + F.cross_entropy(n_score, ones))\n",
    "\n",
    "            dis_optimizer.zero_grad()\n",
    "            tc_loss.backward(retain_graph=True)\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            loss = loss + p_loss + rec\n",
    "            coin += dis_step\n",
    "        optimizer.zero_grad()\n",
    "        relation_optimizer.zero_grad()\n",
    "        ae_optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        relation_optimizer.step()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        if it % disp_interval == 0 and it:\n",
    "            log_text = 'Iter-[{}/{}]; loss: {:.3f}; kl:{:.3f}; p_loss:{:.3f}; rec:{:.3f}; tc:{:.3f}; gamma:{:.3f};'.format(it,\n",
    "                                             niter, loss.item(), kl.item(), p_loss.item(), rec.item(), tc_loss.item(), gamma)\n",
    "            log_print(log_text, log_dir)\n",
    "\n",
    "        if it % evl_interval == 0 and it > evl_start:\n",
    "            model.eval()\n",
    "            ae.eval()\n",
    "            gen_feat, gen_label = synthesize_feature_test(model, ae, dataset, nSample, S_dim, Z_dim)\n",
    "            with torch.no_grad():\n",
    "                train_feature = ae.encoder(dataset.train_feature)[:,:S_dim].cpu()\n",
    "                test_unseen_feature = ae.encoder(dataset.test_unseen_feature)[:,:S_dim].cpu()\n",
    "                test_seen_feature = ae.encoder(dataset.test_seen_feature)[:,:S_dim].cpu()\n",
    "\n",
    "            train_X = torch.cat((train_feature, gen_feat), 0)\n",
    "            train_Y = torch.cat((dataset.train_label, gen_label + dataset.ntrain_class), 0)\n",
    "            if zsl:\n",
    "                \"\"\"ZSL\"\"\"\n",
    "                cls = classifier.CLASSIFIER(gen_feat, gen_label, dataset, test_seen_feature, test_unseen_feature,\n",
    "                                            dataset.ntrain_class + dataset.ntest_class, True, classifier_lr, 0.5, 20,\n",
    "                                            nSample, False)\n",
    "                result_zsl_soft.update(it, cls.acc)\n",
    "                log_print(\"ZSL Softmax:\", log_dir)\n",
    "                log_print(\"Acc {:.2f}%  Best_acc [{:.2f}% | Iter-{}]\".format(\n",
    "                    cls.acc, result_zsl_soft.best_acc, result_zsl_soft.best_iter), log_dir)\n",
    "\n",
    "            else:\n",
    "                \"\"\" GZSL\"\"\"\n",
    "                cls = classifier.CLASSIFIER(train_X, train_Y, dataset, test_seen_feature, test_unseen_feature,\n",
    "                                            dataset.ntrain_class + dataset.ntest_class, True, classifier_lr, 0.5,\n",
    "                                            classifier_steps, nSample, True)\n",
    "\n",
    "                result_gzsl_soft.update_gzsl(it, cls.acc_seen, cls.acc_unseen, cls.H)\n",
    "\n",
    "                log_print(\"GZSL Softmax:\", log_dir)\n",
    "                log_print(\"U->T {:.2f}%  S->T {:.2f}%  H {:.2f}%  Best_H [{:.2f}% {:.2f}% {:.2f}% | Iter-{}]\".format(\n",
    "                    cls.acc_unseen, cls.acc_seen, cls.H,  result_gzsl_soft.best_acc_U_T, result_gzsl_soft.best_acc_S_T,\n",
    "                    result_gzsl_soft.best_acc, result_gzsl_soft.best_iter), log_dir)\n",
    "\n",
    "                if result_gzsl_soft.save_model:\n",
    "                    files2remove = glob.glob(out_dir + '/Best_model_GZSL_*')\n",
    "                    for _i in files2remove:\n",
    "                        os.remove(_i)\n",
    "                    save_model(it, model, manualSeed, log_text,\n",
    "                               out_dir + '/Best_model_GZSL_H_{:.2f}_S_{:.2f}_U_{:.2f}.tar'.format(result_gzsl_soft.best_acc,\n",
    "                                                                                                 result_gzsl_soft.best_acc_S_T,\n",
    "                                                                                                 result_gzsl_soft.best_acc_U_T))\n",
    "            ###############################################################################################################\n",
    "\n",
    "            # retrieval code\n",
    "            cls_centrild = np.zeros((dataset.ntest_class, S_dim))\n",
    "            for i in range(dataset.ntest_class):\n",
    "                cls_centrild[i] = torch.mean(gen_feat[gen_label == i,], dim=0)\n",
    "            dist = cosine_similarity(cls_centrild, test_unseen_feature)\n",
    "\n",
    "            precision_100 = torch.zeros(dataset.ntest_class)\n",
    "            precision_50 = torch.zeros(dataset.ntest_class)\n",
    "            precision_25 = torch.zeros(dataset.ntest_class)\n",
    "\n",
    "            dist = torch.from_numpy(-dist)\n",
    "            for i in range(dataset.ntest_class):\n",
    "                is_class = dataset.test_unseen_label == i\n",
    "                # print(is_class.sum())\n",
    "                cls_num = int(is_class.sum())\n",
    "\n",
    "                # 100%\n",
    "                _, idx = torch.topk(dist[i, :], cls_num, largest=False)\n",
    "                precision_100[i] = (is_class[idx]).sum().float() / cls_num\n",
    "\n",
    "                # 50%\n",
    "                cls_num_50 = int(cls_num / 2)\n",
    "                _, idx = torch.topk(dist[i, :], cls_num_50, largest=False)\n",
    "                precision_50[i] = (is_class[idx]).sum().float() / cls_num_50\n",
    "\n",
    "                # 25%\n",
    "                cls_num_25 = int(cls_num / 4)\n",
    "                _, idx = torch.topk(dist[i, :], cls_num_25, largest=False)\n",
    "                precision_25[i] = (is_class[idx]).sum().float() / cls_num_25\n",
    "            print(\"retrieval results 100%%: %.3f 50%%: %.3f 25%%: %.3f\" % (precision_100.mean().item(),\n",
    "                                                                           precision_50.mean().item(),\n",
    "                                                                           precision_25.mean().item()))\n",
    "            ###############################################################################################################\n",
    "            model.train()\n",
    "            ae.train()\n",
    "        if it % save_interval == 0 and it:\n",
    "            save_model(it, model, manualSeed, log_text,\n",
    "                       out_dir + '/Iter_{:d}.tar'.format(it))\n",
    "            print('Save model to ' + out_dir + '/Iter_{:d}.tar'.format(it))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9bfe2ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out/AWA2/wd-1e-06_b-1_g-0.5_lr-3e-05_sd-1024_dis-0.3_nS-5000_nZ-20_bs-64\n",
      "The output dictionary is out/AWA2/wd-1e-06_b-1_g-0.5_lr-3e-05_sd-1024_dis-0.3_nS-5000_nZ-20_bs-64\n",
      "VAE(\n",
      "  (q_z_nn): Sequential(\n",
      "    (0): Linear(in_features=2133, out_features=2048, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  )\n",
      "  (q_z_mean): Linear(in_features=128, out_features=20, bias=True)\n",
      "  (q_z_var): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=20, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): Softplus(beta=1, threshold=20)\n",
      "  )\n",
      "  (p_x_nn): Sequential(\n",
      "    (0): Linear(in_features=105, out_features=2048, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (4): BatchNorm1d(2048, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (7): BatchNorm1d(2048, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (p_x_mean): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      ")\n",
      "Iter-[200/80740]; loss: 12.877; kl:36.979; p_loss:0.033; rec:0.012; tc:0.000; gamma:0.000;\n",
      "Iter-[400/80740]; loss: 11.110; kl:21.632; p_loss:0.031; rec:0.010; tc:-0.000; gamma:0.001;\n",
      "Iter-[600/80740]; loss: 10.650; kl:21.327; p_loss:0.030; rec:0.010; tc:0.832; gamma:0.001;\n",
      "Iter-[800/80740]; loss: 11.432; kl:15.916; p_loss:0.032; rec:0.010; tc:0.000; gamma:0.002;\n",
      "Iter-[1000/80740]; loss: 10.079; kl:14.915; p_loss:0.032; rec:0.009; tc:-0.000; gamma:0.002;\n",
      "Iter-[1200/80740]; loss: 9.399; kl:13.279; p_loss:0.029; rec:0.008; tc:0.832; gamma:0.003;\n",
      "Iter-[1400/80740]; loss: 10.899; kl:12.269; p_loss:0.030; rec:0.010; tc:-0.000; gamma:0.003;\n",
      "Iter-[1600/80740]; loss: 10.666; kl:9.200; p_loss:0.030; rec:0.009; tc:-0.000; gamma:0.004;\n",
      "Iter-[1800/80740]; loss: 10.667; kl:9.249; p_loss:0.029; rec:0.010; tc:0.832; gamma:0.004;\n",
      "Iter-[2000/80740]; loss: 11.479; kl:8.881; p_loss:0.033; rec:0.010; tc:-0.000; gamma:0.005;\n",
      "Iter-[2200/80740]; loss: 9.735; kl:8.476; p_loss:0.034; rec:0.009; tc:-0.000; gamma:0.005;\n",
      "Iter-[2400/80740]; loss: 10.607; kl:7.104; p_loss:0.033; rec:0.009; tc:0.832; gamma:0.006;\n",
      "Iter-[2600/80740]; loss: 9.547; kl:6.149; p_loss:0.033; rec:0.008; tc:-0.000; gamma:0.007;\n",
      "Iter-[2800/80740]; loss: 9.540; kl:5.796; p_loss:0.032; rec:0.008; tc:0.000; gamma:0.007;\n",
      "Iter-[3000/80740]; loss: 9.871; kl:6.270; p_loss:0.031; rec:0.009; tc:0.832; gamma:0.008;\n",
      "Iter-[3200/80740]; loss: 10.829; kl:6.129; p_loss:0.030; rec:0.009; tc:-0.000; gamma:0.008;\n",
      "Iter-[3400/80740]; loss: 9.295; kl:4.614; p_loss:0.033; rec:0.008; tc:0.000; gamma:0.009;\n",
      "Iter-[3600/80740]; loss: 10.725; kl:5.114; p_loss:0.034; rec:0.009; tc:0.832; gamma:0.009;\n",
      "Iter-[3800/80740]; loss: 9.162; kl:4.290; p_loss:0.033; rec:0.008; tc:-0.000; gamma:0.010;\n",
      "Iter-[4000/80740]; loss: 10.001; kl:4.116; p_loss:0.029; rec:0.009; tc:0.000; gamma:0.010;\n",
      "Iter-[4200/80740]; loss: 8.878; kl:3.607; p_loss:0.032; rec:0.008; tc:0.832; gamma:0.011;\n",
      "Iter-[4400/80740]; loss: 10.070; kl:3.779; p_loss:0.028; rec:0.009; tc:0.000; gamma:0.011;\n",
      "Iter-[4600/80740]; loss: 8.903; kl:3.215; p_loss:0.028; rec:0.008; tc:-0.000; gamma:0.012;\n",
      "Iter-[4800/80740]; loss: 8.685; kl:3.033; p_loss:0.028; rec:0.007; tc:0.832; gamma:0.013;\n",
      "Iter-[5000/80740]; loss: 9.987; kl:3.132; p_loss:0.026; rec:0.009; tc:-0.000; gamma:0.013;\n",
      "Iter-[5200/80740]; loss: 8.717; kl:2.758; p_loss:0.026; rec:0.008; tc:0.000; gamma:0.014;\n",
      "Iter-[5400/80740]; loss: 8.481; kl:2.545; p_loss:0.028; rec:0.007; tc:0.832; gamma:0.014;\n",
      "Iter-[5600/80740]; loss: 8.407; kl:2.402; p_loss:0.025; rec:0.008; tc:-0.000; gamma:0.015;\n",
      "Iter-[5800/80740]; loss: 9.786; kl:2.568; p_loss:0.024; rec:0.009; tc:0.000; gamma:0.015;\n",
      "Iter-[6000/80740]; loss: 9.645; kl:2.507; p_loss:0.025; rec:0.008; tc:0.832; gamma:0.016;\n",
      "Iter-[6200/80740]; loss: 9.007; kl:2.320; p_loss:0.022; rec:0.008; tc:-0.000; gamma:0.016;\n",
      "Iter-[6400/80740]; loss: 8.696; kl:2.017; p_loss:0.025; rec:0.008; tc:0.000; gamma:0.017;\n",
      "Iter-[6600/80740]; loss: 8.266; kl:2.022; p_loss:0.019; rec:0.007; tc:0.832; gamma:0.017;\n",
      "Iter-[6800/80740]; loss: 8.069; kl:1.822; p_loss:0.018; rec:0.007; tc:0.000; gamma:0.018;\n",
      "Iter-[7000/80740]; loss: 9.007; kl:2.151; p_loss:0.021; rec:0.008; tc:0.000; gamma:0.019;\n",
      "Iter-[7200/80740]; loss: 9.582; kl:1.754; p_loss:0.021; rec:0.008; tc:0.832; gamma:0.019;\n",
      "Iter-[7400/80740]; loss: 7.764; kl:1.409; p_loss:0.020; rec:0.007; tc:0.000; gamma:0.020;\n",
      "Iter-[7600/80740]; loss: 8.216; kl:1.631; p_loss:0.018; rec:0.007; tc:-0.000; gamma:0.020;\n",
      "Iter-[7800/80740]; loss: 9.224; kl:1.640; p_loss:0.018; rec:0.008; tc:0.832; gamma:0.021;\n",
      "Iter-[8000/80740]; loss: 8.462; kl:1.418; p_loss:0.019; rec:0.007; tc:-0.000; gamma:0.021;\n",
      "Iter-[8200/80740]; loss: 9.249; kl:1.541; p_loss:0.018; rec:0.008; tc:-0.000; gamma:0.022;\n",
      "Iter-[8400/80740]; loss: 8.978; kl:1.600; p_loss:0.016; rec:0.008; tc:0.832; gamma:0.022;\n",
      "Iter-[8600/80740]; loss: 9.957; kl:1.560; p_loss:0.015; rec:0.009; tc:-0.000; gamma:0.023;\n",
      "Iter-[8800/80740]; loss: 8.026; kl:1.262; p_loss:0.015; rec:0.007; tc:-0.000; gamma:0.023;\n",
      "Iter-[9000/80740]; loss: 8.333; kl:1.392; p_loss:0.015; rec:0.008; tc:0.832; gamma:0.024;\n",
      "Iter-[9200/80740]; loss: 8.330; kl:1.352; p_loss:0.014; rec:0.008; tc:-0.000; gamma:0.025;\n",
      "Iter-[9400/80740]; loss: 9.427; kl:1.199; p_loss:0.014; rec:0.009; tc:-0.000; gamma:0.025;\n",
      "Iter-[9600/80740]; loss: 9.408; kl:1.423; p_loss:0.013; rec:0.009; tc:0.831; gamma:0.026;\n",
      "Iter-[9800/80740]; loss: 8.390; kl:1.257; p_loss:0.014; rec:0.008; tc:-0.000; gamma:0.026;\n",
      "Iter-[10000/80740]; loss: 9.721; kl:1.164; p_loss:0.015; rec:0.009; tc:-0.001; gamma:0.027;\n",
      "Save model to out/AWA2/wd-1e-06_b-1_g-0.5_lr-3e-05_sd-1024_dis-0.3_nS-5000_nZ-20_bs-64/Iter_10000.tar\n",
      "Iter-[10200/80740]; loss: 9.255; kl:1.245; p_loss:0.011; rec:0.009; tc:0.830; gamma:0.027;\n",
      "Iter-[10400/80740]; loss: 9.432; kl:0.997; p_loss:0.013; rec:0.009; tc:-0.000; gamma:0.028;\n",
      "Iter-[10600/80740]; loss: 8.931; kl:1.196; p_loss:0.012; rec:0.009; tc:-0.000; gamma:0.028;\n",
      "Iter-[10800/80740]; loss: 8.892; kl:1.012; p_loss:0.012; rec:0.008; tc:0.831; gamma:0.029;\n",
      "Iter-[11000/80740]; loss: 8.507; kl:0.901; p_loss:0.011; rec:0.008; tc:0.000; gamma:0.029;\n",
      "Iter-[11200/80740]; loss: 9.503; kl:1.127; p_loss:0.013; rec:0.009; tc:0.000; gamma:0.030;\n",
      "Iter-[11400/80740]; loss: 8.633; kl:0.912; p_loss:0.012; rec:0.008; tc:0.832; gamma:0.030;\n",
      "Iter-[11600/80740]; loss: 8.668; kl:1.101; p_loss:0.012; rec:0.008; tc:0.000; gamma:0.031;\n",
      "Iter-[11800/80740]; loss: 8.476; kl:0.817; p_loss:0.013; rec:0.008; tc:-0.000; gamma:0.032;\n",
      "Iter-[12000/80740]; loss: 8.749; kl:0.833; p_loss:0.010; rec:0.008; tc:0.832; gamma:0.032;\n",
      "Iter-[12200/80740]; loss: 8.704; kl:1.020; p_loss:0.012; rec:0.008; tc:-0.000; gamma:0.033;\n",
      "Iter-[12400/80740]; loss: 8.198; kl:0.669; p_loss:0.010; rec:0.008; tc:0.000; gamma:0.033;\n",
      "Iter-[12600/80740]; loss: 9.344; kl:0.727; p_loss:0.010; rec:0.009; tc:0.832; gamma:0.034;\n",
      "Iter-[12800/80740]; loss: 9.536; kl:0.939; p_loss:0.009; rec:0.009; tc:-0.000; gamma:0.034;\n",
      "Iter-[13000/80740]; loss: 9.596; kl:0.746; p_loss:0.010; rec:0.009; tc:-0.000; gamma:0.035;\n",
      "Iter-[13200/80740]; loss: 9.240; kl:0.810; p_loss:0.010; rec:0.009; tc:0.832; gamma:0.035;\n",
      "Iter-[13400/80740]; loss: 8.379; kl:0.813; p_loss:0.009; rec:0.008; tc:-0.000; gamma:0.036;\n",
      "Iter-[13600/80740]; loss: 7.376; kl:0.612; p_loss:0.008; rec:0.007; tc:-0.000; gamma:0.036;\n",
      "Iter-[13800/80740]; loss: 8.365; kl:0.757; p_loss:0.008; rec:0.008; tc:0.832; gamma:0.037;\n",
      "Iter-[14000/80740]; loss: 9.803; kl:0.783; p_loss:0.009; rec:0.009; tc:-0.000; gamma:0.038;\n",
      "Iter-[14200/80740]; loss: 9.872; kl:0.707; p_loss:0.009; rec:0.009; tc:-0.000; gamma:0.038;\n",
      "Iter-[14400/80740]; loss: 9.354; kl:0.708; p_loss:0.008; rec:0.009; tc:0.832; gamma:0.039;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-[14600/80740]; loss: 7.826; kl:0.605; p_loss:0.008; rec:0.007; tc:-0.000; gamma:0.039;\n",
      "Iter-[14800/80740]; loss: 8.602; kl:0.608; p_loss:0.007; rec:0.008; tc:-0.000; gamma:0.040;\n",
      "Iter-[15000/80740]; loss: 9.546; kl:0.654; p_loss:0.008; rec:0.009; tc:0.832; gamma:0.040;\n",
      "Iter-[15200/80740]; loss: 8.678; kl:0.686; p_loss:0.007; rec:0.008; tc:-0.000; gamma:0.041;\n",
      "Iter-[15400/80740]; loss: 8.916; kl:0.558; p_loss:0.008; rec:0.008; tc:-0.000; gamma:0.041;\n",
      "Iter-[15600/80740]; loss: 7.905; kl:0.546; p_loss:0.006; rec:0.007; tc:0.832; gamma:0.042;\n",
      "Iter-[15800/80740]; loss: 9.136; kl:0.582; p_loss:0.008; rec:0.008; tc:-0.000; gamma:0.042;\n",
      "Iter-[16000/80740]; loss: 7.652; kl:0.438; p_loss:0.006; rec:0.007; tc:-0.000; gamma:0.043;\n",
      "Iter-[16200/80740]; loss: 8.033; kl:0.531; p_loss:0.007; rec:0.008; tc:0.832; gamma:0.044;\n",
      "Iter-[16400/80740]; loss: 8.572; kl:0.543; p_loss:0.006; rec:0.008; tc:-0.000; gamma:0.044;\n",
      "Iter-[16600/80740]; loss: 7.677; kl:0.486; p_loss:0.006; rec:0.007; tc:-0.000; gamma:0.045;\n",
      "Iter-[16800/80740]; loss: 8.009; kl:0.498; p_loss:0.005; rec:0.008; tc:0.832; gamma:0.045;\n",
      "Iter-[17000/80740]; loss: 8.330; kl:0.454; p_loss:0.007; rec:0.008; tc:-0.000; gamma:0.046;\n",
      "Iter-[17200/80740]; loss: 8.977; kl:0.439; p_loss:0.005; rec:0.009; tc:-0.000; gamma:0.046;\n",
      "Iter-[17400/80740]; loss: 8.692; kl:0.408; p_loss:0.005; rec:0.008; tc:0.832; gamma:0.047;\n",
      "Iter-[17600/80740]; loss: 8.722; kl:0.530; p_loss:0.005; rec:0.008; tc:-0.000; gamma:0.047;\n",
      "Iter-[17800/80740]; loss: 8.104; kl:0.496; p_loss:0.005; rec:0.008; tc:-0.000; gamma:0.048;\n",
      "Iter-[18000/80740]; loss: 9.626; kl:0.537; p_loss:0.004; rec:0.009; tc:0.832; gamma:0.048;\n",
      "Iter-[18200/80740]; loss: 8.698; kl:0.498; p_loss:0.006; rec:0.008; tc:0.000; gamma:0.049;\n",
      "Iter-[18400/80740]; loss: 9.191; kl:0.457; p_loss:0.005; rec:0.009; tc:0.000; gamma:0.050;\n",
      "Iter-[18600/80740]; loss: 9.360; kl:0.501; p_loss:0.006; rec:0.009; tc:0.832; gamma:0.050;\n",
      "Iter-[18800/80740]; loss: 8.487; kl:0.531; p_loss:0.004; rec:0.008; tc:-0.000; gamma:0.051;\n",
      "Iter-[19000/80740]; loss: 9.592; kl:0.510; p_loss:0.006; rec:0.009; tc:-0.000; gamma:0.051;\n",
      "Iter-[19200/80740]; loss: 8.694; kl:0.478; p_loss:0.004; rec:0.008; tc:0.832; gamma:0.052;\n",
      "Iter-[19400/80740]; loss: 8.857; kl:0.394; p_loss:0.004; rec:0.009; tc:0.000; gamma:0.052;\n",
      "Iter-[19600/80740]; loss: 8.312; kl:0.459; p_loss:0.004; rec:0.008; tc:0.000; gamma:0.053;\n",
      "Iter-[19800/80740]; loss: 8.697; kl:0.426; p_loss:0.004; rec:0.008; tc:0.832; gamma:0.053;\n",
      "Iter-[20000/80740]; loss: 8.488; kl:0.376; p_loss:0.005; rec:0.008; tc:-0.000; gamma:0.054;\n",
      "Save model to out/AWA2/wd-1e-06_b-1_g-0.5_lr-3e-05_sd-1024_dis-0.3_nS-5000_nZ-20_bs-64/Iter_20000.tar\n",
      "Iter-[20200/80740]; loss: 7.819; kl:0.381; p_loss:0.003; rec:0.008; tc:0.000; gamma:0.054;\n",
      "Iter-[20400/80740]; loss: 8.480; kl:0.454; p_loss:0.004; rec:0.008; tc:0.832; gamma:0.055;\n",
      "Iter-[20600/80740]; loss: 8.587; kl:0.472; p_loss:0.003; rec:0.008; tc:0.000; gamma:0.055;\n",
      "Iter-[20800/80740]; loss: 8.899; kl:0.391; p_loss:0.003; rec:0.009; tc:-0.000; gamma:0.056;\n",
      "Iter-[21000/80740]; loss: 8.291; kl:0.364; p_loss:0.003; rec:0.008; tc:0.832; gamma:0.057;\n",
      "Iter-[21200/80740]; loss: 8.005; kl:0.335; p_loss:0.004; rec:0.008; tc:-0.000; gamma:0.057;\n",
      "Iter-[21400/80740]; loss: 10.322; kl:0.572; p_loss:0.005; rec:0.010; tc:-0.000; gamma:0.058;\n",
      "Iter-[21600/80740]; loss: 8.476; kl:0.364; p_loss:0.003; rec:0.008; tc:0.832; gamma:0.058;\n",
      "Iter-[21800/80740]; loss: 8.075; kl:0.278; p_loss:0.003; rec:0.008; tc:0.000; gamma:0.059;\n",
      "Iter-[22000/80740]; loss: 8.155; kl:0.369; p_loss:0.002; rec:0.008; tc:0.000; gamma:0.059;\n",
      "Iter-[22200/80740]; loss: 9.570; kl:0.393; p_loss:0.004; rec:0.009; tc:0.832; gamma:0.060;\n",
      "Iter-[22400/80740]; loss: 9.407; kl:0.403; p_loss:0.004; rec:0.009; tc:-0.000; gamma:0.060;\n",
      "Iter-[22600/80740]; loss: 9.050; kl:0.378; p_loss:0.004; rec:0.009; tc:0.000; gamma:0.061;\n",
      "Iter-[22800/80740]; loss: 9.146; kl:0.382; p_loss:0.003; rec:0.009; tc:0.832; gamma:0.061;\n",
      "Iter-[23000/80740]; loss: 9.133; kl:0.422; p_loss:0.003; rec:0.009; tc:0.000; gamma:0.062;\n",
      "Iter-[23200/80740]; loss: 7.871; kl:0.322; p_loss:0.003; rec:0.008; tc:0.001; gamma:0.063;\n",
      "Iter-[23400/80740]; loss: 8.350; kl:0.282; p_loss:0.003; rec:0.008; tc:0.832; gamma:0.063;\n",
      "Iter-[23600/80740]; loss: 8.873; kl:0.380; p_loss:0.002; rec:0.009; tc:0.001; gamma:0.064;\n",
      "Iter-[23800/80740]; loss: 9.566; kl:0.345; p_loss:0.002; rec:0.009; tc:0.000; gamma:0.064;\n",
      "Iter-[24000/80740]; loss: 8.106; kl:0.278; p_loss:0.002; rec:0.008; tc:0.832; gamma:0.065;\n",
      "Iter-[24200/80740]; loss: 7.495; kl:0.250; p_loss:0.002; rec:0.007; tc:0.001; gamma:0.065;\n",
      "Iter-[24400/80740]; loss: 8.982; kl:0.358; p_loss:0.003; rec:0.008; tc:0.001; gamma:0.066;\n",
      "Iter-[24600/80740]; loss: 8.059; kl:0.256; p_loss:0.002; rec:0.008; tc:0.832; gamma:0.066;\n",
      "Iter-[24800/80740]; loss: 9.854; kl:0.325; p_loss:0.004; rec:0.009; tc:0.001; gamma:0.067;\n",
      "Iter-[25000/80740]; loss: 8.208; kl:0.266; p_loss:0.002; rec:0.008; tc:0.001; gamma:0.067;\n",
      "Iter-[25200/80740]; loss: 8.784; kl:0.332; p_loss:0.004; rec:0.008; tc:0.832; gamma:0.068;\n",
      "Iter-[25400/80740]; loss: 8.788; kl:0.309; p_loss:0.003; rec:0.008; tc:0.001; gamma:0.069;\n",
      "Iter-[25600/80740]; loss: 8.789; kl:0.309; p_loss:0.003; rec:0.008; tc:0.001; gamma:0.069;\n",
      "Iter-[25800/80740]; loss: 9.985; kl:0.350; p_loss:0.002; rec:0.009; tc:0.832; gamma:0.070;\n",
      "Iter-[26000/80740]; loss: 8.321; kl:0.253; p_loss:0.003; rec:0.008; tc:0.000; gamma:0.070;\n",
      "Iter-[26200/80740]; loss: 9.594; kl:0.377; p_loss:0.003; rec:0.009; tc:-0.000; gamma:0.071;\n",
      "Iter-[26400/80740]; loss: 7.900; kl:0.251; p_loss:0.002; rec:0.008; tc:0.832; gamma:0.071;\n",
      "Iter-[26600/80740]; loss: 7.890; kl:0.234; p_loss:0.003; rec:0.008; tc:0.000; gamma:0.072;\n",
      "Iter-[26800/80740]; loss: 8.716; kl:0.273; p_loss:0.004; rec:0.008; tc:0.001; gamma:0.072;\n",
      "Iter-[27000/80740]; loss: 7.972; kl:0.273; p_loss:0.002; rec:0.008; tc:0.832; gamma:0.073;\n",
      "Iter-[27200/80740]; loss: 7.863; kl:0.260; p_loss:0.002; rec:0.008; tc:0.000; gamma:0.073;\n",
      "Iter-[27400/80740]; loss: 8.159; kl:0.269; p_loss:0.003; rec:0.008; tc:0.000; gamma:0.074;\n",
      "Iter-[27600/80740]; loss: 9.076; kl:0.287; p_loss:0.002; rec:0.009; tc:0.832; gamma:0.075;\n",
      "Iter-[27800/80740]; loss: 8.854; kl:0.323; p_loss:0.003; rec:0.008; tc:0.002; gamma:0.075;\n",
      "Iter-[28000/80740]; loss: 8.278; kl:0.225; p_loss:0.003; rec:0.008; tc:0.001; gamma:0.076;\n",
      "Iter-[28200/80740]; loss: 10.111; kl:0.280; p_loss:0.003; rec:0.010; tc:0.832; gamma:0.076;\n",
      "Iter-[28400/80740]; loss: 8.143; kl:0.239; p_loss:0.004; rec:0.008; tc:0.001; gamma:0.077;\n",
      "Iter-[28600/80740]; loss: 8.555; kl:0.245; p_loss:0.003; rec:0.008; tc:-0.001; gamma:0.077;\n",
      "Iter-[28800/80740]; loss: 7.936; kl:0.207; p_loss:0.003; rec:0.008; tc:0.830; gamma:0.078;\n",
      "Iter-[29000/80740]; loss: 7.904; kl:0.206; p_loss:0.003; rec:0.008; tc:-0.001; gamma:0.078;\n",
      "Iter-[29200/80740]; loss: 8.819; kl:0.267; p_loss:0.005; rec:0.008; tc:-0.001; gamma:0.079;\n",
      "Iter-[29400/80740]; loss: 8.245; kl:0.252; p_loss:0.005; rec:0.008; tc:0.829; gamma:0.079;\n",
      "Iter-[29600/80740]; loss: 7.983; kl:0.220; p_loss:0.004; rec:0.008; tc:-0.003; gamma:0.080;\n",
      "Iter-[29800/80740]; loss: 8.389; kl:0.247; p_loss:0.005; rec:0.008; tc:-0.001; gamma:0.080;\n",
      "Iter-[30000/80740]; loss: 8.511; kl:0.269; p_loss:0.007; rec:0.008; tc:0.833; gamma:0.081;\n",
      "Save model to out/AWA2/wd-1e-06_b-1_g-0.5_lr-3e-05_sd-1024_dis-0.3_nS-5000_nZ-20_bs-64/Iter_30000.tar\n",
      "Iter-[30200/80740]; loss: 8.689; kl:0.197; p_loss:0.005; rec:0.008; tc:0.001; gamma:0.082;\n",
      "Iter-[30400/80740]; loss: 8.997; kl:0.200; p_loss:0.006; rec:0.009; tc:0.001; gamma:0.082;\n",
      "Iter-[30600/80740]; loss: 8.429; kl:0.220; p_loss:0.005; rec:0.008; tc:0.833; gamma:0.083;\n",
      "Iter-[30800/80740]; loss: 10.213; kl:0.253; p_loss:0.005; rec:0.010; tc:0.001; gamma:0.083;\n",
      "Iter-[31000/80740]; loss: 7.777; kl:0.224; p_loss:0.003; rec:0.008; tc:0.001; gamma:0.084;\n",
      "Iter-[31200/80740]; loss: 9.263; kl:0.204; p_loss:0.004; rec:0.009; tc:0.824; gamma:0.084;\n",
      "Iter-[31400/80740]; loss: 8.905; kl:0.225; p_loss:0.003; rec:0.009; tc:0.003; gamma:0.085;\n",
      "Iter-[31600/80740]; loss: 8.507; kl:0.235; p_loss:0.004; rec:0.009; tc:0.004; gamma:0.085;\n",
      "Iter-[31800/80740]; loss: 8.141; kl:0.162; p_loss:0.008; rec:0.008; tc:0.821; gamma:0.086;\n",
      "Iter-[32000/80740]; loss: 8.109; kl:0.228; p_loss:0.007; rec:0.008; tc:-0.003; gamma:0.086;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-[32200/80740]; loss: 8.853; kl:0.230; p_loss:0.009; rec:0.009; tc:-0.005; gamma:0.087;\n",
      "Iter-[32400/80740]; loss: 8.994; kl:0.203; p_loss:0.008; rec:0.009; tc:0.834; gamma:0.088;\n",
      "Iter-[32600/80740]; loss: 8.377; kl:0.191; p_loss:0.008; rec:0.009; tc:-0.001; gamma:0.088;\n",
      "Iter-[32800/80740]; loss: 7.595; kl:0.152; p_loss:0.006; rec:0.008; tc:-0.003; gamma:0.089;\n",
      "Iter-[33000/80740]; loss: 7.648; kl:0.211; p_loss:0.006; rec:0.008; tc:0.819; gamma:0.089;\n",
      "Iter-[33200/80740]; loss: 8.952; kl:0.200; p_loss:0.010; rec:0.009; tc:0.004; gamma:0.090;\n",
      "Iter-[33400/80740]; loss: 8.213; kl:0.205; p_loss:0.006; rec:0.009; tc:-0.002; gamma:0.090;\n",
      "Iter-[33600/80740]; loss: 8.175; kl:0.167; p_loss:0.005; rec:0.009; tc:0.828; gamma:0.091;\n",
      "Iter-[33800/80740]; loss: 8.900; kl:0.225; p_loss:0.004; rec:0.009; tc:-0.005; gamma:0.091;\n",
      "Iter-[34000/80740]; loss: 9.241; kl:0.205; p_loss:0.004; rec:0.009; tc:-0.007; gamma:0.092;\n",
      "Iter-[34200/80740]; loss: 8.812; kl:0.209; p_loss:0.005; rec:0.009; tc:0.834; gamma:0.092;\n",
      "Iter-[34400/80740]; loss: 8.846; kl:0.184; p_loss:0.003; rec:0.009; tc:-0.015; gamma:0.093;\n",
      "Iter-[34600/80740]; loss: 7.814; kl:0.164; p_loss:0.004; rec:0.008; tc:-0.008; gamma:0.094;\n",
      "Iter-[34800/80740]; loss: 9.124; kl:0.196; p_loss:0.003; rec:0.009; tc:0.832; gamma:0.094;\n",
      "Iter-[35000/80740]; loss: 8.969; kl:0.194; p_loss:0.004; rec:0.009; tc:-0.005; gamma:0.095;\n",
      "Iter-[35200/80740]; loss: 8.650; kl:0.158; p_loss:0.005; rec:0.009; tc:-0.004; gamma:0.095;\n",
      "Iter-[35400/80740]; loss: 8.332; kl:0.167; p_loss:0.003; rec:0.009; tc:0.832; gamma:0.096;\n",
      "Iter-[35600/80740]; loss: 7.971; kl:0.158; p_loss:0.004; rec:0.008; tc:-0.002; gamma:0.096;\n",
      "Iter-[35800/80740]; loss: 8.407; kl:0.160; p_loss:0.004; rec:0.009; tc:-0.001; gamma:0.097;\n",
      "Iter-[36000/80740]; loss: 8.225; kl:0.199; p_loss:0.003; rec:0.008; tc:0.832; gamma:0.097;\n",
      "Iter-[36200/80740]; loss: 8.656; kl:0.175; p_loss:0.004; rec:0.009; tc:-0.002; gamma:0.098;\n",
      "Iter-[36400/80740]; loss: 8.165; kl:0.166; p_loss:0.003; rec:0.008; tc:-0.000; gamma:0.098;\n",
      "Iter-[36600/80740]; loss: 8.940; kl:0.186; p_loss:0.004; rec:0.009; tc:0.832; gamma:0.099;\n",
      "Iter-[36800/80740]; loss: 8.433; kl:0.180; p_loss:0.003; rec:0.009; tc:-0.002; gamma:0.100;\n",
      "Iter-[37000/80740]; loss: 9.536; kl:0.181; p_loss:0.004; rec:0.010; tc:-0.002; gamma:0.100;\n",
      "Iter-[37200/80740]; loss: 8.347; kl:0.180; p_loss:0.004; rec:0.009; tc:0.832; gamma:0.101;\n",
      "Iter-[37400/80740]; loss: 7.500; kl:0.161; p_loss:0.004; rec:0.008; tc:-0.002; gamma:0.101;\n",
      "Iter-[37600/80740]; loss: 8.257; kl:0.161; p_loss:0.004; rec:0.009; tc:-0.001; gamma:0.102;\n",
      "Iter-[37800/80740]; loss: 9.158; kl:0.182; p_loss:0.005; rec:0.009; tc:0.832; gamma:0.102;\n",
      "Iter-[38000/80740]; loss: 9.256; kl:0.157; p_loss:0.003; rec:0.009; tc:0.004; gamma:0.103;\n",
      "Iter-[38200/80740]; loss: 8.802; kl:0.150; p_loss:0.004; rec:0.009; tc:0.010; gamma:0.103;\n",
      "Iter-[38400/80740]; loss: 9.124; kl:0.178; p_loss:0.003; rec:0.009; tc:0.830; gamma:0.104;\n",
      "Iter-[38600/80740]; loss: 9.028; kl:0.181; p_loss:0.003; rec:0.009; tc:0.003; gamma:0.104;\n",
      "Iter-[38800/80740]; loss: 8.167; kl:0.167; p_loss:0.004; rec:0.008; tc:0.001; gamma:0.105;\n",
      "Iter-[39000/80740]; loss: 8.429; kl:0.177; p_loss:0.003; rec:0.009; tc:0.832; gamma:0.105;\n",
      "Iter-[39200/80740]; loss: 9.083; kl:0.161; p_loss:0.004; rec:0.009; tc:-0.005; gamma:0.106;\n",
      "Iter-[39400/80740]; loss: 8.957; kl:0.159; p_loss:0.006; rec:0.009; tc:-0.003; gamma:0.107;\n",
      "Iter-[39600/80740]; loss: 8.084; kl:0.149; p_loss:0.003; rec:0.008; tc:0.832; gamma:0.107;\n",
      "Iter-[39800/80740]; loss: 7.755; kl:0.145; p_loss:0.003; rec:0.008; tc:-0.004; gamma:0.108;\n",
      "Iter-[40000/80740]; loss: 8.943; kl:0.211; p_loss:0.003; rec:0.009; tc:-0.002; gamma:0.108;\n",
      "Save model to out/AWA2/wd-1e-06_b-1_g-0.5_lr-3e-05_sd-1024_dis-0.3_nS-5000_nZ-20_bs-64/Iter_40000.tar\n",
      "Iter-[40200/80740]; loss: 7.599; kl:0.172; p_loss:0.002; rec:0.008; tc:0.832; gamma:0.109;\n",
      "Iter-[40400/80740]; loss: 8.689; kl:0.164; p_loss:0.004; rec:0.009; tc:0.003; gamma:0.109;\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'test_seen_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(dataset_name, dataroot, workers, image_embedding, class_embedding, gen_nepoch, lr, zsl, finetune, ga, beta, \n\u001b[0;32m      2\u001b[0m           weight_decay, dis, dis_step, kl_warmup, tc_warmup, vae_dec_drop, vae_enc_drop, ae_drop, classifier_lr, \n\u001b[0;32m      3\u001b[0m           classifier_steps, batchsize, nSample, disp_interval, save_interval, evl_interval, evl_start, manualSeed, \n\u001b[0;32m      4\u001b[0m           latent_dim, q_z_nn_output_dim, S_dim, NS_dim)\n",
      "Cell \u001b[1;32mIn[7], line 170\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset_name, dataroot, workers, image_embedding, class_embedding, gen_nepoch, lr, zsl, finetune, ga, beta, weight_decay, dis, dis_step, kl_warmup, tc_warmup, vae_dec_drop, vae_enc_drop, ae_drop, classifier_lr, classifier_steps, batchsize, nSample, disp_interval, save_interval, evl_interval, evl_start, manualSeed, latent_dim, q_z_nn_output_dim, S_dim, NS_dim)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" GZSL\"\"\"\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mCLASSIFIER(train_X, train_Y, dataset, test_seen_feature, test_unseen_feature,\n\u001b[0;32m    171\u001b[0m                         dataset\u001b[38;5;241m.\u001b[39mntrain_class \u001b[38;5;241m+\u001b[39m dataset\u001b[38;5;241m.\u001b[39mntest_class, \u001b[38;5;28;01mTrue\u001b[39;00m, classifier_lr, \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m    172\u001b[0m                                 classifier_steps, nSample, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    174\u001b[0m     result_gzsl_soft\u001b[38;5;241m.\u001b[39mupdate_gzsl(it, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39macc_seen, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39macc_unseen, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mH)\n\u001b[0;32m    176\u001b[0m     log_print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGZSL Softmax:\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_dir)\n",
      "File \u001b[1;32m~\\Sushree\\GZSL_Implementation_SDGZSL\\classifier.py:22\u001b[0m, in \u001b[0;36mCLASSIFIER.__init__\u001b[1;34m(self, opt, _train_X, _train_Y, data_loader, test_seen_feature, test_unseen_feature, _nclass, _cuda, _lr, _beta1, _nepoch, _batch_size, generalized, MCA)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_Y \u001b[38;5;241m=\u001b[39m _train_Y\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_seen_feature \u001b[38;5;241m=\u001b[39m test_seen_feature\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_seen_label \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mtest_seen_label\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_unseen_feature \u001b[38;5;241m=\u001b[39m test_unseen_feature\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_unseen_label \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mtest_unseen_label\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'test_seen_label'"
     ]
    }
   ],
   "source": [
    "train(dataset_name, dataroot, workers, image_embedding, class_embedding, gen_nepoch, lr, zsl, finetune, ga, beta, \n",
    "          weight_decay, dis, dis_step, kl_warmup, tc_warmup, vae_dec_drop, vae_enc_drop, ae_drop, classifier_lr, \n",
    "          classifier_steps, batchsize, nSample, disp_interval, save_interval, evl_interval, evl_start, manualSeed, \n",
    "          latent_dim, q_z_nn_output_dim, S_dim, NS_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a288c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145f11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
